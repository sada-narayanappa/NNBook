{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<textarea rows=5 id=nav_head_content style=\"width:100%;display:none;\">\n",
       "<style>\n",
       "a.bh,  a.bh:visited, a.bh:link, a.bh:active {\n",
       "  ttext-decoration: none; \n",
       "  color: black;\n",
       "  font-family: 'Roboto','Heebo',Arial,sans-serif;\n",
       "  font-size: 3em\n",
       "}\n",
       "</style>\n",
       "<!--\n",
       "<div id='HTMLTopBar' style=\"    \n",
       "    z-index: 50;\n",
       "    align-items: stretch;\n",
       "    width:100%\">\n",
       "    <div  style=\"\n",
       "        text-color: black;\n",
       "        background-color: #fefefe;\n",
       "        bborder-bottom: 1px dotted gray;\n",
       "        padding-left: 2px;\n",
       "        box-shadow: 5px 1px #ccc;\n",
       "        height: 40px; left: 0; \n",
       "        padding: 14px;\n",
       "        \"\n",
       "    >\n",
       "    <a class=bh1 href=\"#\" onclick=\"$('#maintoolbar').toggle();\">X</a>\n",
       "</div>\n",
       "-->\n",
       "</textarea>\n",
       "\n",
       "<script>\n",
       "if ($('#nav_head').length < 1) {\n",
       "    $('#notebook-container').prepend('<div id=\"nav_head\" style=\"width:100%;\">.</div>')\n",
       "    console.log(\"Added a div\")\n",
       "} else{\n",
       "    console.log(\"Already Added\")    \n",
       "}\n",
       "\n",
       "$('#nav_head').html($('#nav_head_content').val())\n",
       "\n",
       "$('.nbp-app-bar').toggle()\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url('https://fonts.googleapis.com/css2?family=Roboto&display=swap');\n",
       "</style>\n",
       "<style>\n",
       "<link href=\"https://fonts.googleapis.com/css2?family=Roboto&display=swap\" rel=\"stylesheet\">\n",
       "\n",
       "body  p ol li {\n",
       "    font-family: \"Roboto\",  \"Lucida Grande\", \"Lucida Sans Unicode\";\n",
       "    font-size: 14px;\n",
       "    background: #fff\n",
       "}\n",
       "\n",
       "h1, h2, h3{\n",
       "    font-family: 'Roboto', sans-serif;\n",
       "}\n",
       "div#notebook_panel div#notebook{\n",
       "    background: #ffffff\n",
       "}\n",
       "div#notebook-container{\n",
       "    box-shadow: 0px;\n",
       "    padding: 0px;\n",
       "    border-left: .05em dotted gray;\n",
       "    -webkit-box-shadow: 0px;\n",
       "    box-shadow: none;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".container { width: 100% !important; }\n",
       "    div.cell{\n",
       "        width:100%;\n",
       "        margin-left:0%;\n",
       "        margin-right:auto;\n",
       "}\n",
       ".CodeMirror {\n",
       "    font-family: monospace;\n",
       "}\n",
       "div.input_area {\n",
       "    border: 0px;\n",
       "}\n",
       "div.cell.selected{\n",
       "  border: '0px';\n",
       "}\n",
       ".cell.selected{\n",
       "  border: '0px';\n",
       "}\n",
       "div#notebook{\n",
       "    padding-top: 0px;\n",
       "}\n",
       "div.prompt_container {\n",
       "    display: block; \n",
       "    background: #f7f7f7;\n",
       "}\n",
       "div.prompt{\n",
       "    min-width:0px;\n",
       "    display: grid;\n",
       "}\n",
       "div.cell{\n",
       "    padding-bottom: 5px;\n",
       "    padding-left: 0px;\n",
       "}\n",
       "\n",
       "a.bh,  a.bh:visited, a.bh:link, a.bh:active {\n",
       "  text-decoration: none; \n",
       "  color:white;\n",
       "  font-family: 'Roboto','Heebo',Arial,sans-serif;\n",
       "  font-size:1em\n",
       "}\n",
       "a.bh:hover {\n",
       "  color: #ffccdd;\n",
       "}\n",
       "\n",
       "a.bh1,  a.bh1:visited, a.bh1:link, a.bh1:active {\n",
       "  text-decoration: none; \n",
       "  color: rgba(0, 0, 0, 0.87);\n",
       "  font-family: 'Roboto','Heebo',Arial,sans-serif;\n",
       "  padding-right: 30px;\n",
       "  font-size:1.1em\n",
       "}\n",
       "a.bh1:hover {\n",
       "  color: rgba(0, 0, 0, 0.57);\n",
       "}\n",
       "    \n",
       "#toc-wrapper {\n",
       "   font-family: 'Roboto','Heebo',Arial,sans-serif;\n",
       "    border: 0px dotted gray;\n",
       "    padding: 10px;\n",
       "    line-height: 1.8em;\n",
       "}    \n",
       "</style>\n",
       "<script>\n",
       "l=\"https://www.ancient-symbols.com/images/wp-image-library/fullsize/infinity.jpg\"\n",
       "l=\"imgs/logo.png\"\n",
       "l=\"\"\n",
       "$('#ipython_notebook').html(`<img src=\"${l}\">`)\n",
       "\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run basic.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Utilities \n",
    "\n",
    "This will be included in all the files as generic utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile  ~/bin/gen/lstmutils1.py\n",
    "#!/usr/local/bin/python \n",
    "'''\n",
    "Some utility Functions to be used in all the apps\n",
    "#=*** NOTE *** DO NOT EDIT THIS FILE - THIS iS CREATED FROM: 01_utils.ipynb\n",
    "'''\n",
    "import re, sklearn, sys, os, datetime, glob, argparse, json, base64, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (16, 4)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "pd.options.display.max_rows = 8\n",
    "\n",
    "sys.path.append(\"~/bin/gen\")\n",
    "import ccallbacks\n",
    "\n",
    "def getconfig(cf = \"config*\"):\n",
    "    confFiles = sorted(glob.glob(cf))\n",
    "    if ( len(confFiles) <= 0):\n",
    "        print(f\"No Configuration files {cf} found!!!\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Read and merge the configuration files\n",
    "    ret = {}\n",
    "    for cf in confFiles:\n",
    "        print(f\"#Getting Configuration from {cf}\")\n",
    "        with open(cf, \"r\") as f:\n",
    "            cf = f.read()\n",
    "\n",
    "        if (not cf.find('[START]') >=0 ):\n",
    "            r1 = cf\n",
    "        else:\n",
    "            r1=re.findall(\"\\[START](.*)\\[END]\", cf, flags=re.MULTILINE|re.DOTALL)\n",
    "            if ( len(r1) <= 0):\n",
    "                print(f\"Ignoring: Configuration not found in {cf}! no worries\")\n",
    "                continue\n",
    "            r1 = r1[0].replace(\"'\", '\"')    \n",
    "        rj = eval(r1)\n",
    "        ret.update(rj)\n",
    "                \n",
    "    return ret\n",
    "    \n",
    "#-----------------------------------------------------------------------------------\n",
    "def getConfigList(conf, key=\"\"):\n",
    "    #print(f\"Getting {key}\")\n",
    "    ll = conf.get(key, []);\n",
    "    ret = []\n",
    "    for l in ll:\n",
    "        if type(l) == list:\n",
    "            ret += l\n",
    "        elif l.startswith(\"$\"):\n",
    "            ret += getConfigList(conf, l[1:])\n",
    "        else:\n",
    "            ret.append(l)\n",
    "\n",
    "    return ret\n",
    "#-----------------------------------------------------------------------------------\n",
    "def getConfigObject(conf, key=\"\"):\n",
    "    ll = conf.get(key, \"\");\n",
    "    if not ll:\n",
    "        return;\n",
    "    \n",
    "    sst = ll[0]\n",
    "    dec = base64.b64decode(sst)\n",
    "    ret = pickle.loads(dec, fix_imports=True)\n",
    "\n",
    "    return ret\n",
    "#--------------------------------------------------------------------------------\n",
    "def runMethod(pyMethod, **kwargs):\n",
    "    spl = pyMethod.split('.');\n",
    "\n",
    "    assert len(spl) >= 2, \"Hmmm ... May be not what is intended!! module name\"\n",
    "\n",
    "    modName = \".\".join(spl[:-1])\n",
    "    __import__(modName, fromlist=\"dummy\")\n",
    "\n",
    "    funName = spl[-1]\n",
    "    ret = None\n",
    "    for v in sys.modules:\n",
    "        if (v.startswith(modName)):\n",
    "            method= getattr(sys.modules[v], funName)\n",
    "            print(v, type(v), funName, method, type(method), callable(method))\n",
    "            ret = method(**kwargs)\n",
    "    return ret\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "def getInvertedPreds(conf, yh):\n",
    "    scalerY = utils1.getConfigObject(conf, \"scalerYString\")\n",
    "    sOuputs = utils1.getConfigList(conf, 'scaleOutputs')\n",
    "    ouputs  = utils1.getConfigList(conf, 'outputs')\n",
    "    \n",
    "    yhdf    = pd.DataFrame(yh, columns=ouputs)    # Dataframe of Predictions\n",
    "    ys      = yhdf[sOuputs].values                # Values to be scaledback\n",
    "    yi      = scalerY.inverse_transform(ys)       # inverse transform the outputs\n",
    "    yidf    = yhdf.copy()                         # Copy and set the values\n",
    "    yidf[sOuputs] = yi\n",
    "    return yhdf, yidf\n",
    "    \n",
    "#--------------------------------------------------------------------------------\n",
    "def getOriginal(conf, unnormdf, index=0):\n",
    "    inputs  = utils1.getConfigList(conf, 'inputs')\n",
    "    ouputs  = utils1.getConfigList(conf, 'outputs')\n",
    "    tsParams= conf.get(\"tsParams\", {})\n",
    "    \n",
    "    index   = 0\n",
    "    startIX = index + tsParams['length']\n",
    "    batchSZ = 1 # batch size\n",
    "    stride  = tsParams.get('stride', 1)\n",
    "    i = startIX + batchSZ * stride * index\n",
    "    return unnormdf[i:], inputs, ouputs\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "def plotInverted(conf, yh, unnormdf, s =-400, howmany=100):\n",
    "    e=s+howmany\n",
    "    \n",
    "    yhdf, yidf = getInvertedPreds(conf, yh)\n",
    "    yorg,ips,ops = getOriginal(conf, unnormdf)\n",
    "\n",
    "    x = pd.to_datetime(yorg[yorg.columns[0]][s:e])\n",
    "    \n",
    "    plt.plot(x, yorg[ops].values[s:e], marker='.', label=\"Original\")\n",
    "    plt.plot(x, yidf.values[s:e], marker='x', label=\"Predicted\")\n",
    "    plt.title(\"Plotting Inverted Values:\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "'''\n",
    "Reconstruct the original diffed columns\n",
    "'''    \n",
    "def reconstructOrig(conf, unnormdf, yh, ouputs):\n",
    "    yhdf, yidf = getInvertedPreds(conf, yh)\n",
    "    yorg,ips,ops = getOriginal(conf, unnormdf)\n",
    "    \n",
    "    for o in ouputs:\n",
    "        if(o.endswith(\"___diff1\")):\n",
    "            oc = o[:-8]\n",
    "            print(f\"Getting Original column for: '{oc}' \")\n",
    "            if ( oc not in yorg.columns):\n",
    "                print('Cannot compute the orginal column values from diffs for {oc}')\n",
    "                continue;\n",
    "                \n",
    "            ## WOW <== this is heavy - undo the diffing in the opposite way\n",
    "            yidf[oc] = yorg[oc].values + yidf[o].shift(-1)\n",
    "\n",
    "    return yidf # y inverted dataframe with adjusted cols for diffs\n",
    "\n",
    "\n",
    "\n",
    "#runMethod(\"gen.utils1.is_number\", **{\"s\":\"123.78\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile  ~/bin/plotutils1.py\n",
    "#!/usr/local/bin/python \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re, sys, os, datetime, glob, json, base64, pickle, sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (16, 3)\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"gen\")\n",
    "sys.path.append(\"/opt/LMCO/git/bin/gen\")\n",
    "import lstmutils1;\n",
    "import lstmfit;\n",
    "import sklearn.metrics\n",
    "\n",
    "def predict(modelFile, valg, model=None):\n",
    "    m1 = model or load_model(modelFile)\n",
    "    #xxt = np.array([valg[i][0][0] for i in range(len(valg))])\n",
    "    #yyt = np.array([valg[i][1][0] for i in range(len(valg))])\n",
    "    yh=m1.predict(valg)\n",
    "    return yh\n",
    "            \n",
    "def plot1(modelFile, valg, model=None, idx=0, n=-150, howmany=50):\n",
    "    yh = predict(modelFile, valg, model)\n",
    "    yy = np.array([valg[i][1][idx] for i in range(len(valg))])\n",
    "                 \n",
    "    plt.gcf().set_size_inches(22, 10, forward=True)\n",
    "    plt.plot( yy[n:n+howmany], marker='o', label=\"original-\")\n",
    "    plt.plot( yh[n:n+howmany], marker='x', label=\"predicted\")\n",
    "    mse = sklearn.metrics.mean_squared_error(yy, yh)\n",
    "    \n",
    "    plt.title(f\"{modelFile} : {model}: MSE: {mse} <==\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return yy, yh, mse\n",
    "\n",
    "\n",
    "def plotyyyh(columns, xxx, yy1, yh1, s, e):\n",
    "    for i in range(yh1.shape[-1]):\n",
    "        yyy = yy1[:,i]\n",
    "        yyh = yh1[:,i]\n",
    "\n",
    "        r2 = sklearn.metrics.r2_score(yyy, yyh)\n",
    "        if (r2 <0.5):\n",
    "            print(f'--{i}: {columns[i+1]} R^2: {r2}')\n",
    "            continue\n",
    "\n",
    "        plt.title(f'{i}: {columns[i+1]} R^2: {r2}')\n",
    "\n",
    "        plt.plot(xxx[s:e], yyy[s:e], marker='.', label=\"original\")\n",
    "        plt.plot(xxx[s:e], yyh[s:e], marker='x', label=\"predictd\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "def plotstuff(valg, confile, normeddf=None, model = None, mcpoint=None ):\n",
    "    if ( model is None):\n",
    "        conf, unnormdf, normeddf, inps, oups = lstmfit.getConf(confile)\n",
    "        model, mcpoint = lstmfit.getModel(conf)\n",
    "\n",
    "    print(len(valg), mcpoint.best)\n",
    "    \n",
    "    xxx = pd.to_datetime(normeddf.time[-len(valg):])\n",
    "    mcpoint.drawLosses()\n",
    "\n",
    "    yh1 = model.predict(valg)            \n",
    "    yy1 = np.array([valg[j][1][0] for j in range(len(valg))])\n",
    "\n",
    "    s = 0\n",
    "    e = s + len(valg)\n",
    "    plotyyyh(normeddf.columns, xxx, yy1, yh1, s, e)\n",
    "    \n",
    "    return model, mcpoint, normeddf, yy1, yh1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Configure  - dataprep.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /opt/utils/gen/dataprep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile  /opt/utils/gen/dataprep.py\n",
    "#!/usr/local/bin/python \n",
    "\n",
    "# DO NOT EDIT THIS FILE - GENERATED FROM: sada/NNBook/notebooks/NNetworks/LSTM/01_Utils.ipynb\n",
    "\n",
    "import re, sys, os, datetime, getopt, glob, argparse, datetime, json, base64, pickle,colabexts\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import sklearn.preprocessing\n",
    "from sklearn.preprocessing import *\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "#--------------------------------------------------------------------------------\n",
    "def runMethod(pyMethod, **kwargs):\n",
    "    spl = pyMethod.split('.');\n",
    "\n",
    "    assert len(spl) >= 2, \"Hmmm ... May be not what is intended!! module name\"\n",
    "\n",
    "    modName = \".\".join(spl[:-1])\n",
    "    __import__(modName, fromlist=\"dummy\")\n",
    "\n",
    "    funName = spl[-1]\n",
    "    ret = None\n",
    "    for v in sys.modules:\n",
    "        if (v.startswith(modName)):\n",
    "            method= getattr(sys.modules[v], funName)\n",
    "            print(v, type(v), funName, method, type(method), callable(method))\n",
    "            ret = method(**kwargs)\n",
    "    return ret\n",
    "\n",
    "#------ 1.--------------------------------------------------------------------------\n",
    "def dfselect(df, columns=[], **kwargs):\n",
    "    if not columns or len(columns) <= 0:\n",
    "        return df\n",
    "    df1 = df[columns]\n",
    "    \n",
    "    return df1\n",
    "#----- 2.---------------------------------------------------------------------------\n",
    "def drop_nonnumerics(df, **kwargs):\n",
    "    #cols = df.columns[df.dtypes.eq('object')]\n",
    "    cols = df.select_dtypes(exclude=np.number).columns\n",
    "    cols = [c for c in cols if not c.lower().startswith('time')]\n",
    "    if (len(cols) > 0):\n",
    "        print(f\"WARNING: *** Non numeric columns => {cols}: Dropping them\")\n",
    "        df.drop(cols, inplace=True, errors=\"ignore\")\n",
    "    else:\n",
    "        print(f\"+Good: no nonnumeric columns\")\n",
    "        \n",
    "    return df\n",
    "#----- 3.---------------------------------------------------------------------------\n",
    "# Drop any columns with less than 6 unique values\n",
    "#\n",
    "def drop_unique(df, unique=6, **kwargs):\n",
    "    unique_vals  = df.nunique()\n",
    "    cols         = unique_vals[ unique_vals <= unique].index\n",
    "\n",
    "    if (len(cols) > 0):\n",
    "        print(f\"WARNING: *** dropping columns having <= {unique} values => {cols}\")\n",
    "        df.drop(cols, inplace=True, errors=\"ignore\")\n",
    "    else:\n",
    "        print(f\"+Good: Nothing to drop\")\n",
    "        \n",
    "    return df\n",
    "    \n",
    "#----- 4a.---------------------------------------------------------------------------\n",
    "# Find any sensor highly correlated with time and drop them.\n",
    "def detectTimeCorrelated(df, timecol=\"time\", val=0.94, **kwargs):\n",
    "    timecol = df.columns[0]\n",
    "    \n",
    "    timeser = pd.Series(df[[timecol]].values.reshape(-1))\n",
    "    if ( timeser.dtype != np.number ):\n",
    "        timeser = pd.to_datetime(timeser).astype(int)\n",
    "    \n",
    "    \n",
    "    DROP_INDEX = 0; # Debugging\n",
    "    corcols    = []\n",
    "    for sensor in df.columns:\n",
    "        if (sensor == timecol ):\n",
    "            continue;\n",
    "        #print(f\"#Testing {sensor}...\")\n",
    "        # The following code tries to detect correlation by dropping first 8 or last 8 values\n",
    "        # sometimes dropping first few will show correlation due to start up times\n",
    "        sensorSeries = pd.Series(df[sensor].values.reshape(-1))\n",
    "        for i in range(8):\n",
    "            c1 = timeser[i:].corr(sensorSeries[i:])\n",
    "            c2 = timeser[i:].corr(sensorSeries[:-i])\n",
    "            if np.abs(c1) >= val or np.abs(c2) >= val:\n",
    "                corcols.append(sensor)\n",
    "                DROP_INDEX = max(DROP_INDEX, i) #lets drop first few rows\n",
    "                break;\n",
    "                \n",
    "    #print(f\"#Time Cor: #{len(timeCorSensors)}, #Shape before:{df.shape}\")\n",
    "    #df.drop(timeCorSensors, axis=1, inplace=True)\n",
    "    #df = df[DROP_INDEX:]\n",
    "    #print(f\"#After dropping: {DROP_INDEX} =>{df.shape}\")\n",
    "        \n",
    "    return corcols\n",
    "#----- 3.---------------------------------------------------------------------------\n",
    "# Drop any time correlated sensors\n",
    "#\n",
    "def drop_time_correlated(df, timecol=\"time\", corr=0.95, **kwargs):\n",
    "    cols = detectTimeCorrelated(df, timecol, corr)\n",
    "\n",
    "    if (len(cols) > 0):\n",
    "        print(f\"WARNING: Dropping time correlated columns having corr-coeffient >= {corr} => {cols}\")\n",
    "        df.drop(cols, axis=1, inplace=True, errors=\"ignore\")\n",
    "    else:\n",
    "        print(f\"+Good: No time correlated columns having corr-coeffient >= {corr}\")\n",
    "        \n",
    "    return df\n",
    "#-----------------------------------------------------------------------------------\n",
    "# Covert to one_hot encoding with prefix for columns'\n",
    "def make_OneHot(df, oheCols=[], **kwargs):\n",
    "    ohe = pd.DataFrame();\n",
    "    for c in oheCols:\n",
    "        one_hot = pd.get_dummies(df[c])\n",
    "        nc = [f'{c}___{k}' for k in one_hot.columns]\n",
    "        one_hot.columns = nc\n",
    "        ohe = pd.concat([ohe, one_hot], axis=1)\n",
    "\n",
    "    return ohe\n",
    "#-----------------------------------------------------------------------------------\n",
    "# Assuming the tf1 is sorted in ascending order of time\n",
    "def add_Diff(tf1, col=[], **kwargs):\n",
    "    if (type(col) == str):\n",
    "        col = [col]\n",
    "    for c in col:\n",
    "        if ( c not in tf1.columns):\n",
    "            print(f\"*WARNING* Column {c} Not FOUND\")\n",
    "            continue\n",
    "        print(f\"+++ Adding {c}\")\n",
    "        tf1[f'{c}___diff1'] = tf1[c] - tf1[c].shift(1)\n",
    "    return tf1\n",
    "#-----------------------------------------------------------------------------------\n",
    "# Assuming the tf1 is sorted in ascending order of time\n",
    "# column1 = 'MSFT_close'\n",
    "# column2 = 'AAPL_close'\n",
    "def add_corr(df, column1, column2, window  = 100, stride=1, **kwargs):\n",
    "    c1      = df[column1]\n",
    "    c2      = df[column2]\n",
    "    corr    = []\n",
    "    for i in range(0, len(c1) - window +1, stride):\n",
    "        cc1 = c1[i : i+window]\n",
    "        cc2 = c2[i : i+window]\n",
    "        cor = cc1.corr( cc2 )\n",
    "        corr.append(cor)\n",
    "    df = df[window:]    \n",
    "    df[column1+\"___\"+column2] = corr\n",
    "    return df\n",
    "#-----------------------------------------------------------------------------------\n",
    "def _convertObjToStr(obj):\n",
    "    astr = base64.b64encode(pickle.dumps(obj, protocol=None, fix_imports=True))\n",
    "    astr = astr.decode(\"utf-8\")\n",
    "    return astr\n",
    "def _convertStrToObj(astr):\n",
    "    astr = base64.b64decode(astr)\n",
    "    obj  = pickle.loads(decoded,fix_imports=True)\n",
    "    return obj\n",
    "#-----------------------------------------------------------------------------------\n",
    "def add_movingavg(df, cols = [], window=0, dropna=True, **kwargs):\n",
    "    for c in cols:\n",
    "        a = df[c].rolling(window=window).mean()\n",
    "        df[f'{c}__MOVING_AVG'] = a\n",
    "    if (dropna):\n",
    "        df.dropna(inplace=True)\n",
    "    #uni_data.plot(subplots=True)\n",
    "    #u3.plot(subplots=True, color='red')\n",
    "    return df;\n",
    "\n",
    "def add_expmovingavg(df, cols = [], window=0, dropna=True, **kwargs):\n",
    "    for c in cols:\n",
    "        a = df[c].ewm(span=window,adjust=False).mean()\n",
    "        df[f'{c}__EXP_MOVING_AVG'] = a\n",
    "    if (dropna):\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def data_fill(df,  **kwargs):\n",
    "    df = df.ffill().bfill()\n",
    "    df.dropna(inplace=True)\n",
    "        \n",
    "    return df\n",
    "#-----------------------------------------------------------------------------------\n",
    "# This will scale the numeric columns - if this changes - you need to use dataprep\n",
    "#    \"scaler\"         : [\"sklearn.preprocessing.StandardScaler()\"],\n",
    "#    \"scaler\"         : [\"sklearn.preprocessing.MinMaxScaler()\"],\n",
    "def data_scale(df, cols = [], start=0, pct=0.9, count=0, scaler=StandardScaler, **kwargs):\n",
    "    if ( type(scaler) == type ):\n",
    "        scaleri = scaler()\n",
    "    elif ( type(scaler) == str ):\n",
    "        scaleri = _convertStrToObj(scaler)\n",
    "    else:\n",
    "        scaleri = scaler\n",
    "    \n",
    "    \n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        cols = cols or [c for c in df.columns] \n",
    "        df1  = df[cols]\n",
    "    else:\n",
    "        df1  = df\n",
    "        \n",
    "    tCnt = count or int(len(df) * pct) or None\n",
    "        \n",
    "    sx1  = scaleri.fit(df1[start:tCnt])        # Fit only training part \n",
    "    di1  = scaleri.transform(df1)              # Tranform the entire df\n",
    "    dfi  = pd.DataFrame(di1, columns=cols )\n",
    "\n",
    "    return dfi, _convertObjToStr(scaleri);\n",
    "#-----------------------------------------------------------------------------------\n",
    "def process():\n",
    "    print(\"Dont know what to do!! NOW\")\n",
    "    \n",
    "#-----------------------------------------------------------------------------------\n",
    "sysargs=None\n",
    "def addargs():\n",
    "    global sysargs\n",
    "    p = argparse.ArgumentParser(f\"{os.path.basename(sys.argv[0])}:\")\n",
    "    p.add_argument('-c', '--config', type=str, default=\"config.txt\", help=\"Config Files\")\n",
    "    p.add_argument('-o', '--output', type=str, default=0, help=\"output file\")\n",
    "    p.add_argument('args', nargs=argparse.REMAINDER)\n",
    "    p.add_argument('input_files',action=\"store\", type=str, nargs='+', help=\"input file(s)\")\n",
    "\n",
    "    try:\n",
    "        sysargs, unknown=p.parse_known_args(sys.argv[1:])\n",
    "    except argparse.ArgumentError as exc:\n",
    "        print(exc.message )\n",
    "        \n",
    "    if (unknown):\n",
    "        print(\"Unknown options: \", unknown)\n",
    "        #p.print_help()\n",
    "    return sysargs    \n",
    "#-----------------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    if (not colabexts.jcommon.inJupyter()):\n",
    "        t1 = datetime.datetime.now()\n",
    "        sysargs = addargs()\n",
    "        ret = process()\n",
    "        t2 = datetime.datetime.now()\n",
    "        print(f\"#All Done in {str(t2-t1)} ***\")\n",
    "    else:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1,  2,  3],\n",
       "        [11, 21, 33],\n",
       "        [12, 22, 32],\n",
       "        [13, 23, 33]]),\n",
       " array([[ 1,  2],\n",
       "        [11, 21],\n",
       "        [12, 22],\n",
       "        [13, 23]]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[\n",
    "    [1,2,3],\n",
    "    [11,21,33],\n",
    "    [12,22,32],\n",
    "    [13,23,33],\n",
    "]\n",
    "a1=np.array(a)\n",
    "a1, a1[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 33, 32, 33])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%writefile  ~/bin/gen/dataconfig.py\n",
    "#!/usr/local/bin/python \n",
    "\n",
    "import re, sys, os, glob, argparse, datetime, json\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "'Covert to one_hot encoding with prefix for columns'\n",
    "def makeOneHotCols(tf1, oheCols=[]):\n",
    "    ret = []\n",
    "    for c in oheCols:\n",
    "        one_hot = pd.get_dummies(tf1[c])\n",
    "        ret += [f'{c}___{k}' for k in one_hot.columns]\n",
    "\n",
    "    return ret\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "def detectCols(file, nUnique=4, tcoeff=0.92):\n",
    "    tf1 = file\n",
    "    if (type(tf1) == str):\n",
    "        tf1 = pd.read_csv(tf1, comment=\"#\")\n",
    "    \n",
    "    #Lets check if it has any non-numeric columns! Warning\n",
    "    precheck(tf1)\n",
    "\n",
    "    unique_vals  = tf1.nunique()\n",
    "    constantCols = unique_vals[ unique_vals == 1].index                             # constant Columns\n",
    "    onehotECols  = unique_vals[(unique_vals > 2 ) & (unique_vals<=nUnique)].index   # Categorical Columns\n",
    "    categorCols  = unique_vals[(unique_vals >=2 ) & (unique_vals <= nUnique)].index # Categorical Columns\n",
    "    binaryCols   = unique_vals[(unique_vals == 2)].index                      # Binary\n",
    "\n",
    "    numericCols  = tf1.select_dtypes(include=np.number).columns           # numerics\n",
    "    numericCols  = [c for c in numericCols if c not in categorCols]\n",
    "    numericCols  = [c for c in numericCols if c not in constantCols]\n",
    "    numericCols  = [c for c in numericCols if c != 'time']\n",
    "    notNumerics  = tf1.select_dtypes(exclude=np.number).columns           # non - numerics\n",
    "    notNumerics  = [c for c in notNumerics if c not in categorCols]       # non - numerics\n",
    "\n",
    "    try:\n",
    "        pass #timeCorCols  = detectTimeCorrelated(tf1, tcoeff)\n",
    "    except:\n",
    "        timeCorCols = []\n",
    "    \n",
    "    onehotEC_ext = makeOneHotCols(tf1, onehotECols)\n",
    "   \n",
    "    ret1 =f'''[START]\n",
    "{{\n",
    "    \"file\"           : {[file] if (type(file) == str) else [\"??\"]},\n",
    "    \"nrowsXncols\"    : {[len(tf1), len(tf1.columns )] }     , \n",
    "    \"number_Unique\"  : {nUnique}            , \n",
    "    \"constantCols\"   : {list(constantCols )},   # No Signals\n",
    "    \"#constantCols\"  : {len(constantCols  )},   # No Signals\n",
    "    \"categorCols\"    : {list(categorCols  )},   # Categorical Columns\n",
    "    \"#categorCols\"   : {len(categorCols   )},   # Categorical Columns\n",
    "    \"onehotECols\"    : {list(onehotECols  )},   # Cats > 2 and < Unique Values\n",
    "    \"onehotEC_ext\"   : {list(onehotEC_ext )},   # Cats > 2 and < Unique Values\n",
    "    \"#onehotECols\"   : {len(onehotECols   )},   # Cats > 2 and < Unique Values\n",
    "    \"binaryCols\"     : {list(binaryCols   )},   # Binary\n",
    "    \"#binaryCols\"    : {len(binaryCols    )},   # Binary\n",
    "    \"notNumerics\"    : {list(notNumerics  )},\n",
    "    \"timeCorrelation\": {tcoeff             },   # Time correlated\n",
    "    \"timeCorrCols\"   : {list(timeCorCols  )},   # Time correlated Columns\n",
    "    \"#timeCorrCols\"  : {len(timeCorCols   )},    # Time correlated Columns\n",
    "    \"excludePattern\" : [] , #Exclude patterns\n",
    "    \"includePattern\" : [] , #include patterns\n",
    "    \"dropColumns\"    : [],\n",
    "    \"diff_suffix\"    : {['__diff1']},\n",
    "    \"addDiffs\"       : [],\n",
    "    \"train_pct\"      : .9,\n",
    "    \"#numericCols\"   : {len(numericCols   )},  \n",
    "    \"scaleInputs\"    : {list(numericCols  )},  \n",
    "    \"scaleOutputs\"   : {[\"$scaleInputs\"]},  \n",
    "    \"inputs\"         : {[\"$binaryCols\", \"$scaleInputs\", \"$onehotECols\"]},\n",
    "    \"outputs\"        : {[\"$scaleOutputs\"]},\n",
    "#-----Copy this generated file and add customization\n",
    "    \"loadModel\"      : 1,\n",
    "    \"scale\"          : 1,\n",
    "    \"scaler\"         : [\"sklearn.preprocessing.StandardScaler()\"],\n",
    "    \"scaler\"         : [\"sklearn.preprocessing.MinMaxScaler()\"],\n",
    "    \"scalerXString\"  : [],\n",
    "    \"scalerYString\"  : [],\n",
    "    \"tsParams\"       : {{\"length\": 60, \"batch_size\": 1, \"stride\": 1, \"sampling_rate\": 1}},\n",
    "    \"lookahead\"      : 60,\n",
    "    \"nsteps\"         : 1,\n",
    "    \"modelFile\"      : [\"lstm.56.h5\"],\n",
    "    \"monitor\"        : \"val_loss\",\n",
    "    \"modelName\"      : \"gen.somemodels.SimpleModel1(50, 5, 1, **{{}})\"\n",
    "}}\n",
    "[END]\n",
    "    '''\n",
    "    return ret1, tf1;\n",
    "#-----------------------------------------------------------------------------------    \n",
    "def process():\n",
    "    n  = len(sysargs.input_files)\n",
    "    un = sysargs.unique\n",
    "    tc = sysargs.tcoeff\n",
    "    for i, file1 in enumerate(sysargs.input_files):\n",
    "        print(f\"#=>Processing {i+1}/{n} {file1} #unique: {un} tcoeff: {tc} - standby\")\n",
    "        outs, df = detectCols(file1, un, tc)\n",
    "        \n",
    "        break;\n",
    "    print(outs)\n",
    "    return outs\n",
    "    \n",
    "#-----------------------------------------------------------------------------------\n",
    "sysargs=None\n",
    "def addargs():\n",
    "    sysargs = None\n",
    "    p = argparse.ArgumentParser(f\"{os.path.basename(sys.argv[0])}:\")\n",
    "    p.add_argument('-u', '--unique', type=int,   default=6,    help=\"# of unique values!\")\n",
    "    p.add_argument('-t', '--tcoeff', type=float, default=0.94, help=\"# Time Correlation value Sensors!\")\n",
    "    p.add_argument('args', nargs=argparse.REMAINDER)\n",
    "    p.add_argument('input_files',action=\"store\", type=str, nargs='+', help=\"input file(s)\")\n",
    "\n",
    "    #p.print_help() # always print -help\n",
    "    try:\n",
    "        sysargs=p.parse_args(sys.argv[1:])\n",
    "        #print(f'using:\\n{sysargs}')\n",
    "    except argparse.ArgumentError as exc:\n",
    "        #par.print_help()\n",
    "        print(exc.message )\n",
    "        \n",
    "    return sysargs\n",
    "#-----------------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    if (not inJupyter()):\n",
    "        t1 = datetime.datetime.now()\n",
    "        sysargs = addargs()\n",
    "        process()\n",
    "        t2 = datetime.datetime.now()\n",
    "        print(f\"#All Done in {str(t2-t1)} ***\")\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile  ~/bin/gen/dataprepare.py\n",
    "#!/usr/local/bin/python \n",
    "\n",
    "import re, sklearn, sys, os, datetime, getopt, glob, argparse, datetime, json, base64, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"gen\")\n",
    "sys.path.append(\"~/bin/gen\")\n",
    "import lstmutils1;\n",
    "\n",
    "'''\n",
    "Make sure data is sorted in asceding order of the time for LSTM to work and all these \n",
    "data prep tools to work.\n",
    "'''\n",
    "\n",
    "\n",
    "'Covert to one_hot encoding with prefix for columns'\n",
    "def makeOneHot(tf1, oheCols=[]):\n",
    "    ohe = pd.DataFrame();\n",
    "    for c in oheCols:\n",
    "        one_hot = pd.get_dummies(tf1[c])\n",
    "        nc = [f'{c}___{k}' for k in one_hot.columns]\n",
    "        one_hot.columns = nc\n",
    "        ohe = pd.concat([ohe, one_hot], axis=1)\n",
    "\n",
    "    return ohe\n",
    "\n",
    "'''\n",
    "Assuming the tf1 is sorted in ascending order of time\n",
    "'''\n",
    "def addDiff(tf1, col):\n",
    "    #col = \"MSFT_open\"\n",
    "    if (type(col) == str):\n",
    "        col = [col]\n",
    "    for c in col:\n",
    "        if ( c not in tf1.columns):\n",
    "            print(f\"*WARNING* Column {c} Not FOUND\")\n",
    "            continue\n",
    "        print(f\"+++ Adding {c}\")\n",
    "        tf1[f'{c}___diff1'] = tf1[c] - tf1[c].shift(1)\n",
    "    return tf1\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "def formatConfig(out: dict):\n",
    "    outj = f\"[START]\\n{{\\n\"\n",
    "    for k,v in out.items():\n",
    "        kk = f'\"{k}\"'\n",
    "        vv = f\"'{v}'\" if type(v) == str else v\n",
    "            \n",
    "        outj += f'{kk:>20}: {vv},\\n'\n",
    "    outj += '\"end\": 0 \\n}\\n[END]\\n'\n",
    "    \n",
    "    return outj\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "def getFinalColumns(df, conf):\n",
    "    inputCols = lstmutils1.getConfigList(conf, \"inputs\")\n",
    "    ouputCols = lstmutils1.getConfigList(conf, \"outputs\")\n",
    "\n",
    "    allcols = set(inputCols +ouputCols);\n",
    "    \n",
    "    assert allcols.issubset(set(df.columns)), \"Hmmm ... columns missing\"\n",
    "    return sorted(list(allcols))\n",
    "#-----------------------------------------------------------------------------------\n",
    "'''\n",
    "This will scale the numeric columns - if this changes - you need to use dataprep\n",
    "'''\n",
    "def scaleNumerics(df, conf={}):\n",
    "    scaleInputCols = lstmutils1.getConfigList(conf, 'scaleInputs')\n",
    "    scaleOuputCols = lstmutils1.getConfigList(conf, 'scaleOutputs')\n",
    "    \n",
    "    dfninps = df[ scaleInputCols ]\n",
    "    dfnouts = df[ scaleOuputCols ]\n",
    "\n",
    "    scale   = conf.get('scale', 0)\n",
    "    dfninpsn, dfnoutsn, scalerX, scalerY = dfninps, dfnouts, None, None\n",
    "    \n",
    "    if (scale):\n",
    "        scalerXStr = conf['scalerXString']\n",
    "        scalerYStr = conf['scalerYString']\n",
    "        trnPct     = conf.get('train_pct', 0.9);\n",
    "        trnCnt     = int(len(df) * trnPct)\n",
    "\n",
    "        conf[\"train_pct\"   ] = trnPct\n",
    "        conf[\"train_count\" ] = trnCnt\n",
    "\n",
    "        if (not scalerXStr):\n",
    "            scaler  = conf.get('scaler', [\"sklearn.preprocessing.MinMaxScaler()\"]);\n",
    "            scalerX = eval(scaler[0]) if type(scaler[0]) == str else scaler\n",
    "            scalerX = scalerX.fit(dfninps[:trnCnt])\n",
    "            scalerstr = base64.b64encode(pickle.dumps(scalerX, protocol=None, fix_imports=True))\n",
    "            scalerstr = scalerstr.decode(\"utf-8\")\n",
    "            conf[\"scalerXString\"] = [scalerstr]\n",
    "            #print(f'==>+1 shape: {dfninps.shape} {scalerX.mean_}')\n",
    "        else:\n",
    "            scalerXStr = scalerXStr[0]\n",
    "            decoded    = base64.b64decode(scalerXStr)\n",
    "            scalerX    = pickle.loads(decoded,fix_imports=True)\n",
    "\n",
    "        if (not scalerYStr):\n",
    "            scaler  = conf.get('scaler', [\"sklearn.preprocessing.MinMaxScaler()\"]);\n",
    "            scalerY = eval(scaler[0]) if type(scaler[0]) == str else scaler\n",
    "            scalerY = scalerY.fit(dfnouts[:trnCnt])\n",
    "            scalerstr = base64.b64encode(pickle.dumps(scalerY, protocol=None, fix_imports=True))\n",
    "            scalerstr = scalerstr.decode(\"utf-8\")\n",
    "            conf[\"scalerYString\"] = [scalerstr]\n",
    "            #print(f'==>+2 shape: {dfninps.shape} {scalerY.mean_}')\n",
    "        else:\n",
    "            scalerYStr = scalerYStr[0]\n",
    "            decoded    = base64.b64decode(scalerYStr)\n",
    "            scalerY    = pickle.loads(decoded,fix_imports=True)\n",
    "\n",
    "        di = scalerX.transform(dfninps)\n",
    "        do = scalerY.transform(dfnoutsn)\n",
    "        \n",
    "        dfninpsn = pd.DataFrame(di, columns=scaleInputCols )\n",
    "        dfnoutsn = pd.DataFrame(do, columns=scaleOuputCols )\n",
    "        \n",
    "        #print(f'==>++ shape: {dfninpsn.shape} {scalerX.mean_}')\n",
    "        #print(f'==>++ shape: {dfnoutsn.shape} {scalerY.mean_}')\n",
    "\n",
    "    return dfninpsn, dfnoutsn, scalerX, scalerY;\n",
    "#-----------------------------------------------------------------------------------\n",
    "def process(config, input_files, output=None):\n",
    "    conf = lstmutils1.getconfig(config)\n",
    "    \n",
    "    n  = len(input_files)\n",
    "    adfInp = pd.DataFrame();\n",
    "    for i, file1 in enumerate(input_files):\n",
    "        print(f\"=>Processing {i+1}/{n} {file1} - standby\")\n",
    "        df = pd.read_csv(file1, comment='#')\n",
    "        \n",
    "        drps = lstmutils1.getConfigList(conf, \"dropColumns\")\n",
    "        df.drop(drps, inplace=True, errors=\"ignore\")\n",
    "        \n",
    "        # STEP 1: Add diffs\n",
    "        cdiffs  = lstmutils1.getConfigList(conf, 'addDiffs')\n",
    "        addDiff(df, cdiffs)    #<< 1. Add tis\n",
    "        df.dropna(inplace=True)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        # STEP 2: => One hot encode \n",
    "        ohe = None\n",
    "        ohecols = lstmutils1.getConfigList(conf, 'onehotECols')\n",
    "        if len(ohecols) > 0:\n",
    "            ohe=makeOneHot(df, conf['onehotECols'])  #< === ADD\n",
    "            df=pd.concat([df,ohe], axis=1)\n",
    "         \n",
    "        # STEP 3: Add\n",
    "        allCols = [df.columns[0]] + getFinalColumns(df,conf)\n",
    "        dfunNormalized = df[allCols]\n",
    "        #Numeric Columns\n",
    "        dfiNorm, dfoNorm, sX, sY = scaleNumerics(dfunNormalized, conf)\n",
    "        \n",
    "        dfNormalized = dfunNormalized.copy()\n",
    "        dfNormalized[dfiNorm.columns] = dfiNorm;\n",
    "        dfNormalized[dfoNorm.columns] = dfoNorm;\n",
    "        \n",
    "        #FINALLY        \n",
    "        if (output is not None):\n",
    "            fi,ext = os.path.splitext(file1)\n",
    "            nfu  = f'{os.path.basename(fi)}_Orig_{i}{ext}'\n",
    "            print(f\"writing unnormalized to: {nfu}\")\n",
    "            dfunNormalized.to_csv(nfu, index=False)\n",
    "            \n",
    "            \n",
    "            nfn  = f'{os.path.basename(fi)}_Norm_{i}{ext}'\n",
    "            print(f\"writing normalized to. : {nfn}\")\n",
    "            dfNormalized.to_csv  (nfn, index=False)\n",
    "            \n",
    "            conf['normalizedFile']   = nfn\n",
    "            conf['unnormalizedFile'] = nfu\n",
    "            \n",
    "        break;\n",
    "\n",
    "    outj = formatConfig(conf)\n",
    "    print(outj)\n",
    "    \n",
    "    return conf, dfunNormalized, dfNormalized\n",
    "#-----------------------------------------------------------------------------------\n",
    "sysargs=None\n",
    "def addargs():\n",
    "    sysargs = None\n",
    "    p = argparse.ArgumentParser(f\"{os.path.basename(sys.argv[0])}:\")\n",
    "    p.add_argument('-c', '--config', type=str, default=\"config.txt\", help=\"Config Files\")\n",
    "    p.add_argument('-o', '--output', type=str, default=0, help=\"output file\")\n",
    "    p.add_argument('args', nargs=argparse.REMAINDER)\n",
    "    p.add_argument('input_files',action=\"store\", type=str, nargs='+', help=\"input file(s)\")\n",
    "\n",
    "    #p.print_help() # always print -help\n",
    "    try:\n",
    "        sysargs=p.parse_args(sys.argv[1:])\n",
    "    except argparse.ArgumentError as exc:\n",
    "        print(exc.message )\n",
    "        \n",
    "    return sysargs\n",
    "    \n",
    "#-----------------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    if (not inJupyter()):\n",
    "        t1 = datetime.datetime.now()\n",
    "        sysargs = addargs()\n",
    "        ret = process(sysargs.config, sysargs.input_files, sysargs.output)\n",
    "        t2 = datetime.datetime.now()\n",
    "        print(f\"#All Done in {str(t2-t1)} ***\")\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f='/opt/SCHAS/NNBook/notebooks/NNetworks/LSTM/data/stockdata_ext.csv'\n",
    "f='/opt/SCHAS/NNBook/notebooks/NNetworks/LSTM/data/daily_MSFT.csv'\n",
    "conf, dfUnNormalized, dfNormalized = process('config.*', [f], \"out.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Callback for Saving and Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile  ~/bin/gen/ccallbacks.py\n",
    "#!/usr/local/bin/python \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re, sys, os, datetime, glob, json, base64, pickle, sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "sys.path.append(\"gen\")\n",
    "sys.path.append(\"/opt/LMCO/git/bin/gen\")\n",
    "import lstmutils1;\n",
    "import sklearn.metrics\n",
    "\n",
    "class ModelCheckAndLoad(Callback):\n",
    "    def __init__(self, filepath, monitor='val_loss', best=np.inf, \n",
    "                 stop_at=False, verbose=0, drawLoss=False):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor  = monitor\n",
    "        self.filepath = filepath\n",
    "        self.verbose  = verbose\n",
    "        self.best     = best or np.inf\n",
    "        self.stop_at  = stop_at;\n",
    "        self.history  = {}\n",
    "        self.epochs   = []\n",
    "        self.drawLoss = drawLoss\n",
    "        self.epochNum = 0\n",
    "        self.numSaved = 0\n",
    "        \n",
    "    def save_ext(self):\n",
    "        ef = self.filepath+\"_ext\"\n",
    "        with open(ef, \"wb\") as f:\n",
    "            myParams = {\n",
    "                'best'     : self.best,\n",
    "                'bestEpoch': self.bestEpoch,\n",
    "                'epochNum' : self.epochNum,\n",
    "                'history'  : self.history,\n",
    "                'monitor'  : self.monitor\n",
    "            }\n",
    "            pickle.dump(myParams, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "    def save_latest(self):\n",
    "        self.model.save(self.filepath+\"_latest\", overwrite=True)\n",
    "        self.save_ext();\n",
    "            \n",
    "    def load_ext(self):\n",
    "        ret = None;\n",
    "        if ( os.path.exists(self.filepath+\"_latest\")):\n",
    "            ret = load_model(self.filepath+\"_latest\")\n",
    "            print(\"Loading from the latest:...\")\n",
    "        elif ( os.path.exists(self.filepath)):\n",
    "            ret = load_model(self.filepath)\n",
    "        \n",
    "        ef = self.filepath+\"_ext\"\n",
    "        if ( not os.path.exists(ef) or os.path.getsize(ef) <= 0):\n",
    "            return ret\n",
    "        \n",
    "        with open(ef, \"rb\") as f:\n",
    "            myParams      = pickle.load(f)\n",
    "            self.best     = myParams.get('best'    , np.inf)\n",
    "            self.epochNum = myParams.get('epochNum', 0);\n",
    "            self.history  = myParams.get('history', {});\n",
    "            self.monitor  = myParams.get('monitor'  , \"val_loss\");\n",
    "            \n",
    "        print(f\"Best Loaded {self.best} occured at: {self.epochNum}\")\n",
    "        return ret;\n",
    "\n",
    "    def drawLosses(self):\n",
    "        history, best = self.history, self.best\n",
    "        #IPython.display.clear_output(wait=True)\n",
    "        plt.clf()\n",
    "\n",
    "        fig, ax1 = plt.subplots()\n",
    "        i, colors, marks = 0, \"rgbcmykw\", \"v.xo+\"\n",
    "\n",
    "        color = colors[i]\n",
    "        ax1.set_xlabel('epochs')\n",
    "        k, v = \"loss\", history['loss']\n",
    "        ax1.set_ylabel(k, color=color)\n",
    "        l1= ax1.plot(v, color=color, marker=marks[i], label=f\"{k}\")\n",
    "\n",
    "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "        i +=1\n",
    "        k, v = \"val_loss\", history['val_loss']\n",
    "        color = colors[i]\n",
    "        ax2.set_ylabel(k, color=color)\n",
    "        l2 = ax2.plot(v, color=color, marker=marks[i], label=f\"{k}\")\n",
    "\n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        l3 = plt.plot(0, best, marker=\"o\",  c=\"b\", label=f\"BEST: {best}\")\n",
    "        ax1.grid()\n",
    "\n",
    "        lns  = l1 + l2 + l3;\n",
    "        labs = [l.get_label() for l in lns]\n",
    "        plt.legend(lns, labs, loc=0)\n",
    "        plt.show()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.epochs.append(epoch)\n",
    "        self.epochNum += 1;\n",
    "        \n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        self.current = logs.get(self.monitor)\n",
    "        if self.current is None:\n",
    "            warnings.warn(f'Can save best model only with {self.monitor} available')\n",
    "            return;\n",
    "                    \n",
    "        if (self.best > self.current):\n",
    "            ou= f'{self.monitor}: {self.best} > {self.current}\\n'\n",
    "            print(f\"Epoch: {epoch+1} Saving: {ou}\");\n",
    "            \n",
    "            self.numSaved += 1\n",
    "            self.bestEpoch+= 1\n",
    "            self.best      = self.current\n",
    "            self.model.save(self.filepath, overwrite=True)\n",
    "            self.save_ext();\n",
    "            self.model.stop_training = self.stop_at\n",
    "        elif self.verbose > 0:\n",
    "            ou= f'{self.monitor}: {self.best} <= {self.current}'\n",
    "            print(f\"{epoch+1} din't improve : {ou} from {self.bestEpoch}\\r\", end=\"\")\n",
    "            \n",
    "        if (self.drawLoss):\n",
    "            drawLosses(self.history, self.best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of Some Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile  ~/bin/gen/LSTMModelDefs.py\n",
    "#!/usr/local/bin/python \n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate\n",
    "from keras.layers import RepeatVector, TimeDistributed\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras import optimizers\n",
    "\n",
    "def SimpleModel1(history, nfeatures, nOut, **kwargs) :\n",
    "    lstm_input = Input(shape=(history, nfeatures), name='lstm_input')\n",
    "    x = LSTM(50, name='lstm_0')(lstm_input)\n",
    "    x = Dropout(0.2, name='lstm_dropout_0')(x)\n",
    "    x = Dense(64, name='dense_0')(x)\n",
    "    x = Activation('sigmoid', name='sigmoid_0')(x)\n",
    "    x = Dense(nOut, name='dense_1')(x)\n",
    "    output = Activation('linear', name='linear_output')(x)\n",
    "\n",
    "    model = Model(inputs=lstm_input, outputs=output)\n",
    "    adam = optimizers.Adam(lr=0.0005)\n",
    "    model.compile(optimizer=adam, loss='mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def SimpleModel2(inps, inshape, units2=None, nsteps=1, opt=\"adam\", loss=\"mse\", bi=False, dropout=None):\n",
    "    s= inshape\n",
    "    print(locals())\n",
    "    print(f\"Creating LSTM: inuts= {inps} time-steps: {s[0]}, features: {s[1]} #out: {nsteps}\")\n",
    "    m = keras.models.Sequential()\n",
    "\n",
    "    if (bi):\n",
    "        m.add(keras.layers.Bidirectional(\n",
    "            keras.layers.LSTM(inps, return_sequences= (units2 is not None), input_shape=s) ) )\n",
    "    else:\n",
    "        m.add(keras.layers.LSTM(inps, return_sequences= (units2 is not None), input_shape=s) )\n",
    "    \n",
    "    if(units2 is not None): #Lets just keep it simple for 2 layers only\n",
    "        m.add(keras.layers.LSTM(units2, activation='relu'))\n",
    "    if (dropout is not None):\n",
    "        m.add( keras.layers.Dropout(dropout) )\n",
    "    m.add(keras.layers.Dense(nsteps))\n",
    "    m.compile(optimizer = opt, loss= loss)\n",
    "    return m\n",
    "\n",
    "def UberModel(lookBack, nFeatures, lstm_IPDim=256, lstm_OPDim=1, opt=None, loss=\"rmse\",  drop=0.3):\n",
    "    opt        = opt or optimizers.Adam(lr=0.0005)\n",
    "    k_rrizer   = None\n",
    "    r_rrizer   = None\n",
    "\n",
    "    input_layer  = Input(shape=(lookBack, nFeatures), dtype='float32', name='input')\n",
    "    memory_layer = LSTM( lstm_IPDim, return_sequences=True, name=\"memory1\")(input_layer)\n",
    "    memory_layer = LSTM (int(lstm_IPDim/2), return_sequences=False, name=\"memory2\")(memory_layer)\n",
    "    repeated     = RepeatVector(lookBack)(memory_layer)\n",
    "    memory_layer = LSTM (int(lstm_IPDim/2), return_sequences=True, name=\"first1out\")(repeated)\n",
    "    memory_layer = LSTM (lstm_IPDim,  return_sequences=True, name=\"first2out\")(memory_layer)\n",
    "    decoded_inputs = TimeDistributed(Dense(units=lstm_OPDim, activation='linear'))( memory_layer)\n",
    "\n",
    "    #  Try spatial dropout?\n",
    "    dropout_input = Dropout(drop)(input_layer)\n",
    "    concat_layer  = concatenate([dropout_input, decoded_inputs])\n",
    "\n",
    "    memory_layer = LSTM (units=lstm_IPDim, \n",
    "                             kernel_regularizer = k_rrizer, \n",
    "                             recurrent_regularizer = r_rrizer, \n",
    "                             return_sequences=False)(concat_layer)\n",
    "    preds = Dense(units=lstm_OPDim, activation='linear')(memory_layer)\n",
    "\n",
    "    model1 = Model(input_layer, preds)\n",
    "    model1.compile(optimizer = opt, loss= loss)             \n",
    "\n",
    "    return model1"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
