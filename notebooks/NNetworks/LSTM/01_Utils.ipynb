{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<textarea rows=5 id=nav_head_content style=\"width:100%;display:none;\">\n",
       "<style>\n",
       "a.bh,  a.bh:visited, a.bh:link, a.bh:active {\n",
       "  ttext-decoration: none; \n",
       "  color: black;\n",
       "  font-family: 'Roboto','Heebo',Arial,sans-serif;\n",
       "  font-size: 3em\n",
       "}\n",
       "</style>\n",
       "<div id='HTMLTopBar' style=\"    \n",
       "    z-index: 50;\n",
       "    align-items: stretch;\n",
       "    width:100%\">\n",
       "    <div  style=\"\n",
       "        text-color: black;\n",
       "        background-color: #fefefe;\n",
       "        bborder-bottom: 1px dotted gray;\n",
       "        padding-left: 2px;\n",
       "        box-shadow: 5px 1px #ccc;\n",
       "        height: 40px; left: 0; \n",
       "        padding: 14px;\n",
       "        \"\n",
       "    >\n",
       "    <a class=bh1 href=\"#\" onclick=\"$('#maintoolbar').toggle();\">X</a>\n",
       "</div>\n",
       "</textarea>\n",
       "\n",
       "<script>\n",
       "if ($('#nav_head').length < 1) {\n",
       "    $('#notebook-container').prepend('<div id=\"nav_head\" style=\"width:100%;\">.</div>')\n",
       "    console.log(\"Added a div\")\n",
       "} else{\n",
       "    console.log(\"Already Added\")    \n",
       "}\n",
       "\n",
       "$('#nav_head').html($('#nav_head_content').val())\n",
       "\n",
       "$('.nbp-app-bar').toggle()\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url('https://fonts.googleapis.com/css2?family=Roboto&display=swap');\n",
       "</style>\n",
       "<style>\n",
       "<link href=\"https://fonts.googleapis.com/css2?family=Roboto&display=swap\" \n",
       "                            rel=\"stylesheet\">\n",
       "\n",
       "body  p ol li {\n",
       "    font-family: \"Roboto\",  \"Lucida Grande\", \"Lucida Sans Unicode\";\n",
       "    font-size: 14px;\n",
       "    background: #fff\n",
       "}\n",
       "\n",
       "h1, h2, h3{\n",
       "    font-family: 'Roboto', sans-serif;\n",
       "}\n",
       "div#notebook_panel div#notebook{\n",
       "    background: #ffffff\n",
       "}\n",
       "div#notebook-container{\n",
       "    box-shadow: 0px;\n",
       "    padding: 0px;\n",
       "    border-left: .05em dotted gray;\n",
       "    -webkit-box-shadow: 0px;\n",
       "    box-shadow: none;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".container { width: 100% !important; }\n",
       "    div.cell{\n",
       "        width:100%;\n",
       "        margin-left:0%;\n",
       "        margin-right:auto;\n",
       "}\n",
       ".CodeMirror {\n",
       "    font-family: monospace;\n",
       "}\n",
       "div.input_area {\n",
       "    border: 0px;\n",
       "}\n",
       "div.cell.selected{\n",
       "  border: '0px';\n",
       "}\n",
       ".cell.selected{\n",
       "  border: '0px';\n",
       "}\n",
       "div#notebook{\n",
       "    padding-top: 0px;\n",
       "}\n",
       "div.prompt_container {\n",
       "    display: block; \n",
       "    background: #f7f7f7;\n",
       "}\n",
       "div.prompt{\n",
       "    min-width:0px;\n",
       "    display: grid;\n",
       "}\n",
       "div.cell{\n",
       "    padding-bottom: 5px;\n",
       "    padding-left: 0px;\n",
       "}\n",
       "\n",
       "a.bh,  a.bh:visited, a.bh:link, a.bh:active {\n",
       "  text-decoration: none; \n",
       "  color:white;\n",
       "  font-family: 'Roboto','Heebo',Arial,sans-serif;\n",
       "  font-size:1em\n",
       "}\n",
       "a.bh:hover {\n",
       "  color: #ffccdd;\n",
       "}\n",
       "\n",
       "a.bh1,  a.bh1:visited, a.bh1:link, a.bh1:active {\n",
       "  text-decoration: none; \n",
       "  color: rgba(0, 0, 0, 0.87);\n",
       "  font-family: 'Roboto','Heebo',Arial,sans-serif;\n",
       "  padding-right: 30px;\n",
       "  font-size:1.1em\n",
       "}\n",
       "a.bh1:hover {\n",
       "  color: rgba(0, 0, 0, 0.57);\n",
       "}\n",
       "    \n",
       "#toc-wrapper {\n",
       "   font-family: 'Roboto','Heebo',Arial,sans-serif;\n",
       "    border: 0px dotted gray;\n",
       "    padding: 10px;\n",
       "    line-height: 1.8em;\n",
       "}    \n",
       "</style>\n",
       "<script>\n",
       "l=\"https://www.ancient-symbols.com/images/wp-image-library/fullsize/infinity.jpg\"\n",
       "l=\"imgs/logo.png\"\n",
       "l=\"\"\n",
       "$('#ipython_notebook').html(`<img src=\"${l}\">`)\n",
       "\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run basic.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Utilities \n",
    "\n",
    "This will be included in all the files as generic utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile  ~/bin/gen/lstmutils1.py\n",
    "#!/usr/local/bin/python \n",
    "'''\n",
    "Some utility Functions to be used in all the apps\n",
    "#=*** NOTE *** DO NOT EDIT THIS FILE - THIS iS CREATED FROM: 01_utils.ipynb\n",
    "'''\n",
    "import re, sklearn, sys, os, datetime, glob, argparse, json, base64, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (16, 4)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "pd.options.display.max_rows = 8\n",
    "\n",
    "sys.path.append(\"~/bin/gen\")\n",
    "import ccallbacks\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "def inJupyter():\n",
    "    try:    get_ipython; return True\n",
    "    except: return False\n",
    "\n",
    "def getconfig(cf = \"config*\"):\n",
    "    confFiles = sorted(glob.glob(cf))\n",
    "    if ( len(confFiles) <= 0):\n",
    "        print(f\"No Configuration files {cf} found!!!\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Read and merge the configuration files\n",
    "    ret = {}\n",
    "    for cf in confFiles:\n",
    "        print(f\"#Getting Configuration from {cf}\")\n",
    "        with open(cf, \"r\") as f:\n",
    "            cf = f.read()\n",
    "\n",
    "        if (not cf.find('[START]') >=0 ):\n",
    "            r1 = cf\n",
    "        else:\n",
    "            r1=re.findall(\"\\[START](.*)\\[END]\", cf, flags=re.MULTILINE|re.DOTALL)\n",
    "            if ( len(r1) <= 0):\n",
    "                print(f\"Ignoring: Configuration not found in {cf}! no worries\")\n",
    "                continue\n",
    "            r1 = r1[0].replace(\"'\", '\"')    \n",
    "        rj = eval(r1)\n",
    "        ret.update(rj)\n",
    "                \n",
    "    return ret\n",
    "    \n",
    "#-----------------------------------------------------------------------------------\n",
    "def getConfigList(conf, key=\"\"):\n",
    "    #print(f\"Getting {key}\")\n",
    "    ll = conf.get(key, []);\n",
    "    ret = []\n",
    "    for l in ll:\n",
    "        if type(l) == list:\n",
    "            ret += l\n",
    "        elif l.startswith(\"$\"):\n",
    "            ret += getConfigList(conf, l[1:])\n",
    "        else:\n",
    "            ret.append(l)\n",
    "\n",
    "    return ret\n",
    "#-----------------------------------------------------------------------------------\n",
    "def getConfigObject(conf, key=\"\"):\n",
    "    ll = conf.get(key, \"\");\n",
    "    if not ll:\n",
    "        return;\n",
    "    \n",
    "    sst = ll[0]\n",
    "    dec = base64.b64decode(sst)\n",
    "    ret = pickle.loads(dec, fix_imports=True)\n",
    "\n",
    "    return ret\n",
    "#--------------------------------------------------------------------------------\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "#--------------------------------------------------------------------------------\n",
    "class Map(dict):\n",
    "    \"\"\"\n",
    "    Example:\n",
    "    m = Map({'first_name': 'Eduardo'}, last_name='Pool', age=24, sports=['Soccer'])\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Map, self).__init__(*args, **kwargs)\n",
    "        for arg in args:\n",
    "            if isinstance(arg, dict):\n",
    "                for k, v in arg.items():\n",
    "                    self[k] = v\n",
    "\n",
    "        if kwargs:\n",
    "            for k, v in kwargs.iteritems():\n",
    "                self[k] = v\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        return self.get(attr)\n",
    "\n",
    "    def __setattr__(self, key, value):\n",
    "        self.__setitem__(key, value)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        super(Map, self).__setitem__(key, value)\n",
    "        self.__dict__.update({key: value})\n",
    "\n",
    "    def __delattr__(self, item):\n",
    "        self.__delitem__(item)\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        super(Map, self).__delitem__(key)\n",
    "        del self.__dict__[key]\n",
    "        \n",
    "#--------------------------------------------------------------------------------\n",
    "def runMethod(pyMethod, **kwargs):\n",
    "    spl = pyMethod.split('.');\n",
    "\n",
    "    assert len(spl) >= 2, \"Hmmm ... May be not what is intended!! module name\"\n",
    "\n",
    "    modName = \".\".join(spl[:-1])\n",
    "    __import__(modName, fromlist=\"dummy\")\n",
    "\n",
    "    funName = spl[-1]\n",
    "    ret = None\n",
    "    for v in sys.modules:\n",
    "        if (v.startswith(modName)):\n",
    "            method= getattr(sys.modules[v], funName)\n",
    "            print(v, type(v), funName, method, type(method), callable(method))\n",
    "            ret = method(**kwargs)\n",
    "    return ret\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "def getInvertedPreds(conf, yh):\n",
    "    scalerY = utils1.getConfigObject(conf, \"scalerYString\")\n",
    "    sOuputs = utils1.getConfigList(conf, 'scaleOutputs')\n",
    "    ouputs  = utils1.getConfigList(conf, 'outputs')\n",
    "    \n",
    "    yhdf    = pd.DataFrame(yh, columns=ouputs)    # Dataframe of Predictions\n",
    "    ys      = yhdf[sOuputs].values                # Values to be scaledback\n",
    "    yi      = scalerY.inverse_transform(ys)       # inverse transform the outputs\n",
    "    yidf    = yhdf.copy()                         # Copy and set the values\n",
    "    yidf[sOuputs] = yi\n",
    "    return yhdf, yidf\n",
    "    \n",
    "#--------------------------------------------------------------------------------\n",
    "def getOriginal(conf, unnormdf, index=0):\n",
    "    inputs  = utils1.getConfigList(conf, 'inputs')\n",
    "    ouputs  = utils1.getConfigList(conf, 'outputs')\n",
    "    tsParams= conf.get(\"tsParams\", {})\n",
    "    \n",
    "    index   = 0\n",
    "    startIX = index + tsParams['length']\n",
    "    batchSZ = 1 # batch size\n",
    "    stride  = tsParams.get('stride', 1)\n",
    "    i = startIX + batchSZ * stride * index\n",
    "    return unnormdf[i:], inputs, ouputs\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "def plotInverted(conf, yh, unnormdf, s =-400, howmany=100):\n",
    "    e=s+howmany\n",
    "    \n",
    "    yhdf, yidf = getInvertedPreds(conf, yh)\n",
    "    yorg,ips,ops = getOriginal(conf, unnormdf)\n",
    "\n",
    "    x = pd.to_datetime(yorg[yorg.columns[0]][s:e])\n",
    "    \n",
    "    plt.plot(x, yorg[ops].values[s:e], marker='.', label=\"Original\")\n",
    "    plt.plot(x, yidf.values[s:e], marker='x', label=\"Predicted\")\n",
    "    plt.title(\"Plotting Inverted Values:\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "'''\n",
    "Reconstruct the original diffed columns\n",
    "'''    \n",
    "def reconstructOrig(conf, unnormdf, yh, ouputs):\n",
    "    yhdf, yidf = getInvertedPreds(conf, yh)\n",
    "    yorg,ips,ops = getOriginal(conf, unnormdf)\n",
    "    \n",
    "    for o in ouputs:\n",
    "        if(o.endswith(\"___diff1\")):\n",
    "            oc = o[:-8]\n",
    "            print(f\"Getting Original column for: '{oc}' \")\n",
    "            if ( oc not in yorg.columns):\n",
    "                print('Cannot compute the orginal column values from diffs for {oc}')\n",
    "                continue;\n",
    "                \n",
    "            ## WOW <== this is heavy - undo the diffing in the opposite way\n",
    "            yidf[oc] = yorg[oc].values + yidf[o].shift(-1)\n",
    "\n",
    "    return yidf # y inverted dataframe with adjusted cols for diffs\n",
    "\n",
    "\n",
    "\n",
    "#runMethod(\"gen.utils1.is_number\", **{\"s\":\"123.78\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile  ~/bin/plotutils1.py\n",
    "#!/usr/local/bin/python \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re, sys, os, datetime, glob, json, base64, pickle, sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (16, 3)\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"gen\")\n",
    "sys.path.append(\"/opt/LMCO/git/bin/gen\")\n",
    "import lstmutils1;\n",
    "import lstmfit;\n",
    "import sklearn.metrics\n",
    "\n",
    "def predict(modelFile, valg, model=None):\n",
    "    m1 = model or load_model(modelFile)\n",
    "    #xxt = np.array([valg[i][0][0] for i in range(len(valg))])\n",
    "    #yyt = np.array([valg[i][1][0] for i in range(len(valg))])\n",
    "    yh=m1.predict(valg)\n",
    "    return yh\n",
    "            \n",
    "def plot1(modelFile, valg, model=None, idx=0, n=-150, howmany=50):\n",
    "    yh = predict(modelFile, valg, model)\n",
    "    yy = np.array([valg[i][1][idx] for i in range(len(valg))])\n",
    "                 \n",
    "    plt.gcf().set_size_inches(22, 10, forward=True)\n",
    "    plt.plot( yy[n:n+howmany], marker='o', label=\"original-\")\n",
    "    plt.plot( yh[n:n+howmany], marker='x', label=\"predicted\")\n",
    "    mse = sklearn.metrics.mean_squared_error(yy, yh)\n",
    "    \n",
    "    plt.title(f\"{modelFile} : {model}: MSE: {mse} <==\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return yy, yh, mse\n",
    "\n",
    "\n",
    "def plotyyyh(columns, xxx, yy1, yh1, s, e):\n",
    "    for i in range(yh1.shape[-1]):\n",
    "        yyy = yy1[:,i]\n",
    "        yyh = yh1[:,i]\n",
    "\n",
    "        r2 = sklearn.metrics.r2_score(yyy, yyh)\n",
    "        if (r2 <0.5):\n",
    "            print(f'--{i}: {columns[i+1]} R^2: {r2}')\n",
    "            continue\n",
    "\n",
    "        plt.title(f'{i}: {columns[i+1]} R^2: {r2}')\n",
    "\n",
    "        plt.plot(xxx[s:e], yyy[s:e], marker='.', label=\"original\")\n",
    "        plt.plot(xxx[s:e], yyh[s:e], marker='x', label=\"predictd\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "def plotstuff(valg, confile, normeddf=None, model = None, mcpoint=None ):\n",
    "    if ( model is None):\n",
    "        conf, unnormdf, normeddf, inps, oups = lstmfit.getConf(confile)\n",
    "        model, mcpoint = lstmfit.getModel(conf)\n",
    "\n",
    "    print(len(valg), mcpoint.best)\n",
    "    \n",
    "    xxx = pd.to_datetime(normeddf.time[-len(valg):])\n",
    "    mcpoint.drawLosses()\n",
    "\n",
    "    yh1 = model.predict(valg)            \n",
    "    yy1 = np.array([valg[j][1][0] for j in range(len(valg))])\n",
    "\n",
    "    s = 0\n",
    "    e = s + len(valg)\n",
    "    plotyyyh(normeddf.columns, xxx, yy1, yh1, s, e)\n",
    "    \n",
    "    return model, mcpoint, normeddf, yy1, yh1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Stocks Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile  ~/bin/gen/getstocksdata.py\n",
    "#!/usr/local/bin/python \n",
    "\n",
    "'GENERATED BY NNBook/notebooks/NNetworks/LSTM/01_Utils.ipynb'\n",
    "\n",
    "import os, datetime, glob, sys\n",
    "import tensorflow as tf\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"gen\")\n",
    "import time, datetime\n",
    "\n",
    "'''\n",
    "The following code: it is just here to protect my API-KEY code not really useful\n",
    "for the problem at hand. If your key is in your or update your API_KEY then you may safely ignore \n",
    "this code.\n",
    "'''\n",
    "\n",
    "def getkey(key='password'):\n",
    "    API_KEY, lines, file =None, None, os.path.expanduser('~/.keys/keys.json')\n",
    "    if os.path.exists(file):\n",
    "        with open(file, 'r') as f:\n",
    "            r = f.read()\n",
    "        j = eval(r)\n",
    "        return j['AV_API_KEY'], j['NEWSAPI_KEY']\n",
    "\n",
    "    #alpha_vantage key\n",
    "    avk = decrypt('baTEje52rx+kAsuAN9PdxMeC03p/HuRVTzskLiso1/c=', key)\n",
    "    newsk = decrypt('505Db5sDvHvptBPzE8IhsewneuanOKV3gKpN+26lS3A=', key)\n",
    "    return avk, newsk;\n",
    "        \n",
    "def encrypt(msg_text = b'message', secret_key='password'):\n",
    "    if (type(msg_text) == str):\n",
    "        msg_text = bytes(msg_text, encoding='utf-8').rjust(32)\n",
    "    if (type(secret_key) == str):\n",
    "        secret_key = bytes(secret_key, encoding='utf-8') .rjust(32)\n",
    "\n",
    "    cipher = AES.new(secret_key,AES.MODE_ECB) \n",
    "    encoded = base64.b64encode(cipher.encrypt(msg_text))\n",
    "    ret = encoded.decode(\"utf-8\")\n",
    "    print(ret)\n",
    "    return ret\n",
    "\n",
    "def decrypt(encoded, secret_key='password'):\n",
    "    if (type(secret_key) == str):\n",
    "        secret_key = bytes(secret_key, encoding='utf-8') .rjust(32)\n",
    "\n",
    "    cipher = AES.new(secret_key,AES.MODE_ECB) \n",
    "    if (type(encoded) == str):\n",
    "        encoded = bytes(encoded, encoding='utf-8')\n",
    "    decoded = cipher.decrypt(base64.b64decode(encoded))\n",
    "    ret =decoded.decode(\"utf-8\").strip()\n",
    "    print(ret)\n",
    "    return ret\n",
    "\n",
    "'''\n",
    "This will read required symbols and saves them to data directory\n",
    "'''\n",
    "def save_data(symbol, API_KEY=\"\", check=True):\n",
    "    from alpha_vantage.timeseries import TimeSeries\n",
    "    \n",
    "    outf = f'daily_{symbol}.csv'\n",
    "    if (check and os.path.exists(outf)):\n",
    "        \n",
    "        dt = datetime.datetime.fromtimestamp(os.path.getmtime(outf))\n",
    "        dn = datetime.datetime.now()\n",
    "        ts = (dn - dt)\n",
    "        hr = (ts.days * 24 * 60 * 60 + ts.seconds)//60//60\n",
    "        if (hr < 4): #if it was created less than 4 hours ago\n",
    "            print(f\"{outf:22} exists, ... recently crested < 4 at {dt} \")\n",
    "            return;\n",
    "    ts = TimeSeries(key=API_KEY, output_format='pandas')\n",
    "    \n",
    "    retry = 0\n",
    "    data = None\n",
    "    while retry <= 5:\n",
    "        try:\n",
    "            print(f\"Getting data for {symbol}\")\n",
    "            data, meta_data = ts.get_daily(symbol, outputsize='full')\n",
    "            break;\n",
    "        except ValueError as ve:\n",
    "            retry += 1\n",
    "            print(f\"Sleep for a Minute, {retry}/5 attempts\");\n",
    "            time.sleep(60)\n",
    "            \n",
    "    if data is None:\n",
    "        print(f\"Could not get data for {symbol}\")\n",
    "        return data\n",
    "    \n",
    "    data.insert(0, 'timestamp', value=data.index)\n",
    "\n",
    "    data.columns = 'timestamp,open,high,low,close,volume'.split(',')\n",
    "    data.to_csv(outf, index=False)\n",
    "    return data\n",
    "\n",
    "'''\n",
    "Read all the files in data with daily_*, reads them and returns a list\n",
    "'''\n",
    "def read_data():\n",
    "    a={}\n",
    "    for f in glob.glob('daily_*'):\n",
    "        symbol = os.path.basename(os.path.splitext(f)[0]).split(\"_\")[1]\n",
    "        print(f'Reading {f} Symbol: {symbol}')\n",
    "        df = pd.read_csv(f)\n",
    "        df.sort_values(by='timestamp', ascending=False, inplace=True)\n",
    "        df.index=(range(0,len(df)))\n",
    "        ncs = ['timestamp'] + [f'{symbol}_{c}' for c in df.columns[1:]]\n",
    "        df.columns = ncs\n",
    "        a[symbol] = df\n",
    "        \n",
    "    minrows = min([len(d) for d in a.values()])\n",
    "    return a, minrows, df\n",
    "\n",
    "'''\n",
    "Combines all the dataframes into one\n",
    "\n",
    "The problem for us to join multiple index funds from ASIA is that they have different holidays\n",
    "Therefore we have gaps in the trading days. Therefore \n",
    "'''\n",
    "def combine_data(a, outp=\"stockdata.csv\"):\n",
    "    # Different excahnges have various holidays, therefore values may be missing  \n",
    "    # Get All Data Frames and their corresponding time stamps\n",
    "    #\n",
    "    ar=np.array([d['timestamp'].values for d in a.values()])\n",
    "    at=np.concatenate(ar)\n",
    "    at=set(at)\n",
    "    af = pd.DataFrame()\n",
    "    af['timestamp'] = list(at);\n",
    "\n",
    "    for k,v in a.items():\n",
    "        #print(f\"Getting {k:32} \\r\", end='')\n",
    "        af=pd.merge(af,v, how=\"left\", left_on=\"timestamp\".split(), right_on=\"timestamp\".split())\n",
    "    print()\n",
    "    af.sort_values(by='timestamp', ascending=True, inplace=True)\n",
    "    af.dropna(inplace=True)\n",
    "    af = af.reset_index(drop=True)\n",
    "    #af = af.fillna(method='ffill' ).fillna(method='bfill')\n",
    "    af.to_csv(outp, index=False)\n",
    "    return af\n",
    "\n",
    "def addDummyCols(df):\n",
    "    tf1=df\n",
    "    if ( \"MSFT_+ve\" not in tf1.columns):\n",
    "        tf1.insert(1, \"MSFT_+ve\", value=[f\"S__{k}\" for k in np.random.randint(0,3, size=len(tf1))] )\n",
    "    if ( \"MSFT_-ve\" not in tf1.columns):\n",
    "        #tf.insert(1, \"MSFT_-ve\", value=[f\"S__{k}\" for k in np.random.randint(0,1, size=len(nf))] )\n",
    "        tf1.insert(1, \"MSFT_-ve\", value=[k for k in np.random.randint(0,2, size=len(tf1))] )\n",
    "    tf1.to_csv(\"stockdata_ext.csv\", index=False);\n",
    "    return tf1\n",
    "\n",
    "\n",
    "def getdata(symbs='MSFT GLD GOOGL SPX AAPL IBM' , dontforce=False):\n",
    "    \n",
    "    stockfile=\"data/stockdata.csv\"\n",
    "    if (dontforce and os.path.exists(stockfile)):\n",
    "        print(f\"{stockfile} exists\")\n",
    "        return\n",
    "    \n",
    "    API_KEY = None  # Put your API KEY if you need to test data download or just use the data\n",
    "    API_KEY, NEWS_API_KEY = API_KEY or getkey()\n",
    "\n",
    "    #print(\"API_KEY \", API_KEY)\n",
    "    for f in symbs.split():\n",
    "        save_data(f, API_KEY=API_KEY)\n",
    "\n",
    "    ASIA='''TOKYO 6758.T HITACHI 6501.T HNGKNG 0168.HK SHANGAI 601288.SS SHNZEN'''\n",
    "    s=ASIA.split()\n",
    "    ASIA = {k[0]:k[1] for k in   zip(s[0::2], s[1::2])}\n",
    "\n",
    "    for k, v in ASIA.items():\n",
    "        print(f'getting data for {k} => symbol {v}')\n",
    "        save_data(v, API_KEY)\n",
    "\n",
    "def getCombined():\n",
    "    #Get All data togehe\n",
    "    a, maxrows, ldf  = read_data()\n",
    "    nf= combine_data(a, \"stockdata.csv\")\n",
    "    addDummyCols(nf)\n",
    "    return nf\n",
    "    \n",
    "#-----------------------------------------------------------------------------------\n",
    "#*** NOTE: DO NOT EDIT THIS FILE - THIS iS CREATED FROM: inv_utils.ipynb\n",
    "def inJupyter():\n",
    "    try:    get_ipython; return True\n",
    "    except: return False\n",
    "#-----------------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    if (not inJupyter()):\n",
    "        t1 = datetime.datetime.now()\n",
    "        getdata()\n",
    "        getCombined();\n",
    "        t2 = datetime.datetime.now()\n",
    "        print(f\"All Done in {str(t2-t1)} ***\")\n",
    "    else:\n",
    "        pass\n",
    "        '''\n",
    "        a, minrows, ldf  = read_data()\n",
    "        stockfile=\"data/stockdata.csv\"\n",
    "        nf= combine_data(a, stockfile)\n",
    "        addDummyCols(nf)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Configure  - dataconfig.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile  ~/bin/gen/dataconfig.py\n",
    "#!/usr/local/bin/python \n",
    "\n",
    "import re, sys, os, datetime, getopt, glob, argparse, datetime, json, base64, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"gen\")\n",
    "sys.path.append(\"~/bin/gen\")\n",
    "import lstmutils1\n",
    "\n",
    "#~~~~ Find any sensor highly correlated with time and drop them.\n",
    "def detectTimeCorrelated(df, val=0.94):\n",
    "    timeCorSensors = []\n",
    "    if ( not 'time' in df.columns): #assume first column is time column\n",
    "        dcols = ['time'] + [c for c in df.columns[1:]]\n",
    "        df.columns = dcols\n",
    "    \n",
    "    timeser = pd.Series(df[['time']].values.reshape(-1))\n",
    "    if ( timeser.dtype != np.number ):\n",
    "        timeser = pd.to_datetime(timeser).astype(int)\n",
    "    \n",
    "    \n",
    "    DROP_INDEX = 0;\n",
    "    for sensor in df.columns:\n",
    "        if (sensor == 'time'):\n",
    "            continue;\n",
    "        #print(f\"#Testing {sensor}...\")\n",
    "        sensorSeries = pd.Series(df[sensor].values.reshape(-1))\n",
    "        for i in range(8):\n",
    "            c1 = timeser[i:].corr(sensorSeries[i:])\n",
    "            c2 = timeser[i:].corr(sensorSeries[:-i])\n",
    "            if np.abs(c1) >= val or np.abs(c2) >= val:\n",
    "                timeCorSensors.append(sensor)\n",
    "                DROP_INDEX = max(DROP_INDEX, i) #lets drop first few rows\n",
    "                break;\n",
    "                \n",
    "    if ( len(timeCorSensors) > 0):\n",
    "        print(f\"#Time Cor: #{len(timeCorSensors)}, #Shape before:{df.shape}\")\n",
    "        #df.drop(timeCorSensors, axis=1, inplace=True)\n",
    "        #df = df[DROP_INDEX:]\n",
    "        #print(f\"#After dropping: {DROP_INDEX} =>{df.shape}\")\n",
    "        \n",
    "    return timeCorSensors\n",
    "#-----------------------------------------------------------------------------------\n",
    "def precheck(df):\n",
    "    cols = df.columns[df.dtypes.eq('object')]\n",
    "    if (len(cols) > 0):\n",
    "        print(f\"WARNING: *** Non numeric columns => {cols}\")\n",
    "        return 0\n",
    "    return 1\n",
    "#-----------------------------------------------------------------------------------\n",
    "'Covert to one_hot encoding with prefix for columns'\n",
    "def makeOneHotCols(tf1, oheCols=[]):\n",
    "    ret = []\n",
    "    for c in oheCols:\n",
    "        one_hot = pd.get_dummies(tf1[c])\n",
    "        ret += [f'{c}___{k}' for k in one_hot.columns]\n",
    "\n",
    "    return ret\n",
    "#-----------------------------------------------------------------------------------\n",
    "def inJupyter():\n",
    "    try:    get_ipython; return True\n",
    "    except: return False\n",
    "#-----------------------------------------------------------------------------------\n",
    "def detectCols(file, nUnique=4, tcoeff=0.92):\n",
    "    tf1 = file\n",
    "    if (type(tf1) == str):\n",
    "        tf1 = pd.read_csv(tf1, comment=\"#\")\n",
    "    \n",
    "    #Lets check if it has any non-numeric columns! Warning\n",
    "    precheck(tf1)\n",
    "    \n",
    "    unique_vals  = tf1.nunique()\n",
    "    constantCols = unique_vals[ unique_vals == 1].index                             # constant Columns\n",
    "    onehotECols  = unique_vals[(unique_vals > 2 ) & (unique_vals<=nUnique)].index   # Categorical Columns\n",
    "    categorCols  = unique_vals[(unique_vals >=2 ) & (unique_vals <= nUnique)].index # Categorical Columns\n",
    "    binaryCols   = unique_vals[(unique_vals == 2)].index                      # Binary\n",
    "\n",
    "    numericCols  = tf1.select_dtypes(include=np.number).columns           # numerics\n",
    "    numericCols  = [c for c in numericCols if c not in categorCols]\n",
    "    numericCols  = [c for c in numericCols if c not in constantCols]\n",
    "    numericCols  = [c for c in numericCols if c != 'time']\n",
    "    notNumerics  = tf1.select_dtypes(exclude=np.number).columns           # non - numerics\n",
    "    notNumerics  = [c for c in notNumerics if c not in categorCols]       # non - numerics\n",
    "\n",
    "    try:\n",
    "        timeCorCols  = detectTimeCorrelated(tf1, tcoeff)\n",
    "    except:\n",
    "        timeCorCols = []\n",
    "    \n",
    "    onehotEC_ext = makeOneHotCols(tf1, onehotECols)\n",
    "   \n",
    "    ret1 =f'''[START]\n",
    "{{\n",
    "    \"file\"           : {[file] if (type(file) == str) else [\"??\"]},\n",
    "    \"nrowsXncols\"    : {[len(tf1), len(tf1.columns )] }     , \n",
    "    \"number_Unique\"  : {nUnique}            , \n",
    "    \"constantCols\"   : {list(constantCols )},   # No Signals\n",
    "    \"#constantCols\"  : {len(constantCols  )},   # No Signals\n",
    "    \"categorCols\"    : {list(categorCols  )},   # Categorical Columns\n",
    "    \"#categorCols\"   : {len(categorCols   )},   # Categorical Columns\n",
    "    \"onehotECols\"    : {list(onehotECols  )},   # Cats > 2 and < Unique Values\n",
    "    \"onehotEC_ext\"   : {list(onehotEC_ext )},   # Cats > 2 and < Unique Values\n",
    "    \"#onehotECols\"   : {len(onehotECols   )},   # Cats > 2 and < Unique Values\n",
    "    \"binaryCols\"     : {list(binaryCols   )},   # Binary\n",
    "    \"#binaryCols\"    : {len(binaryCols    )},   # Binary\n",
    "    \"notNumerics\"    : {list(notNumerics  )},\n",
    "    \"timeCorrelation\": {tcoeff             },   # Time correlated\n",
    "    \"timeCorrCols\"   : {list(timeCorCols  )},   # Time correlated Columns\n",
    "    \"#timeCorrCols\"  : {len(timeCorCols   )},    # Time correlated Columns\n",
    "    \"excludePattern\" : [] , #Exclude patterns\n",
    "    \"includePattern\" : [] , #include patterns\n",
    "    \"dropColumns\"    : [],\n",
    "    \"diff_suffix\"    : {['__diff1']},\n",
    "    \"addDiffs\"       : [],\n",
    "    \"train_pct\"      : .9,\n",
    "    \"#numericCols\"   : {len(numericCols   )},  \n",
    "    \"scaleInputs\"    : {list(numericCols  )},  \n",
    "    \"scaleOutputs\"   : {[\"$scaleInputs\"]},  \n",
    "    \"inputs\"         : {[\"$binaryCols\", \"$scaleInputs\", \"$onehotECols\"]},\n",
    "    \"outputs\"        : {[\"$scaleOutputs\"]},\n",
    "#-----Copy this generated file and add customization\n",
    "    \"loadModel\"      : 1,\n",
    "    \"scale\"          : 1,\n",
    "    \"scaler\"         : [\"sklearn.preprocessing.StandardScaler()\"],\n",
    "    \"scaler\"         : [\"sklearn.preprocessing.MinMaxScaler()\"],\n",
    "    \"scalerXString\"  : [],\n",
    "    \"scalerYString\"  : [],\n",
    "    \"tsParams\"       : {{\"length\": 60, \"batch_size\": 1, \"stride\": 1, \"sampling_rate\": 1}},\n",
    "    \"lookahead\"      : 60,\n",
    "    \"nsteps\"         : 1,\n",
    "    \"modelFile\"      : [\"lstm.56.h5\"],\n",
    "    \"monitor\"        : \"val_loss\",\n",
    "    \"modelName\"      : \"gen.somemodels.SimpleModel1(50, 5, 1, **{{}})\"\n",
    "}}\n",
    "[END]\n",
    "    '''\n",
    "\n",
    "    return ret1, tf1;\n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "def process():\n",
    "    n  = len(sysargs.input_files)\n",
    "    un = sysargs.unique\n",
    "    tc = sysargs.tcoeff\n",
    "    for i, file1 in enumerate(sysargs.input_files):\n",
    "        print(f\"#=>Processing {i+1}/{n} {file1} #unique: {un} tcoeff: {tc} - standby\")\n",
    "        outs, df = detectCols(file1, un, tc)\n",
    "        \n",
    "        break;\n",
    "    print(outs)\n",
    "    return outs\n",
    "    \n",
    "#-----------------------------------------------------------------------------------\n",
    "sysargs=None\n",
    "def addargs():\n",
    "    sysargs = None\n",
    "    p = argparse.ArgumentParser(f\"{os.path.basename(sys.argv[0])}:\")\n",
    "    p.add_argument('-u', '--unique', type=int,   default=6,    help=\"# of unique values!\")\n",
    "    p.add_argument('-t', '--tcoeff', type=float, default=0.94, help=\"# Time Correlation value Sensors!\")\n",
    "    p.add_argument('args', nargs=argparse.REMAINDER)\n",
    "    p.add_argument('input_files',action=\"store\", type=str, nargs='+', help=\"input file(s)\")\n",
    "\n",
    "    #p.print_help() # always print -help\n",
    "    try:\n",
    "        sysargs=p.parse_args(sys.argv[1:])\n",
    "        #print(f'using:\\n{sysargs}')\n",
    "    except argparse.ArgumentError as exc:\n",
    "        #par.print_help()\n",
    "        print(exc.message )\n",
    "        \n",
    "    return sysargs\n",
    "#-----------------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    if (not inJupyter()):\n",
    "        t1 = datetime.datetime.now()\n",
    "        sysargs = addargs()\n",
    "        process()\n",
    "        t2 = datetime.datetime.now()\n",
    "        print(f\"#All Done in {str(t2-t1)} ***\")\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f='/opt/SCHAS/NNBook/notebooks/NNetworks/LSTM/data/stockdata_ext.csv'\n",
    "#r=detectCols(f)\n",
    "#pp=r[0]\n",
    "\n",
    "f='/opt/SCHAS/NNBook/notebooks/NNetworks/LSTM/data/daily_MSFT.csv'\n",
    "pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile  ~/bin/gen/dataprepare.py\n",
    "#!/usr/local/bin/python \n",
    "\n",
    "import re, sklearn, sys, os, datetime, getopt, glob, argparse, datetime, json, base64, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"gen\")\n",
    "sys.path.append(\"~/bin/gen\")\n",
    "import lstmutils1;\n",
    "\n",
    "'''\n",
    "Make sure data is sorted in asceding order of the time for LSTM to work and all these \n",
    "data prep tools to work.\n",
    "'''\n",
    "\n",
    "\n",
    "'Covert to one_hot encoding with prefix for columns'\n",
    "def makeOneHot(tf1, oheCols=[]):\n",
    "    ohe = pd.DataFrame();\n",
    "    for c in oheCols:\n",
    "        one_hot = pd.get_dummies(tf1[c])\n",
    "        nc = [f'{c}___{k}' for k in one_hot.columns]\n",
    "        one_hot.columns = nc\n",
    "        ohe = pd.concat([ohe, one_hot], axis=1)\n",
    "\n",
    "    return ohe\n",
    "\n",
    "'''\n",
    "Assuming the tf1 is sorted in ascending order of time\n",
    "'''\n",
    "def addDiff(tf1, col):\n",
    "    #col = \"MSFT_open\"\n",
    "    if (type(col) == str):\n",
    "        col = [col]\n",
    "    for c in col:\n",
    "        if ( c not in tf1.columns):\n",
    "            print(f\"*WARNING* Column {c} Not FOUND\")\n",
    "            continue\n",
    "        print(f\"+++ Adding {c}\")\n",
    "        tf1[f'{c}___diff1'] = tf1[c] - tf1[c].shift(1)\n",
    "    return tf1\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "#*** NOTE: DO NOT EDIT THIS FILE - THIS iS CREATED FROM: inv_utils.ipynb\n",
    "def inJupyter():\n",
    "    try:    get_ipython; return True\n",
    "    except: return False\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "def formatConfig(out: dict):\n",
    "    outj = f\"[START]\\n{{\\n\"\n",
    "    for k,v in out.items():\n",
    "        kk = f'\"{k}\"'\n",
    "        vv = f\"'{v}'\" if type(v) == str else v\n",
    "            \n",
    "        outj += f'{kk:>20}: {vv},\\n'\n",
    "    outj += '\"end\": 0 \\n}\\n[END]\\n'\n",
    "    \n",
    "    return outj\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "def getFinalColumns(df, conf):\n",
    "    inputCols = lstmutils1.getConfigList(conf, \"inputs\")\n",
    "    ouputCols = lstmutils1.getConfigList(conf, \"outputs\")\n",
    "\n",
    "    allcols = set(inputCols +ouputCols);\n",
    "    \n",
    "    assert allcols.issubset(set(df.columns)), \"Hmmm ... columns missing\"\n",
    "    return sorted(list(allcols))\n",
    "#-----------------------------------------------------------------------------------\n",
    "'''\n",
    "This will scale the numeric columns - if this changes - you need to use dataprep\n",
    "'''\n",
    "def scaleNumerics(df, conf={}):\n",
    "    scaleInputCols = lstmutils1.getConfigList(conf, 'scaleInputs')\n",
    "    scaleOuputCols = lstmutils1.getConfigList(conf, 'scaleOutputs')\n",
    "    \n",
    "    dfninps = df[ scaleInputCols ]\n",
    "    dfnouts = df[ scaleOuputCols ]\n",
    "\n",
    "    scale   = conf.get('scale', 0)\n",
    "    dfninpsn, dfnoutsn, scalerX, scalerY = dfninps, dfnouts, None, None\n",
    "    \n",
    "    if (scale):\n",
    "        scalerXStr = conf['scalerXString']\n",
    "        scalerYStr = conf['scalerYString']\n",
    "        trnPct     = conf.get('train_pct', 0.9);\n",
    "        trnCnt     = int(len(df) * trnPct)\n",
    "\n",
    "        conf[\"train_pct\"   ] = trnPct\n",
    "        conf[\"train_count\" ] = trnCnt\n",
    "\n",
    "        if (not scalerXStr):\n",
    "            scaler  = conf.get('scaler', [\"sklearn.preprocessing.MinMaxScaler()\"]);\n",
    "            scalerX = eval(scaler[0]) if type(scaler[0]) == str else scaler\n",
    "            scalerX = scalerX.fit(dfninps[:trnCnt])\n",
    "            scalerstr = base64.b64encode(pickle.dumps(scalerX, protocol=None, fix_imports=True))\n",
    "            scalerstr = scalerstr.decode(\"utf-8\")\n",
    "            conf[\"scalerXString\"] = [scalerstr]\n",
    "            #print(f'==>+1 shape: {dfninps.shape} {scalerX.mean_}')\n",
    "        else:\n",
    "            scalerXStr = scalerXStr[0]\n",
    "            decoded    = base64.b64decode(scalerXStr)\n",
    "            scalerX    = pickle.loads(decoded,fix_imports=True)\n",
    "\n",
    "        if (not scalerYStr):\n",
    "            scaler  = conf.get('scaler', [\"sklearn.preprocessing.MinMaxScaler()\"]);\n",
    "            scalerY = eval(scaler[0]) if type(scaler[0]) == str else scaler\n",
    "            scalerY = scalerY.fit(dfnouts[:trnCnt])\n",
    "            scalerstr = base64.b64encode(pickle.dumps(scalerY, protocol=None, fix_imports=True))\n",
    "            scalerstr = scalerstr.decode(\"utf-8\")\n",
    "            conf[\"scalerYString\"] = [scalerstr]\n",
    "            #print(f'==>+2 shape: {dfninps.shape} {scalerY.mean_}')\n",
    "        else:\n",
    "            scalerYStr = scalerYStr[0]\n",
    "            decoded    = base64.b64decode(scalerYStr)\n",
    "            scalerY    = pickle.loads(decoded,fix_imports=True)\n",
    "\n",
    "        di = scalerX.transform(dfninps)\n",
    "        do = scalerY.transform(dfnoutsn)\n",
    "        \n",
    "        dfninpsn = pd.DataFrame(di, columns=scaleInputCols )\n",
    "        dfnoutsn = pd.DataFrame(do, columns=scaleOuputCols )\n",
    "        \n",
    "        #print(f'==>++ shape: {dfninpsn.shape} {scalerX.mean_}')\n",
    "        #print(f'==>++ shape: {dfnoutsn.shape} {scalerY.mean_}')\n",
    "\n",
    "    return dfninpsn, dfnoutsn, scalerX, scalerY;\n",
    "#-----------------------------------------------------------------------------------\n",
    "def process(config, input_files, output=None):\n",
    "    conf = lstmutils1.getconfig(config)\n",
    "    \n",
    "    n  = len(input_files)\n",
    "    adfInp = pd.DataFrame();\n",
    "    for i, file1 in enumerate(input_files):\n",
    "        print(f\"=>Processing {i+1}/{n} {file1} - standby\")\n",
    "        df = pd.read_csv(file1, comment='#')\n",
    "        \n",
    "        drps = lstmutils1.getConfigList(conf, \"dropColumns\")\n",
    "        df.drop(drps, inplace=True, errors=\"ignore\")\n",
    "        \n",
    "        # STEP 1: Add diffs\n",
    "        cdiffs  = lstmutils1.getConfigList(conf, 'addDiffs')\n",
    "        addDiff(df, cdiffs)    #<< 1. Add tis\n",
    "        df.dropna(inplace=True)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        # STEP 2: => One hot encode \n",
    "        ohe = None\n",
    "        ohecols = lstmutils1.getConfigList(conf, 'onehotECols')\n",
    "        if len(ohecols) > 0:\n",
    "            ohe=makeOneHot(df, conf['onehotECols'])  #< === ADD\n",
    "            df=pd.concat([df,ohe], axis=1)\n",
    "         \n",
    "        # STEP 3: Add\n",
    "        allCols = [df.columns[0]] + getFinalColumns(df,conf)\n",
    "        dfunNormalized = df[allCols]\n",
    "        #Numeric Columns\n",
    "        dfiNorm, dfoNorm, sX, sY = scaleNumerics(dfunNormalized, conf)\n",
    "        \n",
    "        dfNormalized = dfunNormalized.copy()\n",
    "        dfNormalized[dfiNorm.columns] = dfiNorm;\n",
    "        dfNormalized[dfoNorm.columns] = dfoNorm;\n",
    "        \n",
    "        #FINALLY        \n",
    "        if (output is not None):\n",
    "            fi,ext = os.path.splitext(file1)\n",
    "            nfu  = f'{os.path.basename(fi)}_Orig_{i}{ext}'\n",
    "            print(f\"writing unnormalized to: {nfu}\")\n",
    "            dfunNormalized.to_csv(nfu, index=False)\n",
    "            \n",
    "            \n",
    "            nfn  = f'{os.path.basename(fi)}_Norm_{i}{ext}'\n",
    "            print(f\"writing normalized to. : {nfn}\")\n",
    "            dfNormalized.to_csv  (nfn, index=False)\n",
    "            \n",
    "            conf['normalizedFile']   = nfn\n",
    "            conf['unnormalizedFile'] = nfu\n",
    "            \n",
    "        break;\n",
    "\n",
    "    outj = formatConfig(conf)\n",
    "    print(outj)\n",
    "    \n",
    "    return conf, dfunNormalized, dfNormalized\n",
    "#-----------------------------------------------------------------------------------\n",
    "sysargs=None\n",
    "def addargs():\n",
    "    sysargs = None\n",
    "    p = argparse.ArgumentParser(f\"{os.path.basename(sys.argv[0])}:\")\n",
    "    p.add_argument('-c', '--config', type=str, default=\"config.txt\", help=\"Config Files\")\n",
    "    p.add_argument('-o', '--output', type=str, default=0, help=\"output file\")\n",
    "    p.add_argument('args', nargs=argparse.REMAINDER)\n",
    "    p.add_argument('input_files',action=\"store\", type=str, nargs='+', help=\"input file(s)\")\n",
    "\n",
    "    #p.print_help() # always print -help\n",
    "    try:\n",
    "        sysargs=p.parse_args(sys.argv[1:])\n",
    "    except argparse.ArgumentError as exc:\n",
    "        print(exc.message )\n",
    "        \n",
    "    return sysargs\n",
    "    \n",
    "#-----------------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    if (not inJupyter()):\n",
    "        t1 = datetime.datetime.now()\n",
    "        sysargs = addargs()\n",
    "        ret = process(sysargs.config, sysargs.input_files, sysargs.output)\n",
    "        t2 = datetime.datetime.now()\n",
    "        print(f\"#All Done in {str(t2-t1)} ***\")\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f='/opt/SCHAS/NNBook/notebooks/NNetworks/LSTM/data/stockdata_ext.csv'\n",
    "f='/opt/SCHAS/NNBook/notebooks/NNetworks/LSTM/data/daily_MSFT.csv'\n",
    "conf, dfUnNormalized, dfNormalized = process('config.*', [f], \"out.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Callback for Saving and Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile  ~/bin/gen/ccallbacks.py\n",
    "#!/usr/local/bin/python \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re, sys, os, datetime, glob, json, base64, pickle, sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"gen\")\n",
    "sys.path.append(\"/opt/LMCO/git/bin/gen\")\n",
    "import lstmutils1;\n",
    "import sklearn.metrics\n",
    "\n",
    "class ModelCheckAndLoad(Callback):\n",
    "    def __init__(self, filepath, monitor='val_loss', best=np.inf, \n",
    "                 stop_at=False, verbose=0, drawLoss=False):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor  = monitor\n",
    "        self.filepath = filepath\n",
    "        self.verbose  = verbose\n",
    "        self.best     = best or np.inf\n",
    "        self.stop_at  = stop_at;\n",
    "        self.history  = {}\n",
    "        self.epochs   = []\n",
    "        self.drawLoss = drawLoss\n",
    "        self.epochNum = 0\n",
    "        self.numSaved = 0\n",
    "        \n",
    "    def save_ext(self):\n",
    "        ef = self.filepath+\"_ext\"\n",
    "        with open(ef, \"wb\") as f:\n",
    "            myParams = {\n",
    "                'best'     : self.best,\n",
    "                'bestEpoch': self.bestEpoch,\n",
    "                'epochNum' : self.epochNum,\n",
    "                'history'  : self.history,\n",
    "                'monitor'  : self.monitor\n",
    "            }\n",
    "            pickle.dump(myParams, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "    def save_latest(self):\n",
    "        self.model.save(self.filepath+\"_latest\", overwrite=True)\n",
    "        self.save_ext();\n",
    "            \n",
    "    def load_ext(self):\n",
    "        ret = None;\n",
    "        if ( os.path.exists(self.filepath+\"_latest\")):\n",
    "            ret = load_model(self.filepath+\"_latest\")\n",
    "            print(\"Loading from the latest:...\")\n",
    "        elif ( os.path.exists(self.filepath)):\n",
    "            ret = load_model(self.filepath)\n",
    "        \n",
    "        ef = self.filepath+\"_ext\"\n",
    "        if ( not os.path.exists(ef) or os.path.getsize(ef) <= 0):\n",
    "            return ret\n",
    "        \n",
    "        with open(ef, \"rb\") as f:\n",
    "            myParams      = pickle.load(f)\n",
    "            self.best     = myParams.get('best'    , np.inf)\n",
    "            self.epochNum = myParams.get('epochNum', 0);\n",
    "            self.history  = myParams.get('history', {});\n",
    "            self.monitor  = myParams.get('monitor'  , \"val_loss\");\n",
    "            \n",
    "        print(f\"Best Loaded {self.best} occured at: {self.epochNum}\")\n",
    "        return ret;\n",
    "\n",
    "    def drawLosses(self):\n",
    "        history, best = self.history, self.best\n",
    "        #IPython.display.clear_output(wait=True)\n",
    "        plt.clf()\n",
    "\n",
    "        fig, ax1 = plt.subplots()\n",
    "        i, colors, marks = 0, \"rgbcmykw\", \"v.xo+\"\n",
    "\n",
    "        color = colors[i]\n",
    "        ax1.set_xlabel('epochs')\n",
    "        k, v = \"loss\", history['loss']\n",
    "        ax1.set_ylabel(k, color=color)\n",
    "        l1= ax1.plot(v, color=color, marker=marks[i], label=f\"{k}\")\n",
    "\n",
    "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "        i +=1\n",
    "        k, v = \"val_loss\", history['val_loss']\n",
    "        color = colors[i]\n",
    "        ax2.set_ylabel(k, color=color)\n",
    "        l2 = ax2.plot(v, color=color, marker=marks[i], label=f\"{k}\")\n",
    "\n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        l3 = plt.plot(0, best, marker=\"o\",  c=\"b\", label=f\"BEST: {best}\")\n",
    "        ax1.grid()\n",
    "\n",
    "        lns  = l1 + l2 + l3;\n",
    "        labs = [l.get_label() for l in lns]\n",
    "        plt.legend(lns, labs, loc=0)\n",
    "        plt.show()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.epochs.append(epoch)\n",
    "        self.epochNum += 1;\n",
    "        \n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        self.current = logs.get(self.monitor)\n",
    "        if self.current is None:\n",
    "            warnings.warn(f'Can save best model only with {self.monitor} available')\n",
    "            return;\n",
    "                    \n",
    "        if (self.best > self.current):\n",
    "            ou= f'{self.monitor}: {self.best} > {self.current}\\n'\n",
    "            print(f\"Epoch: {epoch+1} Saving: {ou}\");\n",
    "            \n",
    "            self.numSaved += 1\n",
    "            self.bestEpoch+= 1\n",
    "            self.best      = self.current\n",
    "            self.model.save(self.filepath, overwrite=True)\n",
    "            self.save_ext();\n",
    "            self.model.stop_training = self.stop_at\n",
    "        elif self.verbose > 0:\n",
    "            ou= f'{self.monitor}: {self.best} <= {self.current}'\n",
    "            print(f\"{epoch+1} din't improve : {ou} from {self.bestEpoch}\\r\", end=\"\")\n",
    "            \n",
    "        if (self.drawLoss):\n",
    "            drawLosses(self.history, self.best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of Some Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile  ~/bin/gen/somemodels.py\n",
    "#!/usr/local/bin/python \n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate\n",
    "from keras.layers import RepeatVector, TimeDistributed\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras import optimizers\n",
    "\n",
    "def SimpleModel1(history, nfeatures, nOut, **kwargs) :\n",
    "    lstm_input = Input(shape=(history, nfeatures), name='lstm_input')\n",
    "    x = LSTM(50, name='lstm_0')(lstm_input)\n",
    "    x = Dropout(0.2, name='lstm_dropout_0')(x)\n",
    "    x = Dense(64, name='dense_0')(x)\n",
    "    x = Activation('sigmoid', name='sigmoid_0')(x)\n",
    "    x = Dense(nOut, name='dense_1')(x)\n",
    "    output = Activation('linear', name='linear_output')(x)\n",
    "\n",
    "    model = Model(inputs=lstm_input, outputs=output)\n",
    "    adam = optimizers.Adam(lr=0.0005)\n",
    "    model.compile(optimizer=adam, loss='mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def SimpleModel2(inps, inshape, units2=None, nsteps=1, opt=\"adam\", loss=\"mse\", bi=False, dropout=None):\n",
    "    s= inshape\n",
    "    print(locals())\n",
    "    print(f\"Creating LSTM: inuts= {inps} time-steps: {s[0]}, features: {s[1]} #out: {nsteps}\")\n",
    "    m = keras.models.Sequential()\n",
    "\n",
    "    if (bi):\n",
    "        m.add(keras.layers.Bidirectional(\n",
    "            keras.layers.LSTM(inps, return_sequences= (units2 is not None), input_shape=s) ) )\n",
    "    else:\n",
    "        m.add(keras.layers.LSTM(inps, return_sequences= (units2 is not None), input_shape=s) )\n",
    "    \n",
    "    if(units2 is not None): #Lets just keep it simple for 2 layers only\n",
    "        m.add(keras.layers.LSTM(units2, activation='relu'))\n",
    "    if (dropout is not None):\n",
    "        m.add( keras.layers.Dropout(dropout) )\n",
    "    m.add(keras.layers.Dense(nsteps))\n",
    "    m.compile(optimizer = opt, loss= loss)\n",
    "    return m\n",
    "\n",
    "def UberModel(lookBack, nFeatures, lstm_IPDim=256, lstm_OPDim=1, opt=None, loss=\"mse\",  drop=0.3):\n",
    "    opt        = opt or optimizers.Adam(lr=0.0005)\n",
    "    k_rrizer   = None\n",
    "    r_rrizer   = None\n",
    "\n",
    "    input_layer  = Input(shape=(lookBack, nFeatures), dtype='float32', name='input')\n",
    "    memory_layer = LSTM( lstm_IPDim, return_sequences=True, name=\"memory1\")(input_layer)\n",
    "    memory_layer = LSTM (int(lstm_IPDim/2), return_sequences=False, name=\"memory2\")(memory_layer)\n",
    "    repeated     = RepeatVector(lookBack)(memory_layer)\n",
    "    memory_layer = LSTM (int(lstm_IPDim/2), return_sequences=True, name=\"first1out\")(repeated)\n",
    "    memory_layer = LSTM (lstm_IPDim,  return_sequences=True, name=\"first2out\")(memory_layer)\n",
    "    decoded_inputs = TimeDistributed(Dense(units=lstm_OPDim, activation='linear'))( memory_layer)\n",
    "\n",
    "    #  Try spatial dropout?\n",
    "    dropout_input = Dropout(drop)(input_layer)\n",
    "    concat_layer  = concatenate([dropout_input, decoded_inputs])\n",
    "\n",
    "    memory_layer = LSTM (units=lstm_IPDim, \n",
    "                             kernel_regularizer = k_rrizer, \n",
    "                             recurrent_regularizer = r_rrizer, \n",
    "                             return_sequences=False)(concat_layer)\n",
    "    preds = Dense(units=lstm_OPDim, activation='linear')(memory_layer)\n",
    "\n",
    "    model1 = Model(input_layer, preds)\n",
    "    model1.compile(optimizer = opt, loss= loss)             \n",
    "\n",
    "    return model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%writefile  ~/bin/gen/lstmfit.py\n",
    "#!/usr/local/bin/python \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re, sys, os, datetime, getopt, glob, argparse, json, base64, pickle\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (16, 5)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(13)\n",
    "pd.options.display.max_rows = 8\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"gen\")\n",
    "sys.path.append(\"~/bin/gen\")\n",
    "sys.path.append(\"~/bin\")\n",
    "import lstmutils1;\n",
    "import ccallbacks\n",
    "\n",
    "np.random.seed(4)\n",
    "\n",
    "def getConf(cfile = \"myconfig\"):\n",
    "    conf    = lstmutils1.getconfig(cfile)\n",
    "    trnFile = conf['normalizedFile']\n",
    "    orgFile = conf['unnormalizedFile']\n",
    "\n",
    "    ddir = \"\"\n",
    "    if not os.path.exists(trnFile):\n",
    "        ddir     = os.path.abspath(cfile)\n",
    "        ddir     = os.path.dirname(ddir) \n",
    "        trnFile  = f'{ddir}/{trnFile}'\n",
    "        orgFile  = f'{ddir}/{orgFile}'\n",
    "\n",
    "    normeddf     = pd.read_csv(trnFile)\n",
    "    unnormdf     = pd.read_csv(orgFile)\n",
    "    inputs       = lstmutils1.getConfigList(conf, 'inputs')\n",
    "    ouputs       = lstmutils1.getConfigList(conf, 'outputs')\n",
    "    train_pct    = conf.get('train_pct', 0.9)\n",
    "    train_count  = conf.get('train_count', int(len(normeddf) * train_pct) )\n",
    "\n",
    "    print(f'''\n",
    "    TrnFile: {trnFile},\n",
    "    I/P    : {inputs[0:4]} ...\n",
    "    O/P    : {ouputs[0:4]} ...\n",
    "    Shape  : {normeddf.shape}\n",
    "    trnCnt : {train_count}\"\n",
    "    ''')\n",
    "    \n",
    "    return conf, unnormdf, normeddf, inputs, ouputs\n",
    "\n",
    "def getGenerators(conf, normeddf, inputs, ouputs):\n",
    "    modelFile    = conf['modelFile'] or \"models/simpleModel.h5\"\n",
    "    tsParams     = conf['tsParams']\n",
    "    lookahead    = conf['lookahead']\n",
    "    history      = tsParams['length']\n",
    "    train_pct    = conf.get('train_pct', 0.9)\n",
    "    train_count  = conf.get('train_count', int(len(normeddf) * train_pct) )\n",
    "\n",
    "    X, y = normeddf[inputs].values, normeddf[ouputs].values\n",
    "    X=X[:(-lookahead+1) or None]\n",
    "    y=y[lookahead-1:]\n",
    "\n",
    "    Xtrn,ytrn = X[:train_count], y[:train_count], \n",
    "    Xtst,ytst = X[train_count:], y[train_count:], \n",
    "\n",
    "    tsParams1 = tsParams.copy()\n",
    "    tsParams2 = tsParams.copy()\n",
    "    tsParams2['batch_size'] =1\n",
    "\n",
    "    trng1 = TimeseriesGenerator(Xtrn, ytrn, **tsParams1 )\n",
    "    valg1 = TimeseriesGenerator(Xtst, ytst, **tsParams1 )\n",
    "    valg2 = TimeseriesGenerator(X, y, **tsParams2 )\n",
    "\n",
    "    #history, tsParams1, len(trng1), len(valg1), len(valg2), #trng1[0]\n",
    "    #print(Xtrn.shape, \"\\n\", Xtrn, \"\\n\", ytrn.shape, \"\\n\", ytrn, Xtst.shape)\n",
    "\n",
    "    return modelFile, history, lookahead, trng1, valg1, valg2, X, y\n",
    "\n",
    "def getModel(conf):\n",
    "    modelFile = conf.get('modelFile', [\"model.h5\"])[0]\n",
    "    modelName = conf['modelName']\n",
    "    loadModel = conf.get('loadModel', 1)\n",
    "    tsParams  = conf['tsParams']\n",
    "    monitor   = conf.get('monitor', \"val_loss\")\n",
    "    \n",
    "    importName= modelName.split(\".\")[0]\n",
    "       \n",
    "    exec(f\"import {importName}\")\n",
    "\n",
    "    mcp= ccallbacks.ModelCheckAndLoad(modelFile, monitor, best=np.inf, stop_at=False, verbose=1)\n",
    "    m1 = None\n",
    "    if ( loadModel and os.path.exists(modelFile)):\n",
    "        m1 = mcp.load_ext()\n",
    "    if ( m1 is None):\n",
    "        m1=eval(modelName)\n",
    "\n",
    "    print(f'''\n",
    "    import: {importName}\n",
    "    using : {modelName}\n",
    "    saved : {modelFile}\n",
    "    reload: {loadModel}, exists: {os.path.exists(modelFile)},best: {mcp.best}\n",
    "    ''')\n",
    "        \n",
    "    return m1, mcp\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "def predictyh(valg, modelFile=\"\", model=None  ):\n",
    "    m1 = model or keras.models.load_model(modelFile)\n",
    "    yh=m1.predict(valg)\n",
    "    return yh, m1\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "def printModelMetrics(columns, yy1, yh1):\n",
    "    mts = [\"column                       r2            fit    maxYd    minYd    stdYd\"]\n",
    "    for i in range(yh1.shape[-1]):\n",
    "        yyy = yy1[:,i]\n",
    "        yyh = yh1[:,i]\n",
    "        r2v = sklearn.metrics.r2_score(yyy, yyh)\n",
    "        col = columns[i+1]\n",
    "\n",
    "        yd = yyy - yyh\n",
    "        yym= np.mean(yyy)\n",
    "        dn = np.sum((yyy - yym)**2)+0.000000000001\n",
    "        nm = np.sum(yd ** 2)\n",
    "        fit= 1 - np.sqrt(nm/dn)\n",
    "\n",
    "        mxd = max(abs(yd))\n",
    "        mid = min(abs(yd))\n",
    "        syd = np.std(yd)\n",
    "        op = col, r2v, fit, yym, mxd, mid, syd\n",
    "\n",
    "        a = f\"{col:24} {r2v:>9.4} {max(0.,fit):>11.5} {mxd:8.3} {mid:8.3} {syd:10.6}\"\n",
    "        mts.append(a)\n",
    "    return mts\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "def fit(model, trng1, valg1, mcpoint,validation_steps=50, vv =0, ep = 1, spe =200 ):\n",
    "    model.fit(trng1, verbose=vv, epochs=ep, validation_data=valg1,steps_per_epoch=spe, shuffle=True, \n",
    "                        validation_steps=validation_steps, callbacks=[mcpoint])\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "def main(config=None, epochs=0):\n",
    "    # Step 1 - read the files from configuration\n",
    "    \n",
    "    conf, unnormdf, normeddf, inps, oups    = getConf(config)\n",
    "    modelFile, history, lookahead, trng1, valg1, valg2, X, y = getGenerators(conf, normeddf, inps, oups)\n",
    "    \n",
    "    model, mcpoint = getModel(conf)\n",
    "    losses = None\n",
    "    if ( epochs > 0):\n",
    "        print(f\"Calling fit : #epochs {epochs} : {mcpoint.best}\")\n",
    "        losses = fit(model, trng1, valg1, mcpoint, ep=epochs)\n",
    "        mcpoint.save_ext()\n",
    "\n",
    "    return conf, unnormdf, normeddf, inps, oups, model, mcpoint, trng1, valg1, valg2, X, y, losses\n",
    "        \n",
    "#-----------------------------------------------------------------------------------\n",
    "sysargs=None\n",
    "def addargs():\n",
    "    sysargs = None\n",
    "    p = argparse.ArgumentParser(f\"{os.path.basename(sys.argv[0])}:\")\n",
    "    p.add_argument('-c', '--config', type=str, required=True, default=\"\", help=\"config file\")\n",
    "    p.add_argument('-e', '--epochs', type=int, default=10, help=\"# of epochs!\")\n",
    "    p.add_argument('files', nargs='*')\n",
    "\n",
    "    try:\n",
    "        sysargs=p.parse_args(sys.argv[1:])\n",
    "    except argparse.ArgumentError as exc:\n",
    "        print(exc.message )\n",
    "\n",
    "    return sysargs\n",
    "#-----------------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    if ( not lstmutils1.inJupyter()):\n",
    "        t1 = datetime.datetime.now()\n",
    "        sysargs = addargs()\n",
    "        main(sysargs.config, sysargs.epochs)\n",
    "        t2 = datetime.datetime.now()\n",
    "        print(f\"{sys.argv[0]}: All Done in {str(t2-t1)} ***\")\n",
    "    else: \n",
    "        #dbs= client.get_list_database()\n",
    "        #print(\"Running in Jupyter\", dbs)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile  ~/bin/genfit.py\n",
    "#!/usr/local/bin/python \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re, sys, os, datetime, getopt, glob, argparse, json, base64, pickle, sklearn\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (16, 5)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "import tensorflow as tf\n",
    "pd.options.display.max_rows = 8\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "\n",
    "sys.path.append(\"gen\")\n",
    "sys.path.append(\"/opt/LMCO/git/bin\")\n",
    "import lstmutils1;\n",
    "import lstmfit;\n",
    "import ccallbacks\n",
    "\n",
    "np.random.seed(4)\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "def predictyh(valg, modelFile=\"\", model=None  ):\n",
    "    m1 = model or keras.models.load_model(modelFile)\n",
    "    yh=m1.predict(valg)\n",
    "    return yh, m1\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "def getModelMetrics(columns, yy1, yh1):\n",
    "    #mts = [\"column                       r2            fit    maxYd    minYd    stdYd\"]\n",
    "    mtsc = \"column r2  fit maxYd minYd, std\".split()\n",
    "    mts  = []\n",
    "    for i in range(yh1.shape[-1]):\n",
    "        yyy = yy1[:,i]\n",
    "        yyh = yh1[:,i]\n",
    "        r2v = sklearn.metrics.r2_score(yyy, yyh)\n",
    "        col = columns[i+1]\n",
    "\n",
    "        yd = yyy - yyh\n",
    "        yym= np.mean(yyy)\n",
    "        dn = np.sum((yyy - yym)**2)+0.000000000001\n",
    "        nm = np.sum(yd ** 2)\n",
    "        fit= 1 - np.sqrt(nm/dn)\n",
    "\n",
    "        mxd = max(abs(yd))\n",
    "        mid = min(abs(yd))\n",
    "        syd = np.std(yd)\n",
    "\n",
    "        #a = f\"{col:24} {r2v:>9.4} {max(0.,fit):>11.5} {mxd:8.3} {mid:8.3} {syd:10.6}\"\n",
    "        a = [col, r2v, max(0.,fit), mxd, mid, syd]\n",
    "        mts.append(a)\n",
    "    return mts, mtsc\n",
    "#-----------------------------------------------------------------------------------\n",
    "def main(configFile):\n",
    "    conf, unnormdf, normeddf, inputs, ouputs = lstmfit.getConf(configFile)\n",
    "    lookAhead, valg,gt = lstmfit.getGeneratorsPreds(conf, normeddf)\n",
    "    \n",
    "    model, mcpoint = lstmfit.getModel(conf)\n",
    "\n",
    "    yy1 = np.array([valg[i][1][0] for i in range(len(valg))])\n",
    "    yh1,_ = predictyh(valg, None, model)\n",
    "\n",
    "    mts, mtsc=getModelMetrics(normeddf.columns, yy1, yh1)\n",
    "    \n",
    "    \n",
    "    opt=pd.DataFrame(mts, columns=mtsc)\n",
    "    \n",
    "    sfile = conf['modelFile'][0] + \"_metrics\"\n",
    "    opt.to_csv(sfile, index=False)\n",
    "    \n",
    "    print(f\"Saved to file: {sfile}\")\n",
    "    return opt\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "sysargs=None\n",
    "def addargs():\n",
    "    sysargs = None\n",
    "    p = argparse.ArgumentParser(f\"{os.path.basename(sys.argv[0])}:\")\n",
    "    p.add_argument('-c', '--config', type=str, required=True, default=\"\", help=\"config file\")\n",
    "\n",
    "    try:\n",
    "        sysargs=p.parse_args(sys.argv[1:])\n",
    "    except argparse.ArgumentError as exc:\n",
    "        print(exc.message )\n",
    "\n",
    "    return sysargs\n",
    "#-----------------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    if ( not lstmutils1.inJupyter()):\n",
    "        t1 = datetime.datetime.now()\n",
    "        sysargs = addargs()\n",
    "        main(sysargs.config,)\n",
    "        t2 = datetime.datetime.now()\n",
    "        print(f\"{sys.argv[0]}: All Done in {str(t2-t1)} ***\")\n",
    "    else: \n",
    "        #dbs= client.get_list_database()\n",
    "        #print(\"Running in Jupyter\", dbs)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTENDED Stuff for Future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookahead = 3\n",
    "X, y = np.array(range(20)), np.array(range(20))\n",
    "X=X[:(-lookahead+1) or None]\n",
    "y=y[lookahead-1:]\n",
    "print(f'{X}\\n{y}')\n",
    "\n",
    "print(\"---------------\")\n",
    "tsParams  = {\"length\": 5, \"batch_size\": 1, \"stride\": 1, \"sampling_rate\": 1}\n",
    "train_count = len(X) * 6 //10\n",
    "\n",
    "Xtrn,ytrn = X[:train_count], y[:train_count]\n",
    "Xtst,ytst = X[train_count:], y[train_count:]\n",
    "#Xtrn,ytrn = X, y \n",
    "\n",
    "print(Xtrn.shape, \"\\n\", Xtrn, \"\\n\", ytrn.shape, \"\\n\", ytrn)\n",
    "print(f'Test: {Xtst} {ytst}')\n",
    "tsParams1 = tsParams.copy()\n",
    "tsParams2 = tsParams.copy()\n",
    "tsParams2['batch_size'] =1\n",
    "\n",
    "trng1 = TimeseriesGenerator(Xtrn, ytrn, **tsParams1 )\n",
    "valg1 = TimeseriesGenerator(Xtst, ytst, **tsParams1 )\n",
    "\n",
    "for i in range(len(trng1)):\n",
    "    print(f\"{trng1[i][0]} : {trng1[i][1]}\")\n",
    "print(\"Testing ---\")\n",
    "for i in range(len(valg1)):\n",
    "    print(f\"{valg1[i][0]} : {valg1[i][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1 = pd.DataFrame([i for i in range(0,5)])\n",
    "tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "tf1[f'{c}___diff1'] = tf1[c] - tf1[c].shift(1)\n",
    "tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1[c].shift(1)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "241.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
