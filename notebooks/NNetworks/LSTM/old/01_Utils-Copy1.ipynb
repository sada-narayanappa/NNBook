{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "mkdir: gen: File exists\r\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, datetime\n",
    "import tensorflow as tf\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from IPython import display\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (16, 5)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "pd.options.display.max_rows = 5\n",
    "%matplotlib inline  \n",
    "\n",
    "!mkdir gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gen/getstocksdata.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile  gen/getstocksdata.py\n",
    "#!/usr/local/bin/python \n",
    "\n",
    "'''\n",
    "The following code: it is just here to protect my API-KEY code not really useful\n",
    "for the problem at hand. If your key is in your or update your API_KEY then you may safely ignore \n",
    "this code.\n",
    "'''\n",
    "\n",
    "def getkey(key='password'):\n",
    "    API_KEY, lines, file =None, None, os.path.expanduser('~/.keys/keys.json')\n",
    "    if os.path.exists(file):\n",
    "        j = json.load(open(file, 'r'))\n",
    "        return j['AV_API_KEY'], j['NEWSAPI_KEY']\n",
    "\n",
    "    #alpha_vantage key\n",
    "    avk = decrypt('baTEje52rx+kAsuAN9PdxMeC03p/HuRVTzskLiso1/c=', key)\n",
    "    newsk = decrypt('505Db5sDvHvptBPzE8IhsewneuanOKV3gKpN+26lS3A=', key)\n",
    "    return avk, newsk;\n",
    "        \n",
    "def encrypt(msg_text = b'message', secret_key='password'):\n",
    "    if (type(msg_text) == str):\n",
    "        msg_text = bytes(msg_text, encoding='utf-8').rjust(32)\n",
    "    if (type(secret_key) == str):\n",
    "        secret_key = bytes(secret_key, encoding='utf-8') .rjust(32)\n",
    "\n",
    "    cipher = AES.new(secret_key,AES.MODE_ECB) \n",
    "    encoded = base64.b64encode(cipher.encrypt(msg_text))\n",
    "    ret = encoded.decode(\"utf-8\")\n",
    "    print(ret)\n",
    "    return ret\n",
    "\n",
    "def decrypt(encoded, secret_key='password'):\n",
    "    if (type(secret_key) == str):\n",
    "        secret_key = bytes(secret_key, encoding='utf-8') .rjust(32)\n",
    "\n",
    "    cipher = AES.new(secret_key,AES.MODE_ECB) \n",
    "    if (type(encoded) == str):\n",
    "        encoded = bytes(encoded, encoding='utf-8')\n",
    "    decoded = cipher.decrypt(base64.b64decode(encoded))\n",
    "    ret =decoded.decode(\"utf-8\").strip()\n",
    "    print(ret)\n",
    "    return ret\n",
    "\n",
    "'''\n",
    "This will read required symbols and saves them to data directory\n",
    "'''\n",
    "def save_data(symbol, API_KEY=\"\", check=True):\n",
    "    from alpha_vantage.timeseries import TimeSeries\n",
    "    \n",
    "    outf = f'./data/daily_{symbol}.csv'\n",
    "    if (check and os.path.exists(outf)):\n",
    "        print(f\"{outf:22} exists, ... nothing to do\")\n",
    "        return;\n",
    "        \n",
    "    ts = TimeSeries(key=API_KEY, output_format='pandas')\n",
    "    data, meta_data = ts.get_daily(symbol, outputsize='full')\n",
    "    data.insert(0, 'timestamp', value=data.index)\n",
    "\n",
    "    data.columns = 'timestamp,open,high,low,close,volume'.split(',')\n",
    "    data.to_csv(outf, index=False)\n",
    "    return data\n",
    "\n",
    "'''\n",
    "Read all the files in data with daily_*, reads them and return a list\n",
    "'''\n",
    "def read_data():\n",
    "    a={}\n",
    "    for f in glob.glob('data/daily_*'):\n",
    "        symbol = os.path.basename(os.path.splitext(f)[0]).split(\"_\")[1]\n",
    "        print(f'Reading {f} Symbol: {symbol}')\n",
    "        df = pd.read_csv(f)\n",
    "        df.sort_values(by='timestamp', ascending=False, inplace=True)\n",
    "        df.index=(range(0,len(df)))\n",
    "        ncs = ['timestamp'] + [f'{symbol}_{c}' for c in df.columns[1:]]\n",
    "        df.columns = ncs\n",
    "        a[symbol] = df\n",
    "        \n",
    "    maxrows = min([len(d) for d in a.values()])\n",
    "    return a, maxrows, df\n",
    "\n",
    "'''\n",
    "Combines all the dataframes into one\n",
    "\n",
    "The problem for us to join multiple index funds from ASIA is that they have different holidays\n",
    "Therefore we have gaps in the trading days. Therefore \n",
    "'''\n",
    "def combine_data(a, outp=\"data/stockdata.csv\"):\n",
    "    # Different excahnges have various holidays, therefore values may be missing  \n",
    "    # Get All Data Frames and their corresponding time stamps\n",
    "    #\n",
    "    ar=np.array([d['timestamp'].values for d in a.values()])\n",
    "    at=np.concatenate(ar)\n",
    "    at=set(at)\n",
    "    af = pd.DataFrame()\n",
    "    af['timestamp'] = list(at);\n",
    "\n",
    "    for k,v in a.items():\n",
    "        print(f\"Getting {k:32} \\r\", end='')\n",
    "        af=pd.merge(af,v, how=\"left\", left_on=\"timestamp\".split(), right_on=\"timestamp\".split())\n",
    "    print()\n",
    "    af.sort_values(by='timestamp', ascending=False, inplace=True)\n",
    "    af = af.reset_index(drop=True)\n",
    "    af = af.fillna(method='ffill' ).fillna(method='bfill')\n",
    "    af.to_csv(outp, index=False)\n",
    "    return af\n",
    "\n",
    "def needNewData(stockfile=\"data/stockdata.csv\"):\n",
    "    file= stockfile\n",
    "    c1 = os.path.exists(file)\n",
    "    c2 = os.stat(file)\n",
    "    t1 = pd.to_datetime(c2.st_mtime, unit=\"s\", utc=False)\n",
    "    t1.day\n",
    "    t2 = pd.to_datetime(datetime.datetime.now())\n",
    "    wd = t2.weekday() < 5  # means not saturday or sunday\n",
    "\n",
    "    tdf = t2 - t1\n",
    "    if ( (wd and tdf.seconds < 4) ):\n",
    "        print(f\"{file} is up to date {t1.day}, {t2.day} \")\n",
    "        return False;\n",
    "    print(f'Fetch new set of data')\n",
    "    return True\n",
    "\n",
    "def getdata(symbs='MSFT GLD GOOGL SPX AAPL IBM' , dontforce=False):\n",
    "    \n",
    "    stockfile=\"../data/stockdata.csv\"\n",
    "    if (not os.path.exists(stockfile)):\n",
    "        stockfile=\"data/stockdata.csv\"\n",
    "        \n",
    "    stockfile=\"data/stockdata.csv\"\n",
    "    if (not dontforce and not os.path.exists(stockfile)):\n",
    "        API_KEY = None  # Put your API KEY if you need to test data download or just use the data\n",
    "        API_KEY, NEWS_API_KEY = API_KEY,\"\" or getkey()\n",
    "\n",
    "        for f in symbs.split():\n",
    "            save_data(f, API_KEY=API_KEY)\n",
    "\n",
    "        ASIA='''TOKYO 6758.T HITACHI 6501.T HNGKNG 0168.HK SHANGAI 601288.SS SHNZEN'''\n",
    "        s=ASIA.split()\n",
    "        ASIA = {k[0]:k[1] for k in   zip(s[0::2], s[1::2])}\n",
    "\n",
    "        for k, v in ASIA.items():\n",
    "            print(f'getting data for {k} => symbol {v}')\n",
    "            save_data(v, API_KEY)\n",
    "\n",
    "        a, maxrows, ldf  = read_data()\n",
    "        nf= combine_data(a, stockfile)\n",
    "    \n",
    "    \n",
    "#-----------------------------------------------------------------------------------\n",
    "#*** NOTE: DO NOT EDIT THIS FILE - THIS iS CREATED FROM: inv_utils.ipynb\n",
    "def inJupyter():\n",
    "    try:    get_ipython; return True\n",
    "    except: return False\n",
    "#-----------------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    if (not inJupyter()):\n",
    "        t1 = datetime.datetime.now()\n",
    "        getdata()\n",
    "        t2 = datetime.datetime.now()\n",
    "        print(f\"#All Done in {str(t2-t1)} ***\")\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gen/dataconfig.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile  gen/dataconfig.py\n",
    "#!/usr/local/bin/python \n",
    "\n",
    "import re, sys, os, datetime, getopt, glob, argparse, datetime, json, base64, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#~~~~ Find any sensor highly correlated with time and drop them.\n",
    "def detectTimeCorrelated(df, val=0.94):\n",
    "    timeCorSensors = []\n",
    "    if ( not 'time' in df.columns): #assume first column is time column\n",
    "        dcols = ['time'] + [c for c in df.columns[1:]]\n",
    "        df.columns = dcols\n",
    "    \n",
    "    timeser = pd.Series(df[['time']].values.reshape(-1))\n",
    "    if ( timeser.dtype != np.number ):\n",
    "        timeser = pd.to_datetime(timeser).astype(int)\n",
    "    \n",
    "    \n",
    "    DROP_INDEX = 0;\n",
    "    for sensor in df.columns:\n",
    "        if (sensor == 'time'):\n",
    "            continue;\n",
    "        #print(f\"#Testing {sensor}...\")\n",
    "        sensorSeries = pd.Series(df[sensor].values.reshape(-1))\n",
    "        for i in range(8):\n",
    "            c1 = timeser[i:].corr(sensorSeries[i:])\n",
    "            c2 = timeser[i:].corr(sensorSeries[:-i])\n",
    "            if np.abs(c1) >= val or np.abs(c2) >= val:\n",
    "                timeCorSensors.append(sensor)\n",
    "                DROP_INDEX = max(DROP_INDEX, i) #lets drop first few rows\n",
    "                break;\n",
    "                \n",
    "    if ( len(timeCorSensors) > 0):\n",
    "        print(f\"#Time Cor: #{len(timeCorSensors)}, #Shape before:{df.shape}\")\n",
    "        #df.drop(timeCorSensors, axis=1, inplace=True)\n",
    "        #df = df[DROP_INDEX:]\n",
    "        #print(f\"#After dropping: {DROP_INDEX} =>{df.shape}\")\n",
    "        \n",
    "    return timeCorSensors\n",
    "#-----------------------------------------------------------------------------------\n",
    "def precheck(df):\n",
    "    cols = df.columns[df.dtypes.eq('object')]\n",
    "    if (len(cols) > 0):\n",
    "        print(f\"WARNING: *** Non numeric columns => {cols}\")\n",
    "        return 0\n",
    "    return 1\n",
    "#-----------------------------------------------------------------------------------\n",
    "def detectCols(file, nUnique=4, tcoeff=0.92):\n",
    "    tf1 = file\n",
    "    if (type(tf1) == str):\n",
    "        tf1 = pd.read_csv(tf1, comment=\"#\")\n",
    "    \n",
    "    #Lets check if it has any non-numeric columns! Warning\n",
    "    precheck(tf1)\n",
    "    \n",
    "    unique_vals  = tf1.nunique()\n",
    "    constantCols = unique_vals[ unique_vals == 1].index                             # constant Columns\n",
    "    onehotECols  = unique_vals[(unique_vals > 2 ) & (unique_vals<=nUnique)].index   # Categorical Columns\n",
    "    categorCols  = unique_vals[(unique_vals >=2 ) & (unique_vals <= nUnique)].index # Categorical Columns\n",
    "    binaryCols   = unique_vals[(unique_vals == 2)].index                      # Binary\n",
    "\n",
    "    numericCols  = tf1.select_dtypes(include=np.number).columns           # numerics\n",
    "    numericCols  = [c for c in numericCols if c not in categorCols]\n",
    "    numericCols  = [c for c in numericCols if c not in constantCols]\n",
    "    notNumerics  = tf1.select_dtypes(exclude=np.number).columns           # non - numerics\n",
    "    notNumerics  = [c for c in notNumerics if c not in categorCols]       # non - numerics\n",
    "\n",
    "    try:\n",
    "        timeCorCols  = detectTimeCorrelated(tf1, tcoeff)\n",
    "    except:\n",
    "        timeCorCols = []\n",
    "        \n",
    "    ret = {\n",
    "            \"file\"          : [file] if (type(file) == str) else [\"??\"],\n",
    "            \"number_Unique\" : nUnique            , \n",
    "            \"constantCols\"  : list(constantCols ),   # No Signals\n",
    "            \"categorCols\"   : list(categorCols  ),   # Categorical Columns\n",
    "            \"onehotECols\"   : list(onehotECols  ),   # Cats > 2 and < Unique Values\n",
    "            \"binaryCols\"    : list(binaryCols   ),   # Binary\n",
    "            \"numericCols\"   : list(numericCols  ),  \n",
    "            \"notNumerics\"   : list(notNumerics  ),\n",
    "            \"timeCorrelated\": tcoeff             ,   # Time correlated\n",
    "            \"timeCorrCols\"  : list(timeCorCols  ),   # Time correlated Columns\n",
    "            \"timeCorrLength\": len(timeCorCols  ) ,   # Time correlated Columns length\n",
    "            \"excludePattern\": [] , #Exclude patterns\n",
    "            \"includePattern\": [] , #include patterns\n",
    "            \"dropColumns\"   : [],\n",
    "            \"finalColumns\"  : [\"binaryCols\", \"numericCols\", \"onehotECols\"],\n",
    "            \"scalerString\"  : [],\n",
    "            \"addDiffs\"      : [],\n",
    "            \"train_pct\"     : 100,\n",
    "            \"train_count\"   : len(tf1),\n",
    "        }\n",
    "    return ret, tf1;\n",
    "#-----------------------------------------------------------------------------------\n",
    "'''\n",
    "This will scale the numeric columns - if this changes - you need to use dataprep\n",
    "'''\n",
    "def scaleNumerics(tf1, numericCols=[], train_pct=0.8, config={}):\n",
    "    ltf = tf1[numericCols];\n",
    "    trn_split_count = int(len(ltf) * train_pct) if (train_pct < 1) else train_pct\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    dfs = scaler.fit(ltf[:trn_split_count])\n",
    "    ltf = (ltf-dfs.mean_)/dfs.scale_\n",
    "\n",
    "    scalerstr = base64.b64encode(pickle.dumps(scaler, protocol=None, fix_imports=True)).decode(\"utf-8\")\n",
    "    config[\"scalerString\"] = [scalerstr]\n",
    "    config[\"train_pct\"   ] = train_pct\n",
    "    config[\"train_count\" ] = trn_split_count\n",
    "    \n",
    "    return ltf, scaler, trn_split_count\n",
    "#-----------------------------------------------------------------------------------\n",
    "#*** NOTE: DO NOT EDIT THIS FILE - THIS iS CREATED FROM: inv_utils.ipynb\n",
    "def inJupyter():\n",
    "    try:    get_ipython; return True\n",
    "    except: return False\n",
    "\n",
    "def formatConfig(out: dict):\n",
    "    #outj = json.dumps(out, sort_keys=False, indent=None)\n",
    "    outj = f\"[START]\\n{{\\n\"\n",
    "    for k,v in out.items():\n",
    "        kk = f'\"{k}\"'\n",
    "        outj += f'{kk:>20}: {v},\\n'\n",
    "    outj += '\"end\": 0 \\n}\\n[END]\\n'\n",
    "    \n",
    "    return outj\n",
    "    \n",
    "def process():\n",
    "    n  = len(sysargs.input_files)\n",
    "    un = sysargs.unique\n",
    "    tc = sysargs.tcoeff\n",
    "    for i, file1 in enumerate(sysargs.input_files):\n",
    "        print(f\"#=>Processing {i+1}/{n} {file1} #unique: {un} tcoeff: {tc} - standby\")\n",
    "        out,df = detectCols(file1, un, tc)\n",
    "        \n",
    "        trnSplit = sysargs.tsplit\n",
    "        scaleNumerics(df, out[\"numericCols\"], trnSplit, out)\n",
    "        \n",
    "        #Prepare to save to file\n",
    "        out['file'] = [os.path.abspath(file1)];\n",
    "        outj = formatConfig(out)\n",
    "        break;\n",
    "    print(outj)\n",
    "    return outj\n",
    "    \n",
    "#-----------------------------------------------------------------------------------\n",
    "sysargs=None\n",
    "def addargs():\n",
    "    sysargs = None\n",
    "    p = argparse.ArgumentParser(f\"{os.path.basename(sys.argv[0])}:\")\n",
    "    p.add_argument('-u', '--unique', type=int,   default=6,    help=\"# of unique values!\")\n",
    "    p.add_argument('-t', '--tcoeff', type=float, default=0.94, help=\"# Time Correlation value Sensors!\")\n",
    "    p.add_argument('-s', '--scale',  type=bool,  default=1,    help=\"# Find Scaler!\")\n",
    "    p.add_argument('-p', '--tsplit', type=float, default=0.7,  help=\"# Find training Split!\")\n",
    "    p.add_argument('args', nargs=argparse.REMAINDER)\n",
    "    p.add_argument('input_files',action=\"store\", type=str, nargs='+', help=\"input file(s)\")\n",
    "\n",
    "    #p.print_help() # always print -help\n",
    "    try:\n",
    "        sysargs=p.parse_args(sys.argv[1:])\n",
    "        #print(f'using:\\n{sysargs}')\n",
    "    except argparse.ArgumentError as exc:\n",
    "        #par.print_help()\n",
    "        print(exc.message )\n",
    "        \n",
    "    return sysargs\n",
    "#-----------------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    if (not inJupyter()):\n",
    "        t1 = datetime.datetime.now()\n",
    "        sysargs = addargs()\n",
    "        process()\n",
    "        t2 = datetime.datetime.now()\n",
    "        print(f\"#All Done in {str(t2-t1)} ***\")\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile  gen/dataprepare.py\n",
    "#!/usr/local/bin/python \n",
    "\n",
    "import re, sys, os, datetime, getopt, glob, argparse, datetime, json, base64, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"gen\")\n",
    "import dataconfig;\n",
    "\n",
    "'Covert to one_hot encoding with prefix for columns'\n",
    "def makeOneHot(tf1, oheCols=[]):\n",
    "    ohe = pd.DataFrame();\n",
    "    for c in oheCols:\n",
    "        one_hot = pd.get_dummies(tf1[c])\n",
    "        nc = [f'{c}__{k}' for k in one_hot.columns]\n",
    "        one_hot.columns = nc\n",
    "        ohe = pd.concat([ohe, one_hot], axis=1)\n",
    "\n",
    "    return ohe\n",
    "\n",
    "def addDiff(tf1, col):\n",
    "    #col = \"MSFT_open\"\n",
    "    tf1[f'{col}_diff1'] = tf1[col] - tf1[col].shift(-1)\n",
    "    return tf1\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "'''\n",
    "This will scale the numeric columns - if this changes - you need to use dataprep\n",
    "'''\n",
    "def scaleNumerics(tf1, numericCols=[], train_pct=0.8, config={}):\n",
    "    ltf = tf1[numericCols];\n",
    "    trn_split_count = int(len(ltf) * train_pct) if (train_pct < 1) else train_pct\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    dfs = scaler.fit(ltf[:trn_split_count])\n",
    "    ltf = (ltf-dfs.mean_)/dfs.scale_\n",
    "\n",
    "    scalerstr = base64.b64encode(pickle.dumps(scaler, protocol=None, fix_imports=True)).decode(\"utf-8\")\n",
    "    config[\"scalerString\"] = [scalerstr]\n",
    "    config[\"train_pct\"   ] = train_pct\n",
    "    config[\"train_count\" ] = trn_split_count\n",
    "    \n",
    "    return ltf, scaler, trn_split_count\n",
    "#-----------------------------------------------------------------------------------\n",
    "def formatConfig(out: dict):\n",
    "    #outj = json.dumps(out, sort_keys=False, indent=None)\n",
    "    outj = f\"[START]\\n{{\\n\"\n",
    "    for k,v in out.items():\n",
    "        kk = f'\"{k}\"'\n",
    "        outj += f'{kk:>20}: {v},\\n'\n",
    "    outj += '\"end\": 0 \\n}\\n[END]\\n'\n",
    "    \n",
    "    return outj\n",
    "#-----------------------------------------------------------------------------------\n",
    "def getJSONconfig(cf = \"config*\"):\n",
    "    with open(cf, \"r\") as f:\n",
    "        cf = f.read()\n",
    "\n",
    "    r1=re.findall(\"\\[START](.*)\\[END]\", cf, flags=re.MULTILINE|re.DOTALL)\n",
    "    if ( len(r1) <= 0):\n",
    "        print(f\"Ignoring: Configuration not found in {cf}! no worries\")\n",
    "        return None\n",
    "    r1 = r1[0].replace(\"'\", '\"')    \n",
    "    rj = json.loads(r1)\n",
    "    return rj\n",
    "    \n",
    "def getconfig(cf = \"config*\"):\n",
    "    confFiles = sorted(glob.glob(cf))\n",
    "    if ( len(confFiles) <= 0):\n",
    "        print(f\"No Configuration files {cf} found!!!\")\n",
    "        exit(1)\n",
    "\n",
    "    # Read and merge the configuration files\n",
    "    ret = {}\n",
    "    for cf in confFiles:\n",
    "        print(f\"#Getting Configuration from {cf}\")\n",
    "        rj = getJSONconfig(cf)\n",
    "        if (rj):\n",
    "            ret.update(rj)\n",
    "        \n",
    "    scalerStr= ret.get('scalerString', [])\n",
    "    if len(scalerStr) > 0:\n",
    "        scalerStr = scalerStr[0]\n",
    "        decoded = base64.b64decode(scalerStr)\n",
    "        scaler = pickle.loads(decoded,fix_imports=True)\n",
    "        ret['scaler'] = scaler\n",
    "    else:\n",
    "        ret['scaler'] = []\n",
    "        \n",
    "    return ret\n",
    "#-----------------------------------------------------------------------------------\n",
    "def process(config, input_files, outputFile=None):\n",
    "    conf = getconfig(config)\n",
    "    \n",
    "    n  = len(input_files)\n",
    "    adf = pd.DataFrame();\n",
    "    for i, file1 in enumerate(input_files):\n",
    "        print(f\"=>Processing {i+1}/{n} {file1} - standby\")\n",
    "        df = pd.read_csv(file1, comment='#')\n",
    "        \n",
    "        # STEP 0: Add Differecing Columns\n",
    "        cdiffs = conf.get('addDiffs', [])\n",
    "        for c in cdiffs:\n",
    "            addDiff(df, c)\n",
    "\n",
    "        drps = conf.get(\"dropColumns\", [])\n",
    "        df.drop(drps, inplace=True, errors=\"ignore\")\n",
    "        df.sort_values(by=df.columns[0], ascending=True, inplace=True)\n",
    "        df = df.reset_index(drop=True)\n",
    "            \n",
    "        # STEP 1. --> Scale all Numeric Columns if scaler exists\n",
    "        dfn= df[conf['numericCols']]    # Get all Numeric Columns\n",
    "        \n",
    "        scale  = conf.get('scale', 0)\n",
    "        scaler = conf['scaler']\n",
    "        \n",
    "        if ( scale and scaler ):                  # I know we are assuming Standard Scaler now\n",
    "            dfn = (dfn-scaler.mean_)/scaler.scale_\n",
    "        if ( scale and not scaler ):              # Find a new Scaler\n",
    "            dfn, scaler, trn_split_count = scaleNumerics(dfn, dfn.columns, conf['train_pct'], conf)\n",
    "            \n",
    "        # STEP 2: => One hot encode \n",
    "        ohe = None\n",
    "        if len(conf['onehotECols']) > 0:\n",
    "            ohe=makeOneHot(df, conf['onehotECols'])\n",
    "        \n",
    "        # STEP 3: Get Binary columns\n",
    "        binaryCols= conf['binaryCols']\n",
    "        \n",
    "        # => Todo: I Know I Should account for Exclude Patterns here\n",
    "        \n",
    "        #FINALLY: Concatenate all the data\n",
    "        ndf= pd.concat([df[binaryCols], dfn, ohe, ], axis=1)\n",
    "        adf= pd.concat([adf, ndf], axis=0)\n",
    "        \n",
    "        break; # Will do only one file now - fix it later\n",
    "    \n",
    "        \n",
    "    # Lets Do one final clean up \n",
    "    adf.dropna(inplace=True)\n",
    "    adf.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    conf['resultCols'] = [c for c in adf.columns]\n",
    "    outj = formatConfig(conf)\n",
    "    print(outj)\n",
    "    \n",
    "    if(outputFile):\n",
    "        adf.to_csv(outputFile, index=False)\n",
    "        \n",
    "    return conf, df, adf;\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "sysargs=None\n",
    "def addargs():\n",
    "    sysargs = None\n",
    "    p = argparse.ArgumentParser(f\"{os.path.basename(sys.argv[0])}:\")\n",
    "    p.add_argument('-c', '--config', type=str, default=\"config.txt\", help=\"Config Files\")\n",
    "    p.add_argument('-o', '--output', type=str, default=\"out.csv\", help=\"output file\")\n",
    "    p.add_argument('args', nargs=argparse.REMAINDER)\n",
    "    p.add_argument('input_files',action=\"store\", type=str, nargs='+', help=\"input file(s)\")\n",
    "\n",
    "    #p.print_help() # always print -help\n",
    "    try:\n",
    "        sysargs=p.parse_args(sys.argv[1:])\n",
    "    except argparse.ArgumentError as exc:\n",
    "        print(exc.message )\n",
    "        \n",
    "    return sysargs\n",
    "#-----------------------------------------------------------------------------------\n",
    "#*** NOTE: DO NOT EDIT THIS FILE - THIS iS CREATED FROM: inv_utils.ipynb\n",
    "def inJupyter():\n",
    "    try:    get_ipython; return True\n",
    "    except: return False\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    if (not inJupyter()):\n",
    "        t1 = datetime.datetime.now()\n",
    "        sysargs = addargs()\n",
    "        ret = process(sysargs.config, sysargs.input_files, sysargs.output)\n",
    "        t2 = datetime.datetime.now()\n",
    "        print(f\"#All Done in {str(t2-t1)} ***\")\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "f='/opt/SCHAS/NNBook/notebooks/NNetworks/LSTM/data/stockdata_ext.csv'\n",
    "#df=pd.read_csv(f)\n",
    "#df=df['MSFT_open', 'MSFT_high', 'MSFT_low', 'MSFT_close', 'MSFT_volume']\n",
    "#df.sort_values(by=df.columns[0], inplace=True)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Getting Configuration from gen/myconfig\n",
      "=>Processing 1/1 /opt/SCHAS/NNBook/notebooks/NNetworks/LSTM/data/stockdata_ext.csv - standby\n",
      "[START]\n",
      "{\n",
      "              \"file\": ['/opt/SCHAS/NNBook/notebooks/NNetworks/LSTM/data/stockdata_ext.csv'],\n",
      "     \"number_Unique\": 6,\n",
      "      \"constantCols\": [],\n",
      "       \"categorCols\": ['MSFT_-ve', 'MSFT_+ve'],\n",
      "       \"onehotECols\": ['MSFT_+ve'],\n",
      "        \"binaryCols\": ['MSFT_-ve'],\n",
      "       \"numericCols\": ['MSFT_open', 'MSFT_high', 'MSFT_low', 'MSFT_close', 'MSFT_volume', 'MSFT_open_diff1'],\n",
      "       \"notNumerics\": ['timestamp'],\n",
      "    \"timeCorrelated\": 0.94,\n",
      "      \"timeCorrCols\": [],\n",
      "    \"timeCorrLength\": 0,\n",
      "    \"excludePattern\": [],\n",
      "    \"includePattern\": [],\n",
      "       \"dropColumns\": [],\n",
      "      \"finalColumns\": ['binaryCols', 'numericCols', 'onehotECols'],\n",
      "      \"scalerString\": ['gANjc2tsZWFybi5wcmVwcm9jZXNzaW5nLmRhdGEKU3RhbmRhcmRTY2FsZXIKcQApgXEBfXECKFgJAAAAd2l0aF9tZWFucQOIWAgAAAB3aXRoX3N0ZHEEiFgEAAAAY29weXEFiFgPAAAAbl9zYW1wbGVzX3NlZW5fcQZjbnVtcHkuY29yZS5tdWx0aWFycmF5CnNjYWxhcgpxB2NudW1weQpkdHlwZQpxCFgCAAAAaThxCUsASwGHcQpScQsoSwNYAQAAADxxDE5OTkr/////Sv////9LAHRxDWJDCFcOAAAAAAAAcQ6GcQ9ScRBYBQAAAG1lYW5fcRFjbnVtcHkuY29yZS5tdWx0aWFycmF5Cl9yZWNvbnN0cnVjdApxEmNudW1weQpuZGFycmF5CnETSwCFcRRDAWJxFYdxFlJxFyhLAUsGhXEYaAhYAgAAAGY4cRlLAEsBh3EaUnEbKEsDaAxOTk5K/////0r/////SwB0cRxiiUMwEz3QfpGMSEAUSsvxkMNIQILXoq5/U0hAhk4AxYSOSEA986BcnbOGQeaTnSsUv6U/cR10cR5iWAQAAAB2YXJfcR9oEmgTSwCFcSBoFYdxIVJxIihLAUsGhXEjaBuJQzBde1ffgPeRQGTGJH2VNZJAI7nhjG6pkUD4EJjrmfeRQFhzCihikAlDkYZxQHjF5j9xJHRxJWJYBgAAAHNjYWxlX3EmaBJoE0sAhXEnaBWHcShScSkoSwFLBoVxKmgbiUMwQr9T9nT0QEBYuKKYphFBQDpqvEJ2z0BAJc7dx4D0QEAn/d5W/pl8QYgshxuA/uo/cSt0cSxiWBAAAABfc2tsZWFybl92ZXJzaW9ucS1YBgAAADAuMjEuM3EudWIu'],\n",
      "          \"addDiffs\": ['MSFT_open'],\n",
      "         \"train_pct\": 0.7,\n",
      "       \"train_count\": 3671,\n",
      "               \"end\": 0,\n",
      "             \"scale\": 1,\n",
      "        \"predictors\": ['MSFT_open', 'MSFT_high', 'MSFT_low', 'MSFT_close'],\n",
      "       \"outputsCols\": ['MSFT_open', 'MSFT_open_diff1'],\n",
      "          \"tsParams\": {'length': 5, 'batch_size': 1, 'stride': 1, 'sampling_rate': 1},\n",
      "     \"predictLength\": 5,\n",
      "            \"nsteps\": 1,\n",
      "            \"scaler\": StandardScaler(copy=True, with_mean=True, with_std=True),\n",
      "        \"resultCols\": ['MSFT_-ve', 'MSFT_open', 'MSFT_high', 'MSFT_low', 'MSFT_close', 'MSFT_volume', 'MSFT_open_diff1', 'MSFT_+ve__S__0', 'MSFT_+ve__S__1', 'MSFT_+ve__S__2'],\n",
      "\"end\": 0 \n",
      "}\n",
      "[END]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>MSFT_-ve</th>\n",
       "      <th>MSFT_+ve</th>\n",
       "      <th>MSFT_open</th>\n",
       "      <th>MSFT_high</th>\n",
       "      <th>MSFT_low</th>\n",
       "      <th>MSFT_close</th>\n",
       "      <th>MSFT_volume</th>\n",
       "      <th>0168.HK_open</th>\n",
       "      <th>0168.HK_high</th>\n",
       "      <th>...</th>\n",
       "      <th>6501.T_high</th>\n",
       "      <th>6501.T_low</th>\n",
       "      <th>6501.T_close</th>\n",
       "      <th>6501.T_volume</th>\n",
       "      <th>6758.T_open</th>\n",
       "      <th>6758.T_high</th>\n",
       "      <th>6758.T_low</th>\n",
       "      <th>6758.T_close</th>\n",
       "      <th>6758.T_volume</th>\n",
       "      <th>MSFT_open_diff1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>S__1</td>\n",
       "      <td>95.12</td>\n",
       "      <td>97.12</td>\n",
       "      <td>92.81</td>\n",
       "      <td>93.81</td>\n",
       "      <td>33148100.0</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.500</td>\n",
       "      <td>...</td>\n",
       "      <td>7440.0</td>\n",
       "      <td>7280.0</td>\n",
       "      <td>7380.0</td>\n",
       "      <td>1584800.0</td>\n",
       "      <td>15125.0</td>\n",
       "      <td>15300.0</td>\n",
       "      <td>14680.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>4384800.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>S__1</td>\n",
       "      <td>95.12</td>\n",
       "      <td>97.12</td>\n",
       "      <td>92.81</td>\n",
       "      <td>93.81</td>\n",
       "      <td>33148100.0</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.500</td>\n",
       "      <td>...</td>\n",
       "      <td>7440.0</td>\n",
       "      <td>7280.0</td>\n",
       "      <td>7380.0</td>\n",
       "      <td>1584800.0</td>\n",
       "      <td>15125.0</td>\n",
       "      <td>15300.0</td>\n",
       "      <td>14680.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>4384800.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>S__1</td>\n",
       "      <td>95.12</td>\n",
       "      <td>97.12</td>\n",
       "      <td>92.81</td>\n",
       "      <td>93.81</td>\n",
       "      <td>33148100.0</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.450</td>\n",
       "      <td>...</td>\n",
       "      <td>7440.0</td>\n",
       "      <td>7280.0</td>\n",
       "      <td>7380.0</td>\n",
       "      <td>1584800.0</td>\n",
       "      <td>15125.0</td>\n",
       "      <td>15300.0</td>\n",
       "      <td>14680.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>4384800.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-06</td>\n",
       "      <td>1</td>\n",
       "      <td>S__1</td>\n",
       "      <td>95.12</td>\n",
       "      <td>97.12</td>\n",
       "      <td>92.81</td>\n",
       "      <td>93.81</td>\n",
       "      <td>33148100.0</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.425</td>\n",
       "      <td>...</td>\n",
       "      <td>7440.0</td>\n",
       "      <td>7280.0</td>\n",
       "      <td>7380.0</td>\n",
       "      <td>1584800.0</td>\n",
       "      <td>15125.0</td>\n",
       "      <td>15300.0</td>\n",
       "      <td>14680.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>4384800.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp  MSFT_-ve MSFT_+ve  MSFT_open  MSFT_high  MSFT_low  MSFT_close  \\\n",
       "0  2000-01-03         1     S__1      95.12      97.12     92.81       93.81   \n",
       "1  2000-01-04         1     S__1      95.12      97.12     92.81       93.81   \n",
       "2  2000-01-05         1     S__1      95.12      97.12     92.81       93.81   \n",
       "3  2000-01-06         1     S__1      95.12      97.12     92.81       93.81   \n",
       "\n",
       "   MSFT_volume  0168.HK_open  0168.HK_high  ...  6501.T_high  6501.T_low  \\\n",
       "0   33148100.0          2.35         2.500  ...       7440.0      7280.0   \n",
       "1   33148100.0          2.35         2.500  ...       7440.0      7280.0   \n",
       "2   33148100.0          2.45         2.450  ...       7440.0      7280.0   \n",
       "3   33148100.0          2.35         2.425  ...       7440.0      7280.0   \n",
       "\n",
       "   6501.T_close  6501.T_volume  6758.T_open  6758.T_high  6758.T_low  \\\n",
       "0        7380.0      1584800.0      15125.0      15300.0     14680.0   \n",
       "1        7380.0      1584800.0      15125.0      15300.0     14680.0   \n",
       "2        7380.0      1584800.0      15125.0      15300.0     14680.0   \n",
       "3        7380.0      1584800.0      15125.0      15300.0     14680.0   \n",
       "\n",
       "   6758.T_close  6758.T_volume  MSFT_open_diff1  \n",
       "0       15000.0      4384800.0              NaN  \n",
       "1       15000.0      4384800.0              0.0  \n",
       "2       15000.0      4384800.0              0.0  \n",
       "3       15000.0      4384800.0              0.0  \n",
       "\n",
       "[4 rows x 54 columns]"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config=\"gen/myconfig\"\n",
    "conf, df, adf = process(config, ['/opt/SCHAS/NNBook/notebooks/NNetworks/LSTM/data/stockdata_ext.csv'], \"out.csv\")\n",
    "df[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile  gen/NN1.py\n",
    "#!/usr/local/bin/python \n",
    "\n",
    "import re, sys, os, datetime, getopt, glob, argparse, datetime, json, base64, pickle, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"gen\")\n",
    "import dataconfig;\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (16, 5)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "pd.options.display.max_rows = 5\n",
    "%matplotlib inline  \n",
    "\n",
    "use_keras=1\n",
    "if ( use_keras):\n",
    "    from keras.models import Sequential, Model\n",
    "    from keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate, TimeDistributed\n",
    "    from keras.layers import Conv1D, GlobalMaxPool1D,Flatten, Bidirectional, RepeatVector, MaxPooling1D\n",
    "    from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "    from keras import regularizers\n",
    "    from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, Callback\n",
    "    from keras import optimizers\n",
    "    from keras.models import load_model\n",
    "else:\n",
    "    from tensorflow.keras.models import Sequential, Model\n",
    "    from tensorflow.keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate, TimeDistributed\n",
    "    from tensorflow.keras.layers import Conv1D, GlobalMaxPool1D,Flatten, Bidirectional, RepeatVector, MaxPooling1D\n",
    "    from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "    from tensorflow.keras import regularizers\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, Callback\n",
    "    from tensorflow.keras import optimizers\n",
    "    from tensorflow.keras.models import load_model\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "def getJSONconfig(cf = \"config*\"):\n",
    "    with open(cf, \"r\") as f:\n",
    "        cf = f.read()\n",
    "\n",
    "    r1=re.findall(\"\\[START](.*)\\[END]\", cf, flags=re.MULTILINE|re.DOTALL)\n",
    "    if ( len(r1) <= 0):\n",
    "        print(f\"Ignoring: Configuration not found in {cf}! no worries\")\n",
    "        return None\n",
    "    r1 = r1[0].replace(\"'\", '\"')    \n",
    "    rj = json.loads(r1)\n",
    "    return rj\n",
    "#-----------------------------------------------------------------------------------\n",
    "def process(config, input_files, outputFile=None):\n",
    "    conf = getconfig(config)\n",
    "    \n",
    "    model1=None      \n",
    "\n",
    "    #sensN = len(self.train_transformed[0].columns)  # number of sensors (eliminating the two time ones)\n",
    "    #outN = len(self.num_id_list) # number of output sensors; the non-categorical ones        \n",
    "\n",
    "    lookBack   = tsParam['length']\n",
    "    nFeatures  = X.shape[1]  # Number of features \n",
    "    lstm_OPDim = y.shape[1]  # This is usually all sensors except categorical that to train LSTM on\n",
    "    lstm_IPDim = 256\n",
    "    drop       = 0.3\n",
    "    optimizer  = optimizers.Adam(lr=0.0005)\n",
    "    loss       = 'mse'\n",
    "    k_rrizer   = None\n",
    "    r_rrizer   = None\n",
    "\n",
    "    input_layer  = Input(shape=(lookBack, nFeatures), dtype='float32', name='input')\n",
    "    memory_layer = LSTM( lstm_IPDim, return_sequences=True, name=\"memory1\")(input_layer)\n",
    "    memory_layer = LSTM (int(lstm_IPDim/2), return_sequences=False, name=\"memory2\")(memory_layer)\n",
    "    repeated     = RepeatVector(lookBack)(memory_layer)\n",
    "    memory_layer = LSTM (int(lstm_IPDim/2), return_sequences=True, name=\"first1out\")(repeated)\n",
    "    memory_layer = LSTM (lstm_IPDim,  return_sequences=True, name=\"first2out\")(memory_layer)\n",
    "    decoded_inputs = TimeDistributed(Dense(units=lstm_OPDim, activation='linear'))( memory_layer)\n",
    "\n",
    "    #  Try spatial dropout?\n",
    "    dropout_input = Dropout(drop)(input_layer)\n",
    "    concat_layer  = concatenate([dropout_input, decoded_inputs])\n",
    "\n",
    "    #memory_layer = LSTM (units=self.lstm_dim, return_sequences=False)(concat_layer)\n",
    "    memory_layer = LSTM (units=lstm_IPDim, \n",
    "                             kernel_regularizer = k_rrizer, \n",
    "                             recurrent_regularizer = r_rrizer, \n",
    "                             return_sequences=False)(concat_layer)\n",
    "    preds = Dense(units=lstm_OPDim, activation='linear')(memory_layer)\n",
    "\n",
    "    model1 = Model(input_layer, preds)\n",
    "    model1.compile(optimizer = optimizer, loss= loss)             \n",
    "\n",
    "    print(model1.summary())\n",
    "    return conf, df, adf;\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "sysargs=None\n",
    "def addargs():\n",
    "    sysargs = None\n",
    "    p = argparse.ArgumentParser(f\"{os.path.basename(sys.argv[0])}:\")\n",
    "    p.add_argument('-c', '--config', type=str, default=\"config.txt\", help=\"Config Files\")\n",
    "    p.add_argument('-o', '--output', type=str, default=\"out.csv\", help=\"output file\")\n",
    "    p.add_argument('args', nargs=argparse.REMAINDER)\n",
    "    p.add_argument('input_files',action=\"store\", type=str, nargs='+', help=\"input file(s)\")\n",
    "\n",
    "    #p.print_help() # always print -help\n",
    "    try:\n",
    "        sysargs=p.parse_args(sys.argv[1:])\n",
    "    except argparse.ArgumentError as exc:\n",
    "        print(exc.message )\n",
    "        \n",
    "    return sysargs\n",
    "#-----------------------------------------------------------------------------------\n",
    "#*** NOTE: DO NOT EDIT THIS FILE - THIS iS CREATED FROM: inv_utils.ipynb\n",
    "def inJupyter():\n",
    "    try:    get_ipython; return True\n",
    "    except: return False\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    if (not inJupyter()):\n",
    "        t1 = datetime.datetime.now()\n",
    "        sysargs = addargs()\n",
    "        ret = process(sysargs.config, sysargs.input_files, sysargs.output)\n",
    "        t2 = datetime.datetime.now()\n",
    "        print(f\"#All Done in {str(t2-t1)} ***\")\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "js.update({})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
