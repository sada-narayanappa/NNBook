{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Prediction\n",
    "\n",
    "In this article I use LSTM for predicting stock prices; \n",
    "\n",
    "\n",
    "## Some preliminaries\n",
    "Either\n",
    "\n",
    "* If you like to full experience including fresh data download or customize, then you must\n",
    "get the API key from AlphaVantage [https://www.alphavantage.co/support/#api-key] to download data;\n",
    "\n",
    "OR\n",
    "\n",
    "* Use the data same data to learn about multivariate prediction first and then you can download later.\n",
    "\n",
    "if you choose to use Alpha Vantage pip install as follows:\n",
    "\n",
    "```\n",
    "pip install alpha_vantage\n",
    "pip install git+https://github.com/RomelTorres/alpha_vantage.git@develop\n",
    "```\n",
    "\n",
    "Follow along until you create the dataset; but you have a choice to use the already processed data.\n",
    "Just skip everything and just use **data/stockdata.csv** for moving ahead with LSTM.\n",
    "Later I will add news media, sentiments etc.\n",
    "\n",
    "If you really are interested in pulling data from AlphaVantage, scripts are in **gen/getstocksdata.py** \n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "1. https://towardsdatascience.com/getting-rich-quick-with-machine-learning-and-stock-market-predictions-696802da94fe\n",
    "\n",
    "2. https://towardsdatascience.com/step-by-step-understanding-lstm-autoencoder-layers-ffab055b6352\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing.\n",
    "\n",
    "In order to process data through LSTM, the input must be transformed appropriately.\n",
    "Following transformations are performed to make the input data is suitable for LSTM.\n",
    "\n",
    "\n",
    "1. Scale the numerical sensors using standard scaler or Min-Max Scaler \\\n",
    "**IMPORTANTANT**: ONLY Scale based on training set - not including the test set.\n",
    "\n",
    "\n",
    "2. Convert categorial variables to one-hot encoding only if the number of unique values are \\\n",
    "greater than 2 and less than some pre-specified number, say 4 or 3\n",
    "\n",
    "\n",
    "3. create a new data-set of \\\n",
    "    a. scaled numerical sensors \\\n",
    "    b. binary categorical variable - not needing One-hot-encoding \\\n",
    "    c. One-hot encoded new columns.\n",
    "    \n",
    "\n",
    "4. Prepare a dataset for LSTM using Time Series Generator - using history \\\n",
    "CAREFUL: ensure the oldest is the first one.\n",
    "\n",
    "\n",
    "5. Keep track of all of the above carefully for predictions when needed in the future. \\\n",
    "Save them to disk so it can be recovered for newer training or predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I assume you have the data ready in \"data/stockdata_ext.csv\"\n",
    "\n",
    "\n",
    "In \"stockdata_ext.csv\" I have added a dummy columns with categorical variable to show the processing\n",
    "\n",
    "#STEP1 is to create a configuratio file \n",
    "Run\n",
    "```\n",
    "$  gen/dataconfig.py data/stockdata_ext.csv > config.1\n",
    "```\n",
    "\n",
    "This will create a configuration file - review config.1 \n",
    "We will modify it so it we can run a model on simpler datset\n",
    "\n",
    "```\n",
    "$ cp config.1 config.2\n",
    "\n",
    "```\n",
    "\n",
    "Copy the configuration file as aboveand edit config.2 to contain\n",
    "This is a configuration file for dataprocess.py to use limited columns and\n",
    "Add difference columns\n",
    "\n",
    "--- CONTENTS of config.2\n",
    "\n",
    "```\n",
    "[START]\n",
    "{ \n",
    "\"numericCols\"  : ['MSFT_open', 'MSFT_high', 'MSFT_low', 'MSFT_close', 'MSFT_volume', 'MSFT_open_diff1'],\n",
    "\"addDiffs\"     : ['MSFT_open'],\n",
    "\"scalerString\" : [],\n",
    "\"scale\"        : 1\n",
    "}\n",
    "[END]\n",
    "```\n",
    "\n",
    "\n",
    "Run the following to create a new configuration file \n",
    "\n",
    "```\n",
    "$ gen/dataprepare.py -c config\\* data/stockdata_ext.csv > myconfig\n",
    "\n",
    "```\n",
    "\n",
    "Inspect myconfig ==> This can be used for preparing additonal datasets infuture \n",
    "by default dataprepare.py - creates a file called out.txt for you to use directly in LSTM\n",
    "it would do the following\n",
    "\n",
    "* STEP 0: Add Differecing Columns\n",
    "* STEP 1. --> Scale all Numeric Columns if scaler exists\n",
    "** STEP 2: => One hot encode \n",
    "* STEP 3: Get Binary columns\n",
    "* FINALLY: Concatenate all the data\n",
    "\n",
    "* Finally it will drop all columns with no data\n",
    "* and forward-fill, backwardfill \n",
    "\n",
    "Finally, the last command you ran, \"...gen/dataprepare.py ...\", creates a file out.txt that we can finally use it for our LSTM.\n",
    "\n",
    "\n",
    "### What happend.\n",
    "\n",
    "We create a configuration file for processing the file so that it can be used for generating data for LSTM consistently.\n",
    "\n",
    "the file configuration file \"myconfig\" contents are:\n",
    "\n",
    "```\n",
    "#Getting Configuration from config.1\n",
    "#Getting Configuration from config.2\n",
    "=>Processing 1/1 ../data/stockdata_ext.csv - standby\n",
    "[START]\n",
    "{\n",
    "{\n",
    "              \"file\": ['/opt/SCHAS/NNBook/notebooks/NNetworks/LSTM/data/stockdata_ext.csv'],\n",
    "     \"number_Unique\": 6,\n",
    "      \"constantCols\": [],\n",
    "       \"categorCols\": ['MSFT_-ve', 'MSFT_+ve'],\n",
    "       \"onehotECols\": ['MSFT_+ve'],\n",
    "        \"binaryCols\": ['MSFT_-ve'],\n",
    "       \"numericCols\": ['MSFT_open', 'MSFT_high', 'MSFT_low', 'MSFT_close', 'MSFT_volume', 'MSFT_open_diff1'],\n",
    "       \"notNumerics\": ['timestamp'],\n",
    "    \"timeCorrelated\": 0.94,\n",
    "      \"timeCorrCols\": [],\n",
    "    \"timeCorrLength\": 0,\n",
    "    \"excludePattern\": [],\n",
    "    \"includePattern\": [],\n",
    "       \"dropColumns\": [],\n",
    "      \"finalColumns\": ['binaryCols', 'numericCols', 'onehotECols'],\n",
    "      \"scalerString\": ['gANjc2tsZWF... OMITTED...uM3EudWIu'],\n",
    "          \"addDiffs\": ['MSFT_open'],\n",
    "         \"train_pct\": 0.7,\n",
    "       \"train_count\": 3671,\n",
    "               \"end\": 0,\n",
    "             \"scale\": 1,\n",
    "        \"predictors\": ['MSFT_-ve', 'MSFT_open', 'MSFT_high', 'MSFT_low', 'MSFT_close', 'MSFT_volume', 'MSFT_+ve__S__0', 'MSFT_+ve__S__1', 'MSFT_+ve__S__2'],\n",
    "       \"outputsCols\": ['MSFT_open', 'MSFT_open_diff1'],\n",
    "          \"tsParams\": {'length': 5, 'batch_size': 1, 'stride': 1, 'sampling_rate': 1},\n",
    "     \"predictLength\": 5,\n",
    "            \"nsteps\": 1,\n",
    "            \"scaler\": [],\n",
    "        \"resultCols\": ['MSFT_-ve', 'MSFT_open', 'MSFT_high', 'MSFT_low', 'MSFT_close', 'MSFT_volume', 'MSFT_open_diff1', 'MSFT_+ve__S__0', 'MSFT_+ve__S__1', 'MSFT_+ve__S__2'],\n",
    "\"end\": 0\n",
    "}\n",
    "[END]\n",
    "}\n",
    "\n",
    "[END]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "technical_indicators = np.array([[1,1]])\n",
    "# define two sets of inputs\n",
    "lstm_input = Input(shape=(lookBack, nFeatures), name='lstm_input')\n",
    "dense_input = Input(shape=(technical_indicators.shape[1],), name='tech_input')\n",
    "\n",
    "# the first branch operates on the first input\n",
    "x = LSTM(50, name='lstm_0')(lstm_input)\n",
    "x = Dropout(0.2, name='lstm_dropout_0')(x)\n",
    "lstm_branch = Model(inputs=lstm_input, outputs=x)\n",
    "\n",
    "# the second branch opreates on the second input\n",
    "y = Dense(20, name='tech_dense_0')(dense_input)\n",
    "y = Activation(\"relu\", name='tech_relu_0')(y)\n",
    "y = Dropout(0.2, name='tech_dropout_0')(y)\n",
    "technical_indicators_branch = Model(inputs=dense_input, outputs=y)\n",
    "\n",
    "# combine the output of the two branches\n",
    "combined = concatenate([lstm_branch.output, technical_indicators_branch.output], name='concatenate')\n",
    "\n",
    "z = Dense(64, activation=\"sigmoid\", name='dense_pooling')(combined)\n",
    "z = Dense(1, activation=\"linear\", name='dense_out')(z)\n",
    "\n",
    "# our model will accept the inputs of the two branches and\n",
    "# then output a single value\n",
    "model2 = Model(inputs=[lstm_branch.input, technical_indicators_branch.input], outputs=z)\n",
    "adam = optimizers.Adam(lr=0.0005)\n",
    "model2.compile(optimizer=adam, loss='mse')\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "np.random.seed(4)\n",
    "import base64, os, json, glob, re, sys\n",
    "from sklearn import preprocessing\n",
    "from Crypto.Cipher import AES\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (16, 5)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "pd.options.display.max_rows = 5\n",
    "%matplotlib inline  \n",
    "\n",
    "use_keras=1\n",
    "if ( use_keras):\n",
    "    from keras.models import Sequential, Model\n",
    "    from keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate, TimeDistributed\n",
    "    from keras.layers import Conv1D, GlobalMaxPool1D,Flatten, Bidirectional, RepeatVector, MaxPooling1D\n",
    "    from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "    from keras import regularizers\n",
    "    from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, Callback\n",
    "    from keras import optimizers\n",
    "    from keras.models import load_model\n",
    "else:\n",
    "    from tensorflow.keras.models import Sequential, Model\n",
    "    from tensorflow.keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate, TimeDistributed\n",
    "    from tensorflow.keras.layers import Conv1D, GlobalMaxPool1D,Flatten, Bidirectional, RepeatVector, MaxPooling1D\n",
    "    from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "    from tensorflow.keras import regularizers\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, Callback\n",
    "    from tensorflow.keras import optimizers\n",
    "    from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDummyCols():\n",
    "    tf1=df\n",
    "    if ( \"MSFT_+ve\" not in tf1.columns):\n",
    "        tf1.insert(1, \"MSFT_+ve\", value=[f\"S__{k}\" for k in np.random.randint(0,3, size=len(tf1))] )\n",
    "    if ( \"MSFT_-ve\" not in tf1.columns):\n",
    "        #tf.insert(1, \"MSFT_-ve\", value=[f\"S__{k}\" for k in np.random.randint(0,1, size=len(nf))] )\n",
    "        tf1.insert(1, \"MSFT_-ve\", value=[k for k in np.random.randint(0,2, size=len(tf1))] )\n",
    "    tf1.to_csv(\"data/stockdata_ext.csv\", index=False);\n",
    "    tf1\n",
    "\n",
    "stockfile = \"out.csv\"\n",
    "df = pd.read_csv(stockfile)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## For Prediction usng LSTM\n",
    "\n",
    "We will go directly into the correct LSTM implementation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\".\")\n",
    "sys.path.append(\"gen\")\n",
    "#-----------------------------------------------------------------------------------\n",
    "import dataprepare\n",
    "\n",
    "config = dataprepare.getJSONconfig(\"gen/myconfig\")\n",
    "assert config, \"Config is missing!!\"\n",
    "predsI = config.get(\"predictors\", [])\n",
    "predsV = config.get(\"outputsCols\", [])\n",
    "tsParam= config.get(\"tsParams\")\n",
    "train_count = config.get(\"train_count\")\n",
    "print(predsI, predsV, tsParam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[predsI].values\n",
    "y = df[predsV].values\n",
    "trng    = TimeseriesGenerator(X[:train_count], y[:train_count], **tsParam )\n",
    "valg    = TimeseriesGenerator(X[train_count:], y[train_count:], **tsParam )\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trng[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1=None      \n",
    "\n",
    "#sensN = len(self.train_transformed[0].columns)  # number of sensors (eliminating the two time ones)\n",
    "#outN = len(self.num_id_list) # number of output sensors; the non-categorical ones        \n",
    "\n",
    "lookBack   = tsParam['length']\n",
    "nFeatures  = X.shape[1]  # Number of features \n",
    "lstm_OPDim = y.shape[1]  # This is usually all sensors except categorical that to train LSTM on\n",
    "lstm_IPDim = 256\n",
    "drop       = 0.3\n",
    "optimizer  = optimizers.Adam(lr=0.0005)\n",
    "loss       = 'mse'\n",
    "k_rrizer   = None\n",
    "r_rrizer   = None\n",
    "\n",
    "input_layer  = Input(shape=(lookBack, nFeatures), dtype='float32', name='input')\n",
    "memory_layer = LSTM( lstm_IPDim, return_sequences=True, name=\"memory1\")(input_layer)\n",
    "memory_layer = LSTM (int(lstm_IPDim/2), return_sequences=False, name=\"memory2\")(memory_layer)\n",
    "repeated     = RepeatVector(lookBack)(memory_layer)\n",
    "memory_layer = LSTM (int(lstm_IPDim/2), return_sequences=True, name=\"first1out\")(repeated)\n",
    "memory_layer = LSTM (lstm_IPDim,  return_sequences=True, name=\"first2out\")(memory_layer)\n",
    "decoded_inputs = TimeDistributed(Dense(units=lstm_OPDim, activation='linear'))( memory_layer)\n",
    "\n",
    "#  Try spatial dropout?\n",
    "dropout_input = Dropout(drop)(input_layer)\n",
    "concat_layer  = concatenate([dropout_input, decoded_inputs])\n",
    "\n",
    "#memory_layer = LSTM (units=self.lstm_dim, return_sequences=False)(concat_layer)\n",
    "memory_layer = LSTM (units=lstm_IPDim, \n",
    "                         kernel_regularizer = k_rrizer, \n",
    "                         recurrent_regularizer = r_rrizer, \n",
    "                         return_sequences=False)(concat_layer)\n",
    "preds = Dense(units=lstm_OPDim, activation='linear')(memory_layer)\n",
    "\n",
    "model1 = Model(input_layer, preds)\n",
    "model1.compile(optimizer = optimizer, loss= loss)             \n",
    "\n",
    "print(model1.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "class EarlyStoppingByLossVal(Callback):\n",
    "    def __init__(self, monitor='val_loss', value=0.00001, verbose=0):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "        self.current = np.inf\n",
    "        self.best    = self.current\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.current = logs.get(self.monitor)\n",
    "        self.best    = min(self.best, self.current)\n",
    "        \n",
    "        if self.current is None:\n",
    "            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
    "\n",
    "        if self.current < self.value:\n",
    "            if self.verbose > 0:\n",
    "                print(f\"Epoch:SADA {epoch}: early stopping THR: loss {self.current:10.7}\")\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "est= EarlyStoppingByLossVal(monitor='val_loss', value=0.0000001, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = ModelCheckpoint(filepath=\"stockpredictions.h5\", monitor='val_loss', save_best_only=True, verbose=1)\n",
    "tb = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True,  write_images=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights=0\n",
    "if ( load_weights and os.path.exists(\"stockpredictions.h5\")):\n",
    "    model1.load_weights(\"stockpredictions.h5\")\n",
    "    model1.fit(trng, verbose=1, epochs=1, validation_data=valg,steps_per_epoch=1,\n",
    "                   validation_steps=50, callbacks=[est])\n",
    "    #cp.best = est.best\n",
    "    print(cp.best, est.best, est.current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1.fit(trng, verbose=1, epochs=10, validation_data=valg, steps_per_epoch=200, \n",
    "                validation_steps=50, workers=4, use_multiprocessing=True, callbacks=[cp, est])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['figure.figsize'] = (16, 5)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "yp = model1.predict(valg)\n",
    "#y = ndf[1:][predsV].values\n",
    "yo= list(valg[0][1])\n",
    "for i in range(1,len(valg)):\n",
    "    yo += list(valg[i][1])\n",
    "yo=np.array(yo)\n",
    "\n",
    "f, howmany = -200, 100\n",
    "\n",
    "plt.plot(yo[:,0][f:f + howmany], marker='o', label=\"orginal\", )\n",
    "plt.plot(yp[:,0][f:f + howmany], marker='x', label=\"predicted\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(yo[:,1][f:f + howmany], marker='o', label=\"orginal\", )\n",
    "plt.plot(yp[:,1][f:f + howmany], marker='x', label=\"predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "262.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
