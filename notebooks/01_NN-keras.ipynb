{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The scikit-learn version is 0.21.3.\n"
     ]
    }
   ],
   "source": [
    "%run 00_basic.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile  ../gen/lstm_preprocess.py\n",
    "\n",
    "def updateSubSystems(fname, df, sensorDef=None):\n",
    "    return df;\n",
    "    \n",
    "def lstm_preprocess(dfname, tlmFile=None, force=True, minUniques=6, file2=None):\n",
    "    file2 =  f'{bas}.filtered-{minUniques}{ext}' if (file2 is None) else file2\n",
    "    print (f\"Processing: {file1} ==> {file2}\")\n",
    "    \n",
    "    if (os.path.exists(file2) and not force):\n",
    "        print(f\"** NOTHING TO DO **: File: {file2} already exists!\")\n",
    "        return file2, None;\n",
    "\n",
    "    if ( isinstance(dfname, pd.DataFrame) ):\n",
    "        df = dfname\n",
    "    else:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "    #print(\"1\",dfAll);\n",
    "    #~~~~ Count the unique values and take only columsn qith unique value >= 6\n",
    "    print (f\"Checking for Uniques {minUniques}\")\n",
    "    uvs = dfAll.nunique(axis=0)\n",
    "    cols = uvs[uvs >= minUniques].index.tolist()\n",
    "    cols=['time'] + sorted([c for c in set(cols[1:] )])\n",
    "    \n",
    "    print(f\"=>Unique: {minUniques} #columns: reduced to {len(cols)} from: {len(dfAll.columns)}\")\n",
    "    df=dfAll[cols]\n",
    "    print(f\"=>Unique: {minUniques}: shape after removing {df.shape}\")\n",
    "\n",
    "    #~~~~~~ Get time column correctly named\n",
    "    cols = df.columns[df.dtypes.eq('object')]\n",
    "    if (len(cols) > 0):\n",
    "        print(f\"WARNING: *** Non numeric columns => {cols}\")\n",
    "        df = df.drop(cols, axis=1)\n",
    "\n",
    "    df.apply(pd.to_numeric)\n",
    "    df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "\n",
    "    #~~~~ Find any sensor highly correlated with time and drop them.\n",
    "    timeCorSensors = []\n",
    "    timeser = pd.Series(df[['time']].values.reshape(-1))\n",
    "    DROP_INDEX = 0;\n",
    "    for sensor in df.columns[1:]:\n",
    "        sensorSeries = pd.Series(df[sensor].values.reshape(-1))\n",
    "        for i in range(8):\n",
    "            c1 = timeser[i:].corr(sensorSeries[i:])\n",
    "            c2 = timeser[i:].corr(sensorSeries[:-i])\n",
    "            if np.abs(c1) >= 0.92 or np.abs(c2) >= 0.92:\n",
    "                timeCorSensors.append(sensor)\n",
    "                DROP_INDEX = max(DROP_INDEX, i) #lets drop first few rows\n",
    "                break;\n",
    "                \n",
    "    if ( len(timeCorSensors) > 0):\n",
    "        print(f\"#Time Cor: #{len(timeCorSensors)}, #Shape before:{df.shape}\")\n",
    "        df.drop(timeCorSensors, axis=1, inplace=True)\n",
    "        df = df[DROP_INDEX:]\n",
    "        print(f\"#After dropping: {DROP_INDEX} =>{df.shape}\")\n",
    " \n",
    "    #~~~~ Update the column names to include subsystems\n",
    "    updateSubSystems(file2, df, tlmdef=tlmdef)\n",
    "    \n",
    "    if (df.shape == dfAll.shape):\n",
    "        print(f\"-Nothing to do original is fine: making link {file1}->{file2}\")\n",
    "        if ( os.path.exists(file2)):\n",
    "            os.remove(file2)\n",
    "        os.symlink(file1, file2)\n",
    "    else:\n",
    "        if ( os.path.exists(file2)):\n",
    "            os.remove(file2)\n",
    "        #df.to_csv(file2, index=False) <= dont need this now\n",
    "        print(f\"+Wrote to file: {file2} shape:{df.shape} original: {dfAll.shape}\\n\")\n",
    "        \n",
    "    \n",
    "    return file2, df;\n",
    "\n",
    "def main():\n",
    "    csv_path = 'jena_climate_2009_2016.csv.zip'\n",
    "    df = lstm_preprocess(csv_path)\n",
    "    df\n",
    "    \n",
    "#-----------------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    #main()\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[247]:\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "\n",
    "from keras.layers import Conv1D, GlobalMaxPool1D, Dense, Flatten, LSTM, Bidirectional, RepeatVector, MaxPooling1D, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "\n",
    "class anom_lstm():\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, train, windowL = 30, cat_num = 5, Verbose=True, oldPred=True):\n",
    "        self.train_orig = train #raw training and validation data\n",
    "        self.cat_num = cat_num # num unique values necesasry not to be categorical\n",
    "        self.windowL = windowL # window length to use for one-ahead predictions\n",
    "        self.quality_sensors = [x for x in train.columns if x !='time']\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.train_start = train.time.iloc[0]\n",
    "#         self.year_start = year_start\n",
    "        self.Verbose = Verbose\n",
    "        self.oldPred = oldPred # Tells if a new prediction is needed (eg. after new data or more training)\n",
    "\n",
    "    def process_train(self, train_ratio = 0.9, train_val_ratio = 0.1, bats = 256):\n",
    "        self.batch_size = bats\n",
    "        self.train_ratio = train_ratio\n",
    "        self.train_val_ratio = train_val_ratio\n",
    "        self.train_n = int(len(self.train_orig)*train_ratio)\n",
    "        self.val_n = len(self.train_orig) - self.train_n\n",
    "        n = len(self.train_orig)\n",
    "        self.train_raw = self.train_orig.iloc[:int(n*train_ratio)]\n",
    "        self.validate_raw = self.train_orig.iloc[int(n*train_ratio):]\n",
    "        \n",
    "        self.train_pred = False # Whether or not predictions have been made on the training set\n",
    "        # Eliminate constant sensors\n",
    "        self.constant = [x for x in self.train_raw.columns if self.train_raw[x].nunique() == 1]\n",
    "        self.train_raw.drop(self.constant, axis=1, inplace=True)\n",
    "        self.ids = list(self.train_raw.columns)\n",
    "        \n",
    "        # Find any sensor highly correlated with time.\n",
    "        self.timeSensors = []\n",
    "        timeser = pd.Series(self.train_raw[['time']].values.reshape(-1))\n",
    "        for sensor in self.ids:\n",
    "            sensorSeries = pd.Series(self.train_raw[sensor].values.reshape(-1))\n",
    "            if np.abs(timeser.corr(sensorSeries)) >= 0.9:\n",
    "                self.timeSensors.append(sensor)\n",
    "        \n",
    "        # Difference the sensors highly correlated with time\n",
    "        timedf = pd.DataFrame()\n",
    "        timedf['time'] = self.train_raw.time\n",
    "        for sensor in self.timeSensors:\n",
    "            if sensor == 'time':\n",
    "                continue\n",
    "            parts = re.split(r'\\_\\_',sensor) #THIS IS FOR JCSAT NAMING CONVENTIONS\n",
    "#             parts = re.split(r':',sensor) #THIS IS FOR AM10 NAMING CONVENTIONS\n",
    "            diffname = parts[0]+'_DIFF__'+parts[1]\n",
    "            timedf[diffname]=self.train_raw[sensor].diff()\n",
    "        timedf.fillna(method='bfill', inplace=True)\n",
    "\n",
    "        # Checking for difference still correlated with time\n",
    "        ids = list(timedf.columns)\n",
    "        self.difftimeSensors = []\n",
    "        timeser = pd.Series(timedf[['time']].values.reshape(-1))\n",
    "        for sensor in ids:\n",
    "            sensorSeries = pd.Series(timedf[sensor].values.reshape(-1))\n",
    "            if np.abs(timeser.corr(sensorSeries)) >= 0.9:\n",
    "                self.difftimeSensors.append(sensor)\n",
    "        # Drop diff'ed sensors correlated with time (and drop time)\n",
    "        timedf.drop(self.difftimeSensors, axis=1, inplace=True)\n",
    "        \n",
    "        # Make a scaler that one-hot encodes categorical sensors and scales the others. This should\n",
    "        # probably be fit on the Orig dataframes with 'time' dropped.\n",
    "        unique_vals = self.train_raw.nunique()\n",
    "        self.cat_sensors = [x for x in list(set(self.ids)-set(self.timeSensors)) if unique_vals[x] <= self.cat_num]\n",
    "        self.num_sensors = [x for x in list(set(self.ids)-set(self.timeSensors)) if unique_vals[x] > self.cat_num]\n",
    "\n",
    "        self.numeric_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())])\n",
    "\n",
    "        self.categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "        self.preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', self.numeric_transformer, self.num_sensors),\n",
    "                ('cat', self.categorical_transformer, self.cat_sensors)])\n",
    "                                            \n",
    "        frames = [self.train_raw[list(set(self.ids)-set(self.timeSensors))], timedf]\n",
    "        self.fittingdf = pd.concat(frames, axis=1)\n",
    "        self.preprocessor.fit(self.fittingdf)\n",
    "        trans = self.preprocessor.transform(self.fittingdf) #transformed training data\n",
    "        try:\n",
    "            self.train_transformed = pd.DataFrame(trans.todense())\n",
    "        except:\n",
    "            self.train_transformed = pd.DataFrame(trans)\n",
    "        self.train_transformed['day_time_x'] = np.cos(self.train_raw['time']%(24*60*60*1000)*(2*np.pi/(24*60*60*1000)))\n",
    "        self.train_transformed['day_time_y'] = np.sin(self.train_raw['time']%(24*60*60*1000)*(2*np.pi/(24*60*60*1000)))\n",
    "#         self.train_transformed['time'] = self.train_raw['time']\n",
    "\n",
    "        #Now make dictionaries from the new variable names to the old and back\n",
    "        \n",
    "        # First sensor name to id list\n",
    "        # First the numerical sensors\n",
    "        sen_to_idx_list = [] # Will make a dictionary out of this\n",
    "        for i in range(len(self.num_sensors)):\n",
    "            transCols = list(self.train_transformed.columns)\n",
    "            sen_to_idx_list = sen_to_idx_list + [(self.num_sensors[i],[transCols[i]])]\n",
    "        # Now the categorical sensors\n",
    "        base = len(self.num_sensors)\n",
    "        for x in self.cat_sensors:\n",
    "            newpair = (x, list(range(base,base+self.fittingdf[x].nunique())))\n",
    "            sen_to_idx_list.append(newpair)\n",
    "            base+= self.fittingdf[x].nunique()\n",
    "        # The dictionary mapping sensor id to the list of indices in the transformed data that represent it\n",
    "        self.sen_to_idx_dict = dict(sen_to_idx_list)\n",
    "        \n",
    "        #Now use the dictionary to get a list of the ids for the non-categorical and categorical sensors\n",
    "        self.num_id_list = [x[0] for x in self.sen_to_idx_dict.values() if len(x) ==1]\n",
    "        self.cat_id_list = [x[0] for x in self.sen_to_idx_dict.values() if len(x) > 1]\n",
    "        \n",
    "        \n",
    "        # Next making the reverse dictionary from index to snesor id\n",
    "        idx_to_sen_list = []\n",
    "        for pair in sen_to_idx_list:\n",
    "            sublist = [(idx, pair[0]) for idx in pair[1]]\n",
    "            idx_to_sen_list = idx_to_sen_list + sublist\n",
    "        self.idx_to_sen_dict = dict(idx_to_sen_list)\n",
    "        \n",
    "        # Now make a data frame to hold aggregate values for each numerical sensor (R^2, etc)\n",
    "        self.agg_df = pd.DataFrame(index=self.num_sensors)\n",
    "        \n",
    "        # Finally make the Keras sequence generators\n",
    "        # Make the Keras sequence generator for the training data\n",
    "        train_n = int(len(self.train_transformed)*(1-train_val_ratio))\n",
    "        val_n = len(self.train_transformed) - train_n\n",
    "        self.train_gen = TimeseriesGenerator(self.train_transformed.iloc[:train_n,:].values,\n",
    "                                        self.train_transformed[self.num_id_list].iloc[:train_n,:].values,\n",
    "                                        length=self.windowL, sampling_rate=1,stride=1,\n",
    "                                        batch_size=self.batch_size)\n",
    "        # Make the Keras sequence generator for training time validation data\n",
    "        self.train_val_gen = TimeseriesGenerator(self.train_transformed.iloc[train_n:,:].values,\n",
    "                                        self.train_transformed[self.num_id_list].iloc[train_n:,:].values,\n",
    "                                        length=self.windowL, sampling_rate=1,stride=1,\n",
    "                                        batch_size=self.batch_size)\n",
    "\n",
    "        \n",
    "        \n",
    "    def preprocess(self, data):\n",
    "        data_index = data.index.tolist()\n",
    "        # Difference the sensors highly correlated with time\n",
    "        timedf = pd.DataFrame(index=data.index)\n",
    "        timedf['time'] = data.time\n",
    "        for sensor in self.timeSensors:\n",
    "            if sensor == 'time':\n",
    "                continue\n",
    "            parts = re.split(r'\\_\\_',sensor) #JCSAT NAMING CONVENTIONS\n",
    "#             parts = re.split(r':',sensor) #AM10 NAMING CONVENTIONS\n",
    "            diffname = parts[0]+'_DIFF__'+parts[1]\n",
    "            timedf[diffname]=data[sensor].diff()\n",
    "        timedf.fillna(method='bfill', inplace=True)\n",
    "        # Drop diff'ed sensors correlated with time (and drop time)\n",
    "        timedf.drop(self.difftimeSensors, axis=1, inplace=True)\n",
    "\n",
    "        frames = [data[list(set(self.ids)-set(self.timeSensors))], timedf]\n",
    "        fittingdf = pd.concat(frames, axis=1)\n",
    "        trans = self.preprocessor.transform(fittingdf) #transformed training data\n",
    "        try:\n",
    "            data_transformed = pd.DataFrame(trans.todense(),index=self.test_index)#,index=self.test_raw.index)\n",
    "        except:\n",
    "            data_transformed = pd.DataFrame(trans)#,index=self.test_raw.index\n",
    "        data_transformed['time'] = list(data['time'])\n",
    "        data_transformed['day_time_x'] = np.cos(data_transformed['time']%(24*60*60*1000)*(2*np.pi/(24*60*60*1000)))\n",
    "        data_transformed['day_time_y'] = np.sin(data_transformed['time']%(24*60*60*1000)*(2*np.pi/(24*60*60*1000)))\n",
    "        data_transformed.drop(['time'],axis=1, inplace=True)\n",
    "        return data_transformed\n",
    "    \n",
    "    def process_test(self, test_raw):\n",
    "        self.oldPred = True\n",
    "        self.test_raw = test_raw\n",
    "        self.test_transformed = self.preprocess(test_raw)\n",
    "        \n",
    "        # Make the Keras sequence generator for the test data\n",
    "        self.test_gen = TimeseriesGenerator(self.test_transformed.values,\n",
    "                                            self.test_transformed[self.num_id_list].values,\n",
    "                                            length=self.windowL, sampling_rate=1,stride=1,\n",
    "                                            batch_size=1)\n",
    "        \n",
    "    def process_validation(self):\n",
    "        self.validate_transformed = self.preprocess(self.validate_raw)\n",
    "        \n",
    "        # Make the Keras sequence generator for the validate data\n",
    "        self.val_gen = TimeseriesGenerator(self.validate_transformed.values,\n",
    "                                           self.validate_transformed[self.num_id_list].values,\n",
    "                                           length=self.windowL, sampling_rate=1,stride=1,\n",
    "                                           batch_size=1)\n",
    "        \n",
    "        \n",
    "    def make_batch(self, bats):\n",
    "        self.bats = bats #batch size\n",
    "        self.batch_data = np.zeros((self.bats,self.windowL,len(self.train_transformed.columns)))\n",
    "        self.batch_y = np.zeros((self.bats,len(self.num_id_list)))\n",
    "        startinds = np.random.randint(0,len(self.train_transformed)-(self.windowL+1),bats)\n",
    "#         print(startinds[:10])\n",
    "        for ind, start_loc in enumerate(startinds):\n",
    "            self.batch_data[ind,:,:] = self.train_transformed.iloc[start_loc:start_loc+self.windowL].values\n",
    "            self.batch_y[ind:] = self.train_transformed[self.num_id_list].iloc[start_loc+self.windowL,:]\n",
    "       \n",
    "            \n",
    "    def make_model(self, lstm_layers = 1, lstm_units = 1000, metric='mean_squared_error'):\n",
    "        if lstm_layers == 1:\n",
    "            sensN = len(self.train_transformed.columns) -2 # number of sensors (eliminating the two time ones)\n",
    "            outN = len(self.num_id_list) # number of output sensors; the non-categorical ones\n",
    "            self.model = Sequential()\n",
    "            self.model.add(LSTM(units=lstm_units, input_shape = (self.windowL, sensN+2), return_sequences=False))\n",
    "            self.model.add(Dense(units = outN, activation='linear'))\n",
    "            self.model.compile(optimizer='adam', loss='mean_squared_error',metrics=[metric])\n",
    "            print(self.model.summary())\n",
    "        elif lstm_layers == 2:\n",
    "            sensN = len(self.train_transformed.columns) -2 # number of sensors (eliminating the two time ones)\n",
    "            self.model = Sequential()\n",
    "            self.model.add(LSTM(units=lstm_units, input_shape = (self.windowL, sensN+2), return_sequences=True))\n",
    "            self.model.add(Dropout(0.4))\n",
    "            self.model.add(LSTM(units=lstm_units, return_sequences=False))\n",
    "            self.model.add(Dropout(0.4))\n",
    "            self.model.add(Dense(units = outN, activation='linear'))\n",
    "            self.model.compile(optimizer='adam', loss='mean_squared_error',metrics=[metric])\n",
    "            print(self.model.summary())\n",
    "        else:\n",
    "            print('lstm_layers must be 1 or 2')\n",
    "        \n",
    "    def fit_model(self, epochs = 3, verbose=True):\n",
    "        self.oldPred = True\n",
    "        self.model.fit_generator(self.train_gen,\n",
    "                       validation_data = self.train_val_gen,\n",
    "                       epochs = epochs,\n",
    "                       verbose=verbose)\n",
    "        self.train_loss = self.train_loss + self.model.history.history['loss']\n",
    "        self.val_loss = self.val_loss + self.model.history.history['val_loss']\n",
    "\n",
    "        \n",
    "    def predict_train(self):\n",
    "        \"\"\"\n",
    "        Predict on the training set for the purpose of determining which sensors are well-predictable.\n",
    "        \"\"\"\n",
    "        self.train_pred = True\n",
    "        self.train_preds = self.model.predict_generator(self.train_gen)\n",
    "        # Because the validation set is taken from the training set before scaling and training, this must be cut.\n",
    "        self.train_y = self.train_transformed[self.num_id_list].values[self.windowL:len(self.train_preds)+self.windowL]\n",
    "\n",
    "        self.train_error_var = np.var(self.train_y - self.train_preds, axis=0) # variance of the predictions error on training\n",
    "        self.train_error_mean = np.mean(self.train_y - self.train_preds, axis=0) # mean of the prediction error on training\n",
    "        self.train_abserrordf = pd.DataFrame(np.abs(self.train_y - self.train_preds), columns=self.num_id_list)\n",
    "        self.train_abs_errordf = pd.DataFrame(np.abs(self.train_y - self.train_preds), columns=self.num_id_list)\n",
    "        self.train_errordf = pd.DataFrame((self.train_y - self.train_preds), columns=self.num_id_list)\n",
    "        \n",
    "        \n",
    "    def predict_validate(self):\n",
    "        \"\"\"\n",
    "        Predict on the validation set for the purpose of determining which sensors are well-predictable.\n",
    "        \"\"\"\n",
    "        self.validate_pred = True\n",
    "        self.validate_preds = self.model.predict_generator(self.val_gen)\n",
    "        self.validate_y = self.validate_transformed[self.num_id_list].values[self.windowL:]\n",
    "        \n",
    "        self.r2 = r2_score(self.validate_y, self.validate_preds, multioutput='raw_values')\n",
    "        self.agg_df['validate_r2'] = self.r2\n",
    "        self.validate_error_var = np.var(self.validate_y - self.validate_preds, axis=0) # variance of the predictions error on training\n",
    "        self.agg_df['validate_error_var'] = self.validate_error_var\n",
    "        self.validate_error_mean = np.mean(self.validate_y - self.validate_preds, axis=0) # mean of the prediction error on training\n",
    "        self.agg_df['validate_error_mean'] = self.validate_error_mean\n",
    "#        self.validate_abserrordf = pd.DataFrame(np.abs(self.validate_y - self.validate_preds), columns=self.num_id_list)\n",
    "        self.validate_abs_errordf = pd.DataFrame(np.abs(self.validate_y - self.validate_preds), columns=self.num_id_list)\n",
    "        self.validate_errordf = pd.DataFrame((self.validate_y - self.validate_preds), columns=self.num_id_list)\n",
    "     \n",
    "     \n",
    "         \n",
    "       \n",
    "    def predict_new(self):\n",
    "        self.oldPred = False\n",
    "        self.preds = self.model.predict_generator(self.test_gen)\n",
    "        self.test_y = self.test_transformed[self.num_id_list].values[self.windowL:]\n",
    "        self.test_abs_errordf = pd.DataFrame(np.abs(self.test_y - self.preds), columns=self.num_id_list)\n",
    "        self.test_errordf = pd.DataFrame((self.test_y - self.preds), columns=self.num_id_list)\n",
    "        \n",
    "          \n",
    "\n",
    "            \n",
    "    def score_test(self, r2_threshold = -np.inf):\n",
    "        self.r2_threshold = r2_threshold # # R^2 cutoff for using a sensor in score, etc.\n",
    "#         self.high_quality_sensors2 = [x for (i,x) in enumerate(self.num_id_list) if self.r2[i] >= self.r2_threshold]\n",
    "        self.high_quality_sensor_names = [x for x in self.agg_df.index.tolist() if self.agg_df.loc[x,'validate_r2'] >= self.r2_threshold]\n",
    "        self.high_quality_sensors = [self.sen_to_idx_dict[x][0] for x in self.high_quality_sensor_names]\n",
    "        self.high_qualitypred_df = pd.DataFrame(self.preds, columns=self.num_id_list)[self.high_quality_sensors]\n",
    "        if self.oldPred:\n",
    "            print('Predictions are old, so re-predicting.')\n",
    "            self.predict_new()\n",
    "        high_quality_y = pd.DataFrame(self.test_y, columns = self.num_id_list)[self.high_quality_sensors]\n",
    "        self.score = np.linalg.norm(self.high_qualitypred_df.values - high_quality_y.values,axis=1) #score not scaled by R^2\n",
    "        \n",
    "        # For R^2 scaling for the score\n",
    "        \n",
    "        error_mat = np.abs(self.high_qualitypred_df.values - high_quality_y.values)\n",
    "        scaled_error_mat = error_mat * self.r2[np.where(self.r2 > self.r2_threshold)].reshape(1,-1)\n",
    "        self.r2_score = np.sum(scaled_error_mat, axis=1)\n",
    "        \n",
    "    def score_validate(self):\n",
    "        high_qualitypred_df = pd.DataFrame(self.validate_preds, columns=self.num_id_list)[self.high_quality_sensors]\n",
    "        high_quality_y = pd.DataFrame(self.validate_y, columns = self.num_id_list)[self.high_quality_sensors]\n",
    "        self.valid_score = np.linalg.norm(high_qualitypred_df.values - high_quality_y.values,axis=1) #score not scaled by R^2\n",
    "        \n",
    "        # For R^2 scaling for the score\n",
    "        \n",
    "        error_mat = np.abs(high_qualitypred_df.values - high_quality_y.values)\n",
    "        scaled_error_mat = error_mat * self.r2[np.where(self.r2 > self.r2_threshold)].reshape(1,-1)\n",
    "        self.r2_valid_score = np.sum(scaled_error_mat, axis=1)\n",
    "          \n",
    "       \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
